{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Geometry: Mathematical Foundations\n",
    "\n",
    "## How We Model Information Using Differential Geometry\n",
    "\n",
    "This notebook explains the **mathematical design** behind the diffgeo library â€” how differential geometry provides the natural language for describing uncertainty, learning, and inference.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: The Core Insight â€” Why Geometry?\n",
    "\n",
    "### The Problem with Euclidean Thinking\n",
    "\n",
    "When we optimize a model with parameters $\\theta \\in \\mathbb{R}^n$, standard gradient descent treats parameter space as **flat**:\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta L$$\n",
    "\n",
    "But this is **coordinate-dependent**! If we reparameterize $\\phi = f(\\theta)$, the \"same\" step looks completely different:\n",
    "\n",
    "$$\\|\\Delta\\theta\\|_2 \\neq \\|\\Delta\\phi\\|_2$$\n",
    "\n",
    "**Example**: For a Gaussian $\\mathcal{N}(\\mu, \\sigma^2)$, the Euclidean distance from $(\\mu, \\sigma) = (0, 1)$ to $(0, 2)$ equals the distance to $(0, 101)$ from $(0, 100)$. But statistically, doubling the variance is a massive change; adding 1 to a variance of 100 is negligible.\n",
    "\n",
    "### The Principle of Covariance\n",
    "\n",
    "The resolution comes from physics: **physical laws should not depend on coordinate choice**. This is Einstein's *principle of general covariance* [Einstein, 1916].\n",
    "\n",
    "In machine learning, we adopt an analogous principle:\n",
    "\n",
    "> **Statistical Covariance Principle**: Quantities that measure statistical relationships should be invariant under reparameterization.\n",
    "\n",
    "This forces us to use **tensors** â€” objects with specific transformation rules that cancel coordinate dependence.\n",
    "\n",
    "### The Geometric Objects\n",
    "\n",
    "A **smooth manifold** $\\mathcal{M}$ is a space that locally looks like $\\mathbb{R}^n$ but may have global curvature. The key structures:\n",
    "\n",
    "| Object | Symbol | Definition | What It Measures |\n",
    "|--------|--------|------------|------------------|\n",
    "| **Tangent space** | $T_p\\mathcal{M}$ | Vector space of \"velocities\" at $p$ | Directions you can move |\n",
    "| **Cotangent space** | $T^*_p\\mathcal{M}$ | Dual space of linear functionals | Gradients live here |\n",
    "| **Metric tensor** | $g_{ij}(p)$ | Smoothly varying inner product | Length, angle, distance |\n",
    "| **Connection** | $\\Gamma^k_{ij}$ | Rule for parallel transport | How to compare vectors at different points |\n",
    "| **Curvature** | $R^l_{ijk}$ | Failure of parallel transport to close | Intrinsic bending of space |\n",
    "\n",
    "### The Metric Tensor: Formal Definition\n",
    "\n",
    "**Definition**: A *Riemannian metric* on $\\mathcal{M}$ is a smooth assignment $p \\mapsto g_p$ where each $g_p: T_p\\mathcal{M} \\times T_p\\mathcal{M} \\to \\mathbb{R}$ is:\n",
    "1. **Symmetric**: $g_p(u, v) = g_p(v, u)$\n",
    "2. **Positive definite**: $g_p(v, v) > 0$ for $v \\neq 0$\n",
    "3. **Bilinear**: Linear in each argument\n",
    "\n",
    "In coordinates, $g_p(u, v) = g_{ij}(p) u^i v^j$ (Einstein summation).\n",
    "\n",
    "The **infinitesimal arc length** is:\n",
    "$$ds^2 = g_{ij}(\\theta) \\, d\\theta^i \\, d\\theta^j$$\n",
    "\n",
    "**Transformation law**: Under $\\phi = f(\\theta)$:\n",
    "$$\\tilde{g}_{ab}(\\phi) = \\frac{\\partial \\theta^i}{\\partial \\phi^a} \\frac{\\partial \\theta^j}{\\partial \\phi^b} g_{ij}(\\theta)$$\n",
    "\n",
    "This ensures $ds^2 = g_{ij} d\\theta^i d\\theta^j = \\tilde{g}_{ab} d\\phi^a d\\phi^b$ â€” the length element is **invariant**.\n",
    "\n",
    "### Geodesics: Shortest Paths\n",
    "\n",
    "**Definition**: A *geodesic* is a curve $\\gamma(t)$ that parallel transports its own tangent vector:\n",
    "$$\\nabla_{\\dot{\\gamma}} \\dot{\\gamma} = 0$$\n",
    "\n",
    "In coordinates, this becomes the **geodesic equation**:\n",
    "$$\\ddot{\\gamma}^k + \\Gamma^k_{ij} \\dot{\\gamma}^i \\dot{\\gamma}^j = 0$$\n",
    "\n",
    "where the **Christoffel symbols** are:\n",
    "$$\\Gamma^k_{ij} = \\frac{1}{2} g^{kl} \\left( \\partial_i g_{jl} + \\partial_j g_{il} - \\partial_l g_{ij} \\right)$$\n",
    "\n",
    "Geodesics are the \"straight lines\" of curved space â€” they minimize arc length locally.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“š Further Reading: Part 1\n",
    "\n",
    "**Foundational texts**:\n",
    "- **do Carmo, M.P.** (1992). *Riemannian Geometry*. BirkhÃ¤user. â€” The standard graduate introduction.\n",
    "- **Lee, J.M.** (2018). *Introduction to Riemannian Manifolds*. Springer. â€” Modern, rigorous treatment.\n",
    "- **Spivak, M.** (1979). *A Comprehensive Introduction to Differential Geometry* (5 vols). â€” Exhaustive, from first principles.\n",
    "\n",
    "**For machine learning audience**:\n",
    "- **Absil, P.-A., Mahony, R., Sepulchre, R.** (2008). *Optimization Algorithms on Matrix Manifolds*. Princeton. â€” Practical focus on optimization.\n",
    "- **Boumal, N.** (2023). *An Introduction to Optimization on Smooth Manifolds*. Cambridge. â€” Modern, freely available online.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Fisher Information â€” The Natural Metric on Probability Space\n",
    "\n",
    "### The Statistical Manifold\n",
    "\n",
    "Consider a parametric family of probability distributions:\n",
    "$$\\mathcal{S} = \\{p(x|\\theta) : \\theta \\in \\Theta \\subseteq \\mathbb{R}^n\\}$$\n",
    "\n",
    "This is a **statistical manifold** â€” the parameter space $\\Theta$ inherits geometric structure from the distributions it parameterizes.\n",
    "\n",
    "### Definition: Fisher Information Matrix\n",
    "\n",
    "**Definition** [Fisher, 1925]: The *Fisher Information Matrix* at $\\theta$ is:\n",
    "\n",
    "$$F_{ij}(\\theta) = \\mathbb{E}_{x \\sim p(x|\\theta)}\\left[\\frac{\\partial \\log p(x|\\theta)}{\\partial \\theta^i} \\frac{\\partial \\log p(x|\\theta)}{\\partial \\theta^j}\\right]$$\n",
    "\n",
    "**Equivalent form** (under regularity conditions):\n",
    "$$F_{ij}(\\theta) = -\\mathbb{E}_{x \\sim p(x|\\theta)}\\left[\\frac{\\partial^2 \\log p(x|\\theta)}{\\partial \\theta^i \\partial \\theta^j}\\right]$$\n",
    "\n",
    "The equivalence follows from:\n",
    "$$\\mathbb{E}\\left[\\frac{\\partial \\log p}{\\partial \\theta^i}\\right] = \\int \\frac{\\partial p}{\\partial \\theta^i} dx = \\frac{\\partial}{\\partial \\theta^i} \\int p \\, dx = \\frac{\\partial}{\\partial \\theta^i} (1) = 0$$\n",
    "\n",
    "### The CramÃ©r-Rao Bound: Why Fisher Information Matters\n",
    "\n",
    "**Theorem** [CramÃ©r, 1946; Rao, 1945]: For any unbiased estimator $\\hat{\\theta}$ of $\\theta$:\n",
    "\n",
    "$$\\text{Cov}(\\hat{\\theta}) \\geq F(\\theta)^{-1}$$\n",
    "\n",
    "in the sense of positive semidefinite ordering.\n",
    "\n",
    "**Proof sketch**: Apply Cauchy-Schwarz to the score function $\\nabla_\\theta \\log p(x|\\theta)$ and the estimator error $\\hat{\\theta} - \\theta$.\n",
    "\n",
    "**Interpretation**: The Fisher Information sets a **fundamental limit** on estimation precision. High Fisher = parameters are precisely estimable. Low Fisher = parameters are \"sloppy.\"\n",
    "\n",
    "### ÄŒencov's Uniqueness Theorem\n",
    "\n",
    "**Theorem** [ÄŒencov, 1982]: The Fisher metric is the **unique** Riemannian metric on statistical manifolds (up to a constant factor) that satisfies:\n",
    "\n",
    "1. **Monotonicity under Markov morphisms**: If $\\kappa: \\mathcal{X} \\to \\mathcal{Y}$ is a Markov kernel (stochastic map), and $q(y|\\theta) = \\int \\kappa(y|x) p(x|\\theta) dx$, then:\n",
    "   $$F_q(\\theta) \\leq F_p(\\theta)$$\n",
    "   (Information can only be lost, never gained, by coarse-graining.)\n",
    "\n",
    "2. **Invariance under sufficient statistics**: If $T(x)$ is sufficient for $\\theta$, the Fisher metric computed from $T$ equals that computed from $x$.\n",
    "\n",
    "**Significance**: This theorem elevates Fisher Information from \"a useful quantity\" to \"the unique correct choice\" â€” it's forced by information-theoretic axioms.\n",
    "\n",
    "**Proof**: See [Campbell, 1986] for an accessible proof, or [ÄŒencov, 1982] for the original.\n",
    "\n",
    "### Local Equivalence with KL Divergence\n",
    "\n",
    "**Theorem**: For nearby distributions:\n",
    "$$D_{KL}(p_\\theta \\| p_{\\theta + d\\theta}) = \\frac{1}{2} F_{ij}(\\theta) d\\theta^i d\\theta^j + O(\\|d\\theta\\|^3)$$\n",
    "\n",
    "**Proof**:\n",
    "$$D_{KL}(p_\\theta \\| p_{\\theta+d\\theta}) = \\int p_\\theta \\log \\frac{p_\\theta}{p_{\\theta+d\\theta}} dx$$\n",
    "\n",
    "Taylor expand $\\log p_{\\theta+d\\theta}$ around $\\theta$:\n",
    "$$\\log p_{\\theta+d\\theta} = \\log p_\\theta + d\\theta^i \\partial_i \\log p_\\theta + \\frac{1}{2} d\\theta^i d\\theta^j \\partial_i \\partial_j \\log p_\\theta + O(\\|d\\theta\\|^3)$$\n",
    "\n",
    "Substituting and using $\\mathbb{E}[\\partial_i \\log p] = 0$:\n",
    "$$D_{KL} = -\\frac{1}{2} d\\theta^i d\\theta^j \\mathbb{E}[\\partial_i \\partial_j \\log p] + O(\\|d\\theta\\|^3) = \\frac{1}{2} F_{ij} d\\theta^i d\\theta^j + O(\\|d\\theta\\|^3)$$\n",
    "\n",
    "**Interpretation**: The Fisher metric measures the **instantaneous rate of KL divergence** as you move in parameter space.\n",
    "\n",
    "### Example: Gaussian Family\n",
    "\n",
    "For the univariate Gaussian $p(x|\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$:\n",
    "\n",
    "$$\\log p = -\\log\\sigma - \\frac{1}{2}\\log(2\\pi) - \\frac{(x-\\mu)^2}{2\\sigma^2}$$\n",
    "\n",
    "Computing derivatives and expectations:\n",
    "$$F_{\\mu\\mu} = \\mathbb{E}\\left[\\left(\\frac{x-\\mu}{\\sigma^2}\\right)^2\\right] = \\frac{1}{\\sigma^2}$$\n",
    "$$F_{\\sigma\\sigma} = \\mathbb{E}\\left[\\left(-\\frac{1}{\\sigma} + \\frac{(x-\\mu)^2}{\\sigma^3}\\right)^2\\right] = \\frac{2}{\\sigma^2}$$\n",
    "$$F_{\\mu\\sigma} = 0 \\quad \\text{(by symmetry)}$$\n",
    "\n",
    "So:\n",
    "$$F = \\begin{pmatrix} 1/\\sigma^2 & 0 \\\\ 0 & 2/\\sigma^2 \\end{pmatrix}$$\n",
    "\n",
    "The manifold of Gaussians has **negative curvature** (hyperbolic geometry). The Gaussian family is isometric to the PoincarÃ© half-plane!\n",
    "\n",
    "### The Score Function and Natural Gradient\n",
    "\n",
    "The **score function** is $s(\\theta; x) = \\nabla_\\theta \\log p(x|\\theta)$.\n",
    "\n",
    "- It has zero mean: $\\mathbb{E}[s] = 0$\n",
    "- Its covariance is Fisher: $\\text{Cov}(s) = F$\n",
    "\n",
    "The **natural gradient** [Amari, 1998] uses Fisher to convert gradients:\n",
    "$$\\tilde{\\nabla} L = F^{-1} \\nabla L$$\n",
    "\n",
    "This gives the steepest descent direction in the **intrinsic geometry** of the statistical manifold, not the arbitrary Euclidean geometry of parameter space.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“š Further Reading: Part 2\n",
    "\n",
    "**Original sources**:\n",
    "- **Fisher, R.A.** (1925). \"Theory of Statistical Estimation.\" *Proc. Cambridge Phil. Soc.* 22:700â€“725.\n",
    "- **Rao, C.R.** (1945). \"Information and accuracy attainable in estimation.\" *Bull. Calcutta Math. Soc.* 37:81â€“91.\n",
    "- **ÄŒencov, N.N.** (1982). *Statistical Decision Rules and Optimal Inference*. AMS Translations.\n",
    "\n",
    "**Information geometry**:\n",
    "- **Amari, S.** (2016). *Information Geometry and Its Applications*. Springer. â€” The definitive modern treatment by the field's founder.\n",
    "- **Amari, S. & Nagaoka, H.** (2000). *Methods of Information Geometry*. AMS/Oxford. â€” More mathematical, covers Î±-connections.\n",
    "- **Nielsen, F.** (2020). \"An Elementary Introduction to Information Geometry.\" *Entropy* 22(10):1100. â€” Accessible overview.\n",
    "\n",
    "**Natural gradient in ML**:\n",
    "- **Amari, S.** (1998). \"Natural Gradient Works Efficiently in Learning.\" *Neural Computation* 10(2):251â€“276.\n",
    "- **Martens, J.** (2020). \"New Insights and Perspectives on the Natural Gradient Method.\" *JMLR* 21:1â€“76. â€” Excellent modern perspective.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "562d631e53e149fa9d6f8d681b1d189e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=1.0, continuous_update=False, description='Ïƒ:', max=3.0, min=0.3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e723c17b69743069ca026f102832005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Interactive demo: Fisher metric depends on where you are in parameter space\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def fisher_gaussian_viz(sigma=1.0):\n",
    "    \"\"\"Visualize Fisher metric ellipsoid for Gaussian at different Ïƒ values.\"\"\"\n",
    "    # Fisher matrix for N(Î¼, ÏƒÂ²) at given Ïƒ\n",
    "    F = np.array([[1/sigma**2, 0], [0, 2/sigma**2]])\n",
    "    \n",
    "    # Create ellipse (unit ball under Fisher metric)\n",
    "    theta = np.linspace(0, 2*np.pi, 100)\n",
    "    # Ellipse: F^(-1/2) @ unit circle\n",
    "    eigvals, eigvecs = np.linalg.eigh(F)\n",
    "    radii = 1.0 / np.sqrt(eigvals)\n",
    "    ellipse = eigvecs @ np.diag(radii) @ np.array([np.cos(theta), np.sin(theta)])\n",
    "    \n",
    "    fig = make_subplots(rows=1, cols=2, \n",
    "                        subplot_titles=[f'Fisher Metric at Ïƒ={sigma:.1f}', \n",
    "                                       'Gaussian Distribution'],\n",
    "                        column_widths=[0.5, 0.5])\n",
    "    \n",
    "    # Left: Fisher ellipse (unit ball)\n",
    "    fig.add_trace(go.Scatter(x=ellipse[0], y=ellipse[1], mode='lines',\n",
    "                            fill='toself', fillcolor='rgba(100,149,237,0.3)',\n",
    "                            line=dict(color='cornflowerblue', width=2),\n",
    "                            name='Unit ball'), row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=[0], y=[0], mode='markers',\n",
    "                            marker=dict(size=10, color='red'),\n",
    "                            name='Current Î¸'), row=1, col=1)\n",
    "    \n",
    "    # Right: The Gaussian itself\n",
    "    x = np.linspace(-5, 5, 200)\n",
    "    y = np.exp(-x**2 / (2*sigma**2)) / (sigma * np.sqrt(2*np.pi))\n",
    "    fig.add_trace(go.Scatter(x=x, y=y, mode='lines', line=dict(width=3, color='coral'),\n",
    "                            name=f'N(0, {sigma**2:.1f})'), row=1, col=2)\n",
    "    \n",
    "    fig.update_xaxes(title_text='Î”Î¼', range=[-3, 3], row=1, col=1)\n",
    "    fig.update_yaxes(title_text='Î”Ïƒ', range=[-3, 3], scaleanchor='x', row=1, col=1)\n",
    "    fig.update_xaxes(title_text='x', row=1, col=2)\n",
    "    fig.update_yaxes(title_text='p(x)', row=1, col=2)\n",
    "    \n",
    "    fig.update_layout(height=400, showlegend=False,\n",
    "                     title_text='<b>Fisher metric shrinks as Ïƒ grows</b><br>' + \n",
    "                               '<sub>Small Ïƒ â†’ changes are detectable â†’ large metric</sub>')\n",
    "    return fig\n",
    "\n",
    "# Interactive slider\n",
    "sigma_slider = widgets.FloatSlider(value=1.0, min=0.3, max=3.0, step=0.1, \n",
    "                                   description='Ïƒ:', continuous_update=False)\n",
    "output = widgets.interactive_output(lambda s: display(fisher_gaussian_viz(s).show()), \n",
    "                                   {'s': sigma_slider})\n",
    "display(sigma_slider, output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Vectors vs Covectors â€” The Fundamental Duality\n",
    "\n",
    "### The Tangent-Cotangent Duality\n",
    "\n",
    "At each point $p$ of a manifold $\\mathcal{M}$, there are **two** associated vector spaces:\n",
    "\n",
    "| Space | Name | Elements | Physical Example |\n",
    "|-------|------|----------|------------------|\n",
    "| $T_p\\mathcal{M}$ | Tangent space | Vectors (velocities) | Velocity of a particle |\n",
    "| $T^*_p\\mathcal{M}$ | Cotangent space | Covectors (1-forms) | Gradient of potential energy |\n",
    "\n",
    "These are **dual** spaces: $T^*_p\\mathcal{M} = (T_p\\mathcal{M})^*$.\n",
    "\n",
    "### Formal Definitions\n",
    "\n",
    "**Definition**: A *tangent vector* $v \\in T_p\\mathcal{M}$ is an equivalence class of curves through $p$, or equivalently, a derivation on smooth functions:\n",
    "$$v: C^\\infty(\\mathcal{M}) \\to \\mathbb{R}, \\quad v(fg) = v(f)g(p) + f(p)v(g)$$\n",
    "\n",
    "**Definition**: A *covector* (or *1-form*) $\\omega \\in T^*_p\\mathcal{M}$ is a linear functional:\n",
    "$$\\omega: T_p\\mathcal{M} \\to \\mathbb{R}$$\n",
    "\n",
    "### Index Notation and Transformation Laws\n",
    "\n",
    "We use the **Einstein summation convention**: repeated indices (one up, one down) are summed.\n",
    "\n",
    "**Contravariant vectors** (tangent vectors) have **upper indices**: $v^i$\n",
    "\n",
    "**Covariant vectors** (covectors) have **lower indices**: $\\omega_i$\n",
    "\n",
    "Under coordinate change $\\tilde{\\theta}^a = f^a(\\theta)$:\n",
    "\n",
    "**Vectors transform contravariantly**:\n",
    "$$\\tilde{v}^a = \\frac{\\partial \\tilde{\\theta}^a}{\\partial \\theta^i} v^i$$\n",
    "\n",
    "**Covectors transform covariantly**:\n",
    "$$\\tilde{\\omega}_a = \\frac{\\partial \\theta^i}{\\partial \\tilde{\\theta}^a} \\omega_i$$\n",
    "\n",
    "**Key insight**: The product $\\omega_i v^i$ is **invariant** â€” the Jacobians cancel:\n",
    "$$\\tilde{\\omega}_a \\tilde{v}^a = \\frac{\\partial \\theta^i}{\\partial \\tilde{\\theta}^a} \\omega_i \\cdot \\frac{\\partial \\tilde{\\theta}^a}{\\partial \\theta^j} v^j = \\delta^i_j \\omega_i v^j = \\omega_i v^i$$\n",
    "\n",
    "### The Gradient Is a Covector\n",
    "\n",
    "**Theorem**: For $f: \\mathcal{M} \\to \\mathbb{R}$, the gradient $df$ is naturally a **covector**, not a vector.\n",
    "\n",
    "**Proof**: The differential $df$ acts on tangent vectors:\n",
    "$$(df)_p(v) = v(f) = \\left.\\frac{d}{dt}\\right|_{t=0} f(\\gamma(t))$$\n",
    "where $\\gamma$ is any curve with $\\gamma(0) = p$, $\\dot{\\gamma}(0) = v$.\n",
    "\n",
    "In coordinates: $(df)_i = \\frac{\\partial f}{\\partial \\theta^i}$ â€” lower index!\n",
    "\n",
    "**Why this matters**: When we compute $\\nabla_\\theta L$ in deep learning, we get a **covector**. But parameter updates require a **vector** (direction to move).\n",
    "\n",
    "### The Musical Isomorphisms\n",
    "\n",
    "The metric tensor provides canonical isomorphisms between $T_p\\mathcal{M}$ and $T^*_p\\mathcal{M}$:\n",
    "\n",
    "**Flat** (â™­): Lower an index (vector â†’ covector)\n",
    "$$v^\\flat_i = g_{ij} v^j$$\n",
    "\n",
    "**Sharp** (â™¯): Raise an index (covector â†’ vector)\n",
    "$$\\omega^{\\sharp i} = g^{ij} \\omega_j$$\n",
    "\n",
    "These are **inverses**: $(v^\\flat)^\\sharp = v$ and $(\\omega^\\sharp)^\\flat = \\omega$.\n",
    "\n",
    "### Natural Gradient as Index Raising\n",
    "\n",
    "Standard gradient descent makes a **type error**:\n",
    "$$\\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta L \\quad \\text{(WRONG: subtracting covector from point)}$$\n",
    "\n",
    "Natural gradient **fixes** this by raising the index:\n",
    "$$\\theta_{t+1} = \\theta_t - \\eta \\, (\\nabla L)^\\sharp = \\theta_t - \\eta \\, F^{-1} \\nabla L$$\n",
    "\n",
    "This is the **geometrically correct** update: convert the gradient covector to a tangent vector using the metric (Fisher Information).\n",
    "\n",
    "### Why Euclidean Space Hides This\n",
    "\n",
    "In $\\mathbb{R}^n$ with standard basis and Euclidean metric $g_{ij} = \\delta_{ij}$:\n",
    "- $v^\\flat_i = \\delta_{ij} v^j = v_i$ (components unchanged)\n",
    "- Vectors and covectors have the same components\n",
    "\n",
    "This **coincidence** leads to \"metric blindness\" â€” confusing objects that are geometrically distinct. On curved manifolds, this confusion causes bugs.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“š Further Reading: Part 3\n",
    "\n",
    "**Mathematical foundations**:\n",
    "- **Burke, W.L.** (1985). *Applied Differential Geometry*. Cambridge. â€” Emphasizes the geometric distinction; beautifully written.\n",
    "- **Frankel, T.** (2011). *The Geometry of Physics*. Cambridge. â€” Connects to physics; covers differential forms.\n",
    "- **Schutz, B.** (1980). *Geometrical Methods of Mathematical Physics*. Cambridge. â€” Excellent index notation pedagogy.\n",
    "\n",
    "**The gradient as covector**:\n",
    "- **Baez, J. & Muniain, J.P.** (1994). *Gauge Fields, Knots and Gravity*. World Scientific. Ch. 1 gives beautiful exposition.\n",
    "- **Misner, Thorne, Wheeler** (1973). *Gravitation*. Freeman. â€” The \"telephone book\"; definitive on index gymnastics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "596d7a13186f4746b39a34551fa77d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatSlider(value=1.0, continuous_update=False, description='Stretch:', max=3.0, min=0.3), IntSâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "595588fffdbb417f91b17425d9c959c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Interactive demo: Vectors vs Covectors under metric transformation\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def vector_covector_viz(metric_stretch=1.0, metric_angle=0):\n",
    "    \"\"\"Show how vectors and covectors transform differently under a metric.\"\"\"\n",
    "    # Metric tensor (stretched ellipse)\n",
    "    angle_rad = np.radians(metric_angle)\n",
    "    R = np.array([[np.cos(angle_rad), -np.sin(angle_rad)],\n",
    "                  [np.sin(angle_rad), np.cos(angle_rad)]])\n",
    "    D = np.diag([1.0, metric_stretch])\n",
    "    g = R @ D @ D @ R.T  # g = R DÂ² R^T (SPD matrix)\n",
    "    g_inv = np.linalg.inv(g)\n",
    "    \n",
    "    # A covector (gradient) - fixed\n",
    "    omega = np.array([1.0, 0.5])\n",
    "    \n",
    "    # Convert to vector using metric: v = g^{-1} @ omega\n",
    "    v = g_inv @ omega\n",
    "    \n",
    "    # Create metric ellipse (unit ball)\n",
    "    theta = np.linspace(0, 2*np.pi, 100)\n",
    "    eigvals, eigvecs = np.linalg.eigh(g)\n",
    "    radii = 1.0 / np.sqrt(eigvals)\n",
    "    ellipse = eigvecs @ np.diag(radii) @ np.array([np.cos(theta), np.sin(theta)])\n",
    "    \n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=['Covector (gradient) Ï‰', 'Vector v = gâ»Â¹Ï‰'])\n",
    "    \n",
    "    # Left plot: covector as hyperplane level sets\n",
    "    for c in [-0.5, 0, 0.5, 1.0, 1.5]:\n",
    "        if abs(omega[1]) > 1e-6:\n",
    "            x_line = np.linspace(-2, 2, 100)\n",
    "            y_line = (c - omega[0]*x_line) / omega[1]\n",
    "            mask = np.abs(y_line) < 2\n",
    "            fig.add_trace(go.Scatter(x=x_line[mask], y=y_line[mask], mode='lines',\n",
    "                                    line=dict(color='lightblue', width=1),\n",
    "                                    showlegend=False), row=1, col=1)\n",
    "    \n",
    "    # Covector as arrow (actually represents direction of steepest ascent in Euclidean)\n",
    "    fig.add_trace(go.Scatter(x=[0, omega[0]], y=[0, omega[1]], mode='lines+markers',\n",
    "                            line=dict(color='blue', width=3),\n",
    "                            marker=dict(size=[8, 12], symbol=['circle', 'arrow'],\n",
    "                                       angleref='previous'),\n",
    "                            name='Covector Ï‰'), row=1, col=1)\n",
    "    \n",
    "    # Right plot: vector and metric ellipse\n",
    "    fig.add_trace(go.Scatter(x=ellipse[0], y=ellipse[1], mode='lines',\n",
    "                            fill='toself', fillcolor='rgba(200,200,200,0.3)',\n",
    "                            line=dict(color='gray', width=2, dash='dash'),\n",
    "                            name='Metric unit ball'), row=1, col=2)\n",
    "    fig.add_trace(go.Scatter(x=[0, v[0]], y=[0, v[1]], mode='lines+markers',\n",
    "                            line=dict(color='red', width=3),\n",
    "                            marker=dict(size=[8, 12], symbol=['circle', 'arrow'],\n",
    "                                       angleref='previous'),\n",
    "                            name='Vector v'), row=1, col=2)\n",
    "    \n",
    "    # Show both in right plot for comparison\n",
    "    fig.add_trace(go.Scatter(x=[0, omega[0]], y=[0, omega[1]], mode='lines',\n",
    "                            line=dict(color='blue', width=2, dash='dot'),\n",
    "                            name='Original Ï‰'), row=1, col=2)\n",
    "    \n",
    "    for col in [1, 2]:\n",
    "        fig.update_xaxes(range=[-2.5, 2.5], row=1, col=col)\n",
    "        fig.update_yaxes(range=[-2.5, 2.5], scaleanchor=f'x{col if col > 1 else \"\"}', row=1, col=col)\n",
    "    \n",
    "    fig.update_layout(height=400, \n",
    "                     title_text=f'<b>Metric: stretch={metric_stretch:.1f}, angle={metric_angle}Â°</b><br>' +\n",
    "                               '<sub>Blue: covector (gradient). Red: vector (update direction). ' +\n",
    "                               'They differ when metric â‰  identity!</sub>')\n",
    "    return fig\n",
    "\n",
    "# Interactive controls\n",
    "stretch_slider = widgets.FloatSlider(value=1.0, min=0.3, max=3.0, step=0.1,\n",
    "                                    description='Stretch:', continuous_update=False)\n",
    "angle_slider = widgets.IntSlider(value=0, min=-45, max=45, step=5,\n",
    "                                description='Angle:', continuous_update=False)\n",
    "\n",
    "def update(stretch, angle):\n",
    "    display(vector_covector_viz(stretch, angle).show())\n",
    "\n",
    "out = widgets.interactive_output(update, {'stretch': stretch_slider, 'angle': angle_slider})\n",
    "display(widgets.HBox([stretch_slider, angle_slider]), out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: The SPD Manifold â€” Geometry of Covariance Matrices\n",
    "\n",
    "### The Space $\\mathcal{P}_n$ of SPD Matrices\n",
    "\n",
    "**Definition**: The *symmetric positive definite (SPD) manifold* is:\n",
    "$$\\mathcal{P}_n = \\{P \\in \\mathbb{R}^{n \\times n} : P = P^T, \\; v^T P v > 0 \\; \\forall v \\neq 0\\}$$\n",
    "\n",
    "**Topology**: $\\mathcal{P}_n$ is an **open convex cone** in the vector space $\\text{Sym}(n)$ of symmetric matrices:\n",
    "- Open: Small perturbations preserve positive definiteness\n",
    "- Convex: $\\alpha P + (1-\\alpha) Q \\in \\mathcal{P}_n$ for $P, Q \\in \\mathcal{P}_n$, $\\alpha \\in (0,1)$\n",
    "- Cone: $\\lambda P \\in \\mathcal{P}_n$ for $\\lambda > 0$\n",
    "\n",
    "**Dimension**: $\\dim(\\mathcal{P}_n) = \\dim(\\text{Sym}(n)) = \\frac{n(n+1)}{2}$\n",
    "\n",
    "### The Tangent Space\n",
    "\n",
    "At $P \\in \\mathcal{P}_n$, the tangent space is the space of symmetric matrices:\n",
    "$$T_P \\mathcal{P}_n \\cong \\text{Sym}(n) = \\{V \\in \\mathbb{R}^{n \\times n} : V = V^T\\}$$\n",
    "\n",
    "### The Affine-Invariant (AI) Metric\n",
    "\n",
    "**Definition** [Pennec et al., 2006]: The *affine-invariant Riemannian metric* at $P \\in \\mathcal{P}_n$:\n",
    "\n",
    "$$\\langle V, W \\rangle_P = \\text{tr}(P^{-1} V P^{-1} W)$$\n",
    "\n",
    "**Theorem** [Skovgaard, 1984]: This metric has the following properties:\n",
    "\n",
    "1. **GL(n)-invariance**: For any invertible $A \\in GL(n)$:\n",
    "   $$d(APA^T, AQA^T) = d(P, Q)$$\n",
    "   \n",
    "2. **Inversion invariance**: $d(P^{-1}, Q^{-1}) = d(P, Q)$\n",
    "\n",
    "3. **Geodesic completeness**: Geodesics extend for all time\n",
    "\n",
    "4. **Non-positive sectional curvature**: $\\mathcal{P}_n$ is a **Hadamard manifold** (simply connected, non-positive curvature)\n",
    "\n",
    "**Why GL(n)-invariance matters**: If we change coordinates by $\\tilde{x} = Ax$, covariances transform as $\\tilde{\\Sigma} = A\\Sigma A^T$. The AI metric is **blind to this change** â€” the geometry is intrinsic.\n",
    "\n",
    "### Geodesics: Closed-Form Solution\n",
    "\n",
    "**Theorem**: The geodesic from $P$ to $Q$ is:\n",
    "$$\\gamma(t) = P^{1/2} \\left(P^{-1/2} Q P^{-1/2}\\right)^t P^{1/2}$$\n",
    "\n",
    "where $M^t$ for SPD $M$ means: diagonalize $M = U \\Lambda U^T$, then $M^t = U \\Lambda^t U^T$.\n",
    "\n",
    "**Proof sketch**: Verify that $\\gamma$ satisfies the geodesic equation $\\nabla_{\\dot{\\gamma}} \\dot{\\gamma} = 0$ where $\\nabla$ is the Levi-Civita connection of the AI metric.\n",
    "\n",
    "**Special cases**:\n",
    "- $\\gamma(0) = P$\n",
    "- $\\gamma(1) = Q$\n",
    "- $\\gamma(1/2)$ = **geometric mean** of $P$ and $Q$\n",
    "\n",
    "### The Geodesic Distance\n",
    "\n",
    "**Theorem**: The Riemannian distance is:\n",
    "$$d(P, Q) = \\left\\| \\log(P^{-1/2} Q P^{-1/2}) \\right\\|_F = \\sqrt{\\sum_{i=1}^n \\log^2 \\lambda_i}$$\n",
    "\n",
    "where $\\lambda_1, \\ldots, \\lambda_n$ are the eigenvalues of $P^{-1}Q$ (equivalently, the generalized eigenvalues of $(Q, P)$).\n",
    "\n",
    "**Properties**:\n",
    "- $d(P, Q) = 0 \\Leftrightarrow P = Q$\n",
    "- $d(P, Q) = d(Q, P)$ (symmetry)\n",
    "- $d(P, R) \\leq d(P, Q) + d(Q, R)$ (triangle inequality)\n",
    "\n",
    "### The Swelling Effect: Why Euclidean is Wrong\n",
    "\n",
    "**Theorem** [Arsigny et al., 2007]: The Euclidean mean of SPD matrices **swells**:\n",
    "$$\\det\\left(\\frac{1}{N}\\sum_{i=1}^N P_i\\right) \\geq \\left(\\prod_{i=1}^N \\det(P_i)\\right)^{1/N}$$\n",
    "\n",
    "with equality iff all $P_i$ are equal.\n",
    "\n",
    "**The FrÃ©chet mean** (Riemannian center of mass) avoids this:\n",
    "$$\\bar{P} = \\arg\\min_{P \\in \\mathcal{P}_n} \\sum_{i=1}^N d(P, P_i)^2$$\n",
    "\n",
    "For $N=2$: $\\bar{P} = \\gamma(1/2)$ = geometric mean.\n",
    "\n",
    "### Connection to Fisher Information\n",
    "\n",
    "**Theorem** [Skovgaard, 1984]: For the centered Gaussian family $\\mathcal{N}(0, \\Sigma)$, the Fisher Information metric on $\\Sigma$ equals the affine-invariant metric on $\\mathcal{P}_n$ (up to a factor of 2).\n",
    "\n",
    "**Proof**: For $p(x|\\Sigma) = \\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}} \\exp(-\\frac{1}{2}x^T\\Sigma^{-1}x)$:\n",
    "\n",
    "$$\\log p = -\\frac{n}{2}\\log(2\\pi) - \\frac{1}{2}\\log|\\Sigma| - \\frac{1}{2}x^T\\Sigma^{-1}x$$\n",
    "\n",
    "The Fisher metric works out to:\n",
    "$$F(V, W) = \\frac{1}{2}\\text{tr}(\\Sigma^{-1} V \\Sigma^{-1} W)$$\n",
    "\n",
    "This is exactly (1/2 times) the AI metric!\n",
    "\n",
    "**Significance**: The \"natural\" Riemannian geometry of $\\mathcal{P}_n$ and the \"natural\" information geometry of Gaussian covariances are **the same**. This is not coincidence â€” it reflects deep structure.\n",
    "\n",
    "### Other Metrics on $\\mathcal{P}_n$\n",
    "\n",
    "| Metric | Formula | Properties |\n",
    "|--------|---------|------------|\n",
    "| **Affine-Invariant** | $\\langle V,W\\rangle_P = \\text{tr}(P^{-1}VP^{-1}W)$ | GL(n)-invariant, geodesically complete |\n",
    "| **Log-Euclidean** | $d(P,Q) = \\|\\log P - \\log Q\\|_F$ | Commutative, fast to compute |\n",
    "| **Bures-Wasserstein** | $d(P,Q)^2 = \\text{tr}(P) + \\text{tr}(Q) - 2\\text{tr}(P^{1/2}QP^{1/2})^{1/2}$ | Optimal transport metric |\n",
    "| **Power-Euclidean** | $d_\\alpha(P,Q) = \\|P^\\alpha - Q^\\alpha\\|_F$ | Interpolates AI (Î±â†’0) and Euclidean (Î±=1) |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“š Further Reading: Part 4\n",
    "\n",
    "**Foundational papers**:\n",
    "- **Pennec, X., Fillard, P., Ayache, N.** (2006). \"A Riemannian Framework for Tensor Computing.\" *IJCV* 66(1):41â€“66. â€” The foundational paper for DTI and BCI applications.\n",
    "- **Skovgaard, L.T.** (1984). \"A Riemannian Geometry of the Multivariate Normal Model.\" *Scand. J. Stat.* 11(4):211â€“223.\n",
    "- **Arsigny, V. et al.** (2007). \"Geometric Means in a Novel Vector Space Structure on SPD Matrices.\" *SIAM J. Matrix Anal.* 29(1):328â€“347.\n",
    "\n",
    "**Applications**:\n",
    "- **Barachant, A. et al.** (2012). \"Multiclass Brain-Computer Interface Classification by Riemannian Geometry.\" *IEEE Trans. Biomed. Eng.* 59(4):920â€“928. â€” BCI classification using SPD geometry.\n",
    "- **Yger, F. et al.** (2017). \"Riemannian Approaches in Brain-Computer Interfaces: A Review.\" *IEEE Trans. NSRE* 25(10):1753â€“1762.\n",
    "\n",
    "**Textbooks**:\n",
    "- **Bhatia, R.** (2007). *Positive Definite Matrices*. Princeton. â€” Comprehensive mathematical treatment.\n",
    "- **Bridson, M. & Haefliger, A.** (1999). *Metric Spaces of Non-Positive Curvature*. Springer. â€” For Hadamard manifold theory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7f32037f68f4e56bb03a52022f58dbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.5, continuous_update=False, description='t:', max=1.0, step=0.05)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a9c8581bad84c2881ee2f951567721f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Interactive demo: SPD Geodesics and the \"swelling effect\"\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def spd_geodesic_viz(t=0.5, show_swelling=True):\n",
    "    \"\"\"Visualize geodesic interpolation on SPD manifold vs Euclidean.\"\"\"\n",
    "    # Two 2x2 SPD matrices (displayed as ellipses)\n",
    "    P = np.array([[2.0, 0.5], [0.5, 1.0]])\n",
    "    Q = np.array([[1.0, -0.3], [-0.3, 2.5]])\n",
    "    \n",
    "    # Euclidean interpolation (WRONG for SPD)\n",
    "    M_euclidean = (1-t) * P + t * Q\n",
    "    \n",
    "    # Riemannian geodesic (CORRECT)\n",
    "    P_sqrt = np.linalg.cholesky(P)\n",
    "    P_sqrt_inv = np.linalg.inv(P_sqrt)\n",
    "    M = P_sqrt_inv @ Q @ P_sqrt_inv.T\n",
    "    eigvals, eigvecs = np.linalg.eigh(M)\n",
    "    M_t = eigvecs @ np.diag(eigvals**t) @ eigvecs.T\n",
    "    M_riemannian = P_sqrt @ M_t @ P_sqrt.T\n",
    "    \n",
    "    def make_ellipse(matrix, n_points=100):\n",
    "        theta = np.linspace(0, 2*np.pi, n_points)\n",
    "        circle = np.array([np.cos(theta), np.sin(theta)])\n",
    "        L = np.linalg.cholesky(matrix)\n",
    "        return L @ circle\n",
    "    \n",
    "    fig = make_subplots(rows=1, cols=2, \n",
    "                       subplot_titles=['Euclidean Interpolation (Wrong)', \n",
    "                                      'Riemannian Geodesic (Correct)'])\n",
    "    \n",
    "    colors = {'P': 'blue', 'Q': 'red', 'interp': 'green'}\n",
    "    \n",
    "    for col, (M_interp, title) in enumerate([(M_euclidean, 'Euclidean'), \n",
    "                                              (M_riemannian, 'Riemannian')], 1):\n",
    "        # Start ellipse\n",
    "        e_P = make_ellipse(P)\n",
    "        fig.add_trace(go.Scatter(x=e_P[0], y=e_P[1], mode='lines',\n",
    "                                line=dict(color=colors['P'], width=2),\n",
    "                                name='P (start)' if col==1 else None,\n",
    "                                showlegend=(col==1)), row=1, col=col)\n",
    "        \n",
    "        # End ellipse\n",
    "        e_Q = make_ellipse(Q)\n",
    "        fig.add_trace(go.Scatter(x=e_Q[0], y=e_Q[1], mode='lines',\n",
    "                                line=dict(color=colors['Q'], width=2),\n",
    "                                name='Q (end)' if col==1 else None,\n",
    "                                showlegend=(col==1)), row=1, col=col)\n",
    "        \n",
    "        # Interpolated ellipse\n",
    "        e_M = make_ellipse(M_interp)\n",
    "        fig.add_trace(go.Scatter(x=e_M[0], y=e_M[1], mode='lines',\n",
    "                                line=dict(color=colors['interp'], width=3),\n",
    "                                fill='toself', fillcolor='rgba(0,255,0,0.2)',\n",
    "                                name=f'Interpolated (t={t:.1f})' if col==1 else None,\n",
    "                                showlegend=(col==1)), row=1, col=col)\n",
    "    \n",
    "    # Show determinant (measure of \"size\") \n",
    "    det_P, det_Q = np.linalg.det(P), np.linalg.det(Q)\n",
    "    det_euc, det_riem = np.linalg.det(M_euclidean), np.linalg.det(M_riemannian)\n",
    "    \n",
    "    for col in [1, 2]:\n",
    "        fig.update_xaxes(range=[-3, 3], row=1, col=col)\n",
    "        fig.update_yaxes(range=[-3, 3], scaleanchor=f'x{col if col > 1 else \"\"}', row=1, col=col)\n",
    "    \n",
    "    swelling_text = (f'<br><sub>det(P)={det_P:.2f}, det(Q)={det_Q:.2f} | '\n",
    "                    f'Euclidean det={det_euc:.2f}, Riemannian det={det_riem:.2f}</sub>')\n",
    "    \n",
    "    fig.update_layout(height=450, \n",
    "                     title_text=f'<b>SPD Geodesic at t={t:.1f}</b>' + swelling_text +\n",
    "                               '<br><sub>Euclidean interpolation \"swells\" (det increases), '\n",
    "                               'Riemannian stays geometrically consistent</sub>')\n",
    "    return fig\n",
    "\n",
    "# Interactive slider\n",
    "t_slider = widgets.FloatSlider(value=0.5, min=0.0, max=1.0, step=0.05,\n",
    "                              description='t:', continuous_update=False)\n",
    "out = widgets.interactive_output(lambda t: display(spd_geodesic_viz(t).show()), {'t': t_slider})\n",
    "display(t_slider, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Finsler Geometry â€” Beyond Symmetric Distances\n",
    "\n",
    "### Motivation: Asymmetry in Nature and Statistics\n",
    "\n",
    "Riemannian geometry assumes **symmetric** distances: $d(p, q) = d(q, p)$.\n",
    "\n",
    "But asymmetry is ubiquitous:\n",
    "- **KL divergence**: $D_{KL}(P \\| Q) \\neq D_{KL}(Q \\| P)$\n",
    "- **Directed graphs**: Edge $A \\to B$ doesn't imply $B \\to A$\n",
    "- **Thermodynamics**: Entropy increases; time has an arrow\n",
    "- **Biology**: Predator-prey dynamics, infection spread\n",
    "- **Economics**: Transaction costs differ by direction\n",
    "\n",
    "### Finsler Manifolds: Formal Definition\n",
    "\n",
    "**Definition** [Finsler, 1918]: A *Finsler structure* on manifold $M$ is a function $F: TM \\to [0, \\infty)$ such that:\n",
    "\n",
    "1. **Smoothness**: $F$ is $C^\\infty$ on $TM \\setminus \\{0\\}$\n",
    "\n",
    "2. **Positive homogeneity**: $F(x, \\lambda v) = \\lambda F(x, v)$ for $\\lambda > 0$\n",
    "\n",
    "3. **Strong convexity**: The **fundamental tensor**\n",
    "   $$g_{ij}(x, v) = \\frac{1}{2} \\frac{\\partial^2 F^2}{\\partial v^i \\partial v^j}$$\n",
    "   is positive definite for $v \\neq 0$\n",
    "\n",
    "**Key difference from Riemannian**: The metric $g_{ij}$ depends on **both position and direction**!\n",
    "\n",
    "### The Indicatrix (Unit Ball)\n",
    "\n",
    "**Definition**: The *indicatrix* at $x$ is:\n",
    "$$\\Sigma_x = \\{v \\in T_x M : F(x, v) = 1\\}$$\n",
    "\n",
    "- **Riemannian case**: $\\Sigma_x$ is an ellipsoid centered at origin\n",
    "- **Finsler case**: $\\Sigma_x$ can be any smooth, strongly convex body\n",
    "\n",
    "**Physical interpretation**: $\\Sigma_x$ is the set of velocities you can achieve in unit time from $x$.\n",
    "\n",
    "### The Randers Metric\n",
    "\n",
    "**Definition** [Randers, 1941]: The *Randers metric* is:\n",
    "$$F(x, v) = \\alpha(x, v) + \\beta(x, v)$$\n",
    "\n",
    "where:\n",
    "- $\\alpha(x, v) = \\sqrt{a_{ij}(x) v^i v^j}$ is a Riemannian norm\n",
    "- $\\beta(x, v) = b_i(x) v^i$ is a 1-form (drift)\n",
    "\n",
    "**Explicit form**:\n",
    "$$F(v) = \\sqrt{v^T A v} + b^T v$$\n",
    "\n",
    "**Validity condition**: $\\|b\\|_\\alpha = \\sqrt{b^T A^{-1} b} < 1$ (otherwise $F$ can be negative)\n",
    "\n",
    "### Geometric Interpretation: Zermelo Navigation\n",
    "\n",
    "**Theorem** [Zermelo, 1931]: A Randers metric models **navigation in a wind field**.\n",
    "\n",
    "- $\\alpha$: Your walking/swimming speed (isotropic)\n",
    "- $\\beta$: Ambient drift (wind, current)\n",
    "- $F(v)$: Effective cost to move in direction $v$\n",
    "\n",
    "**Going with the wind**: $F(v)$ is small (cheap)\n",
    "**Going against the wind**: $F(-v)$ is large (expensive)\n",
    "\n",
    "**Asymmetry ratio**:\n",
    "$$\\frac{F(v)}{F(-v)} = \\frac{\\alpha + \\beta}{\\alpha - \\beta}$$\n",
    "\n",
    "### The Fundamental Tensor\n",
    "\n",
    "For Randers metric $F = \\alpha + \\beta$, the fundamental tensor is:\n",
    "\n",
    "$$g_{ij}(v) = \\frac{\\alpha}{F} a_{ij} + \\frac{\\beta}{F} \\frac{\\partial \\alpha}{\\partial v^i} \\frac{\\partial \\alpha}{\\partial v^j} - \\frac{\\alpha}{F^2} \\left( \\frac{\\partial \\alpha}{\\partial v^i} b_j + \\frac{\\partial \\alpha}{\\partial v^j} b_i \\right) + \\frac{b_i b_j}{F}$$\n",
    "\n",
    "**Key point**: $g_{ij}$ depends on $v$, unlike Riemannian metrics.\n",
    "\n",
    "### Geodesics in Finsler Geometry\n",
    "\n",
    "Finsler geodesics minimize arc length:\n",
    "$$\\text{Length}(\\gamma) = \\int_0^1 F(\\gamma(t), \\dot{\\gamma}(t)) \\, dt$$\n",
    "\n",
    "The geodesic equation involves the **Chern connection** (or Cartan connection):\n",
    "$$\\ddot{\\gamma}^k + 2 G^k(\\gamma, \\dot{\\gamma}) = 0$$\n",
    "\n",
    "where $G^k$ are the **geodesic spray coefficients**.\n",
    "\n",
    "**For Randers**: Geodesics are related to Riemannian geodesics via a \"navigation\" transformation [Robles, 2007].\n",
    "\n",
    "### Connection to Information Geometry\n",
    "\n",
    "**Theorem** [Shen, 2003]: The KL divergence can be locally approximated as:\n",
    "$$D_{KL}(p_\\theta \\| p_{\\theta + v}) = \\frac{1}{2} g_{ij}(\\theta) v^i v^j + \\frac{1}{6} T_{ijk}(\\theta) v^i v^j v^k + O(\\|v\\|^4)$$\n",
    "\n",
    "where:\n",
    "- $g_{ij}$ = Fisher metric (symmetric, Riemannian)\n",
    "- $T_{ijk}$ = **skewness tensor** (captures asymmetry)\n",
    "\n",
    "**The cubic term** $T_{ijk}$ is what makes KL asymmetric. To capture this faithfully requires Finsler geometry.\n",
    "\n",
    "### Why Finsler for Machine Learning?\n",
    "\n",
    "1. **Directed distances**: Model causal or irreversible relationships\n",
    "2. **Asymmetric costs**: Different costs for moving forward vs. backward in parameter space\n",
    "3. **Information asymmetry**: Better model for KL-based objectives\n",
    "4. **Physical constraints**: Navigation problems with drift\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“š Further Reading: Part 5\n",
    "\n",
    "**Foundational texts**:\n",
    "- **Bao, D., Chern, S.-S., Shen, Z.** (2000). *An Introduction to Riemann-Finsler Geometry*. Springer. â€” The definitive graduate text.\n",
    "- **Shen, Z.** (2001). *Differential Geometry of Spray and Finsler Spaces*. Kluwer.\n",
    "- **Randers, G.** (1941). \"On an Asymmetrical Metric in the Four-Space of General Relativity.\" *Phys. Rev.* 59:195â€“199.\n",
    "\n",
    "**Navigation and Zermelo**:\n",
    "- **Zermelo, E.** (1931). \"Ãœber das Navigationsproblem bei ruhender oder verÃ¤nderlicher Windverteilung.\" *ZAMM* 11:114â€“124.\n",
    "- **Robles, C.** (2007). \"Geodesics in Randers Spaces of Constant Curvature.\" *Trans. AMS* 359(4):1633â€“1651.\n",
    "\n",
    "**Information-theoretic connections**:\n",
    "- **Shen, Z.** (2003). \"Finsler Metrics with K = 0 and S = 0.\" *Canadian J. Math.* 55(1):112â€“132.\n",
    "- **Ohta, S.** (2009). \"Finsler Interpolation Inequalities.\" *Calc. Var.* 36:211â€“249.\n",
    "\n",
    "**Applications**:\n",
    "- **Astola, L. & Florack, L.** (2011). \"Finsler Geometry on Higher Order Tensor Fields and Applications to High Angular Resolution Diffusion Imaging.\" *IJCV* 92:325â€“336. â€” Finsler in medical imaging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877027b9a19e404a9fc7bcbbd3035d4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatSlider(value=0.3, continuous_update=False, description='Drift x:', max=0.8, min=-0.8, stepâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6445129b4414024abdacdf9a50ff4dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Interactive demo: Finsler (Randers) metric asymmetry\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def randers_viz(drift_x=0.3, drift_y=0.0):\n",
    "    \"\"\"Visualize Randers indicatrix (unit ball) showing asymmetry.\"\"\"\n",
    "    A = np.eye(2)  # Riemannian part = identity\n",
    "    b = np.array([drift_x, drift_y])\n",
    "    \n",
    "    # Check validity: ||b||_A < 1\n",
    "    b_norm = np.sqrt(b @ np.linalg.solve(A, b))\n",
    "    if b_norm >= 0.99:\n",
    "        b = b * 0.95 / b_norm  # Clamp to valid range\n",
    "    \n",
    "    # Randers norm: F(v) = sqrt(v'Av) + b'v\n",
    "    def randers_norm(v):\n",
    "        return np.sqrt(v @ A @ v) + b @ v\n",
    "    \n",
    "    # Sample unit vectors and compute F(v) for each\n",
    "    theta = np.linspace(0, 2*np.pi, 200)\n",
    "    directions = np.array([np.cos(theta), np.sin(theta)]).T\n",
    "    \n",
    "    # Randers indicatrix: F(v) = 1 surface\n",
    "    # For each direction, find radius r such that F(r*d) = 1\n",
    "    indicatrix = []\n",
    "    for d in directions:\n",
    "        # F(r*d) = r*sqrt(d'Ad) + r*(b'd) = r*(alpha + beta) = 1\n",
    "        # where alpha = ||d||_A, beta = b'd\n",
    "        alpha = np.sqrt(d @ A @ d)\n",
    "        beta = b @ d\n",
    "        if alpha + beta > 0:\n",
    "            r = 1.0 / (alpha + beta)\n",
    "            indicatrix.append(r * d)\n",
    "        else:\n",
    "            indicatrix.append([np.nan, np.nan])\n",
    "    indicatrix = np.array(indicatrix)\n",
    "    \n",
    "    # Riemannian unit ball (for comparison)\n",
    "    riemannian_ball = np.array([np.cos(theta), np.sin(theta)]).T\n",
    "    \n",
    "    fig = make_subplots(rows=1, cols=2, \n",
    "                       subplot_titles=['Indicatrix (Unit Ball)', \n",
    "                                      'Distance Asymmetry: d(O,P) vs d(P,O)'])\n",
    "    \n",
    "    # Left: indicatrix\n",
    "    fig.add_trace(go.Scatter(x=riemannian_ball[:,0], y=riemannian_ball[:,1], \n",
    "                            mode='lines', line=dict(color='gray', dash='dash', width=2),\n",
    "                            name='Riemannian (symmetric)'), row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=indicatrix[:,0], y=indicatrix[:,1],\n",
    "                            mode='lines', line=dict(color='crimson', width=3),\n",
    "                            fill='toself', fillcolor='rgba(220,20,60,0.2)',\n",
    "                            name='Randers (asymmetric)'), row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=[0, b[0]*2], y=[0, b[1]*2], mode='lines+markers',\n",
    "                            line=dict(color='blue', width=2),\n",
    "                            marker=dict(size=[8, 12], symbol=['circle', 'arrow-up'],\n",
    "                                       angleref='previous'),\n",
    "                            name='Drift vector b'), row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=[0], y=[0], mode='markers',\n",
    "                            marker=dict(size=10, color='black'), \n",
    "                            name='Origin'), row=1, col=1)\n",
    "    \n",
    "    # Right: distance comparison heatmap\n",
    "    # Show d(0, P) - d(P, 0) for points P on a grid\n",
    "    grid_size = 30\n",
    "    x_grid = np.linspace(-1.5, 1.5, grid_size)\n",
    "    y_grid = np.linspace(-1.5, 1.5, grid_size)\n",
    "    X, Y = np.meshgrid(x_grid, y_grid)\n",
    "    \n",
    "    asymmetry = np.zeros_like(X)\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            P = np.array([X[i,j], Y[i,j]])\n",
    "            if np.linalg.norm(P) > 0.01:\n",
    "                d_forward = randers_norm(P)  # d(0, P) â‰ˆ F(P) for small P\n",
    "                d_backward = randers_norm(-P)  # d(P, 0) â‰ˆ F(-P)\n",
    "                asymmetry[i,j] = (d_forward - d_backward) / (d_forward + d_backward + 1e-8)\n",
    "            else:\n",
    "                asymmetry[i,j] = 0\n",
    "    \n",
    "    fig.add_trace(go.Heatmap(x=x_grid, y=y_grid, z=asymmetry,\n",
    "                            colorscale='RdBu', zmid=0,\n",
    "                            colorbar=dict(title='Asymmetry', x=1.02),\n",
    "                            name='Asymmetry'), row=1, col=2)\n",
    "    fig.add_trace(go.Scatter(x=[0, b[0]*2], y=[0, b[1]*2], mode='lines+markers',\n",
    "                            line=dict(color='black', width=2),\n",
    "                            marker=dict(size=[6, 10]),\n",
    "                            showlegend=False), row=1, col=2)\n",
    "    \n",
    "    for col in [1, 2]:\n",
    "        fig.update_xaxes(range=[-2, 2], row=1, col=col)\n",
    "        fig.update_yaxes(range=[-2, 2], scaleanchor=f'x{col if col > 1 else \"\"}', row=1, col=col)\n",
    "    \n",
    "    fig.update_layout(height=450,\n",
    "                     title_text=f'<b>Randers Metric: b=({drift_x:.2f}, {drift_y:.2f})</b><br>' +\n",
    "                               '<sub>Going \"with the wind\" (drift direction) is cheaper than against it</sub>')\n",
    "    return fig\n",
    "\n",
    "# Interactive controls\n",
    "dx_slider = widgets.FloatSlider(value=0.3, min=-0.8, max=0.8, step=0.05,\n",
    "                               description='Drift x:', continuous_update=False)\n",
    "dy_slider = widgets.FloatSlider(value=0.0, min=-0.8, max=0.8, step=0.05,\n",
    "                               description='Drift y:', continuous_update=False)\n",
    "\n",
    "def update(dx, dy):\n",
    "    display(randers_viz(dx, dy).show())\n",
    "\n",
    "out = widgets.interactive_output(update, {'dx': dx_slider, 'dy': dy_slider})\n",
    "display(widgets.HBox([dx_slider, dy_slider]), out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Dually Flat Manifolds and Bregman Divergences\n",
    "\n",
    "### Beyond Metric: The Î±-Connection Family\n",
    "\n",
    "A Riemannian manifold has a unique torsion-free, metric-compatible connection (Levi-Civita). But statistical manifolds have a **family** of natural connections.\n",
    "\n",
    "**Definition** [Amari, 1985]: The *Î±-connection* $\\nabla^{(\\alpha)}$ for $\\alpha \\in \\mathbb{R}$ is defined by:\n",
    "$$\\Gamma^{(\\alpha)}_{ij,k} = \\mathbb{E}\\left[\\left(\\partial_i \\partial_j \\ell + \\frac{1-\\alpha}{2} \\partial_i \\ell \\, \\partial_j \\ell\\right) \\partial_k \\ell\\right]$$\n",
    "\n",
    "where $\\ell = \\log p(x|\\theta)$.\n",
    "\n",
    "**Special cases**:\n",
    "- $\\alpha = 0$: Levi-Civita connection (metric-compatible)\n",
    "- $\\alpha = 1$: **e-connection** $\\nabla^{(e)}$ (exponential)\n",
    "- $\\alpha = -1$: **m-connection** $\\nabla^{(m)}$ (mixture)\n",
    "\n",
    "### Duality: The Î± and -Î± Connections\n",
    "\n",
    "**Theorem** [Amari, 1985]: The $\\nabla^{(\\alpha)}$ and $\\nabla^{(-\\alpha)}$ connections are **dually coupled**:\n",
    "$$X \\cdot g(Y, Z) = g(\\nabla^{(\\alpha)}_X Y, Z) + g(Y, \\nabla^{(-\\alpha)}_X Z)$$\n",
    "\n",
    "This means: if you parallel transport along $\\nabla^{(\\alpha)}$ in one slot, you must use $\\nabla^{(-\\alpha)}$ in the other to preserve inner products.\n",
    "\n",
    "### Exponential Families: The Canonical Example\n",
    "\n",
    "**Definition**: An *exponential family* has the form:\n",
    "$$p(x|\\theta) = h(x) \\exp\\left(\\theta^i T_i(x) - \\psi(\\theta)\\right)$$\n",
    "\n",
    "where:\n",
    "- $\\theta = (\\theta^1, \\ldots, \\theta^n)$: **natural parameters**\n",
    "- $T(x) = (T_1(x), \\ldots, T_n(x))$: **sufficient statistics**\n",
    "- $\\psi(\\theta) = \\log \\int h(x) \\exp(\\theta \\cdot T(x)) dx$: **log-partition function** (cumulant generating function)\n",
    "\n",
    "### The Two Coordinate Systems\n",
    "\n",
    "**Natural coordinates** $\\theta$: The parameters in the exponential form.\n",
    "\n",
    "**Expectation coordinates** $\\eta$: The expected sufficient statistics:\n",
    "$$\\eta_i = \\mathbb{E}_{p_\\theta}[T_i(x)] = \\frac{\\partial \\psi}{\\partial \\theta^i}$$\n",
    "\n",
    "**Legendre duality**: The map $\\theta \\mapsto \\eta = \\nabla\\psi(\\theta)$ is a diffeomorphism (assuming minimal exponential family). The inverse is $\\eta \\mapsto \\theta = \\nabla\\phi(\\eta)$ where $\\phi(\\eta) = \\theta \\cdot \\eta - \\psi(\\theta)$ is the **conjugate potential**.\n",
    "\n",
    "### Dually Flat Structure\n",
    "\n",
    "**Theorem** [Amari & Nagaoka, 2000]: On an exponential family:\n",
    "\n",
    "1. The e-connection $\\nabla^{(e)}$ is **flat in $\\theta$ coordinates**: Christoffel symbols vanish\n",
    "2. The m-connection $\\nabla^{(m)}$ is **flat in $\\eta$ coordinates**: Christoffel symbols vanish\n",
    "3. The Fisher metric has the form:\n",
    "   $$g_{ij}(\\theta) = \\frac{\\partial^2 \\psi}{\\partial \\theta^i \\partial \\theta^j} = \\frac{\\partial \\eta_i}{\\partial \\theta^j}$$\n",
    "\n",
    "**Geodesics**:\n",
    "- e-geodesics: Straight lines in $\\theta$ coordinates\n",
    "- m-geodesics: Straight lines in $\\eta$ coordinates\n",
    "\n",
    "### Bregman Divergences\n",
    "\n",
    "**Definition** [Bregman, 1967]: For strictly convex $\\phi: \\Omega \\to \\mathbb{R}$:\n",
    "$$D_\\phi(p \\| q) = \\phi(p) - \\phi(q) - \\langle \\nabla\\phi(q), p - q \\rangle$$\n",
    "\n",
    "**Properties**:\n",
    "- $D_\\phi(p \\| q) \\geq 0$ (by convexity)\n",
    "- $D_\\phi(p \\| q) = 0 \\Leftrightarrow p = q$\n",
    "- Generally **asymmetric**: $D_\\phi(p \\| q) \\neq D_\\phi(q \\| p)$\n",
    "\n",
    "### The KL-Bregman Connection\n",
    "\n",
    "**Theorem**: For exponential families:\n",
    "$$D_{KL}(p_\\theta \\| p_{\\theta'}) = D_\\psi(\\theta' \\| \\theta) = D_\\phi(\\eta \\| \\eta')$$\n",
    "\n",
    "where $D_\\psi$ uses the log-partition function, and $D_\\phi$ uses the conjugate potential.\n",
    "\n",
    "**Proof**: Direct calculation using $D_{KL} = \\mathbb{E}_\\theta[\\log p_\\theta - \\log p_{\\theta'}]$ and the exponential family form.\n",
    "\n",
    "### Pythagorean Theorem\n",
    "\n",
    "**Theorem** [Amari, 1985]: If $r$ lies on the e-geodesic from $p$ to $q$, then:\n",
    "$$D_{KL}(p \\| q) = D_{KL}(p \\| r) + D_{KL}(r \\| q)$$\n",
    "\n",
    "Similarly for m-geodesics with arguments reversed.\n",
    "\n",
    "**Consequence**: Bregman projections onto e-flat or m-flat submanifolds satisfy a generalized Pythagorean relation.\n",
    "\n",
    "### Applications in Machine Learning\n",
    "\n",
    "| Algorithm | Geometric Interpretation |\n",
    "|-----------|-------------------------|\n",
    "| **EM algorithm** | Alternating e- and m-projections |\n",
    "| **Natural gradient** | Following e-geodesics |\n",
    "| **Mirror descent** | Following m-geodesics |\n",
    "| **Information projection** | m-projection onto constraint set |\n",
    "| **Maximum likelihood** | m-projection of empirical distribution |\n",
    "| **Maximum entropy** | e-projection onto moment constraints |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“š Further Reading: Part 6\n",
    "\n",
    "**Original sources**:\n",
    "- **Amari, S.** (1985). *Differential-Geometrical Methods in Statistics*. Springer Lecture Notes in Statistics 28.\n",
    "- **Bregman, L.M.** (1967). \"The Relaxation Method of Finding the Common Point of Convex Sets.\" *USSR Computational Math. & Math. Physics* 7(3):200â€“217.\n",
    "\n",
    "**Comprehensive treatments**:\n",
    "- **Amari, S. & Nagaoka, H.** (2000). *Methods of Information Geometry*. AMS/Oxford. â€” The definitive mathematical treatment of dually flat structures.\n",
    "- **Nielsen, F. & Nock, R.** (2009). \"Sided and Symmetrized Bregman Centroids.\" *IEEE Trans. IT* 55(6):2882â€“2904.\n",
    "\n",
    "**Machine learning connections**:\n",
    "- **Banerjee, A. et al.** (2005). \"Clustering with Bregman Divergences.\" *JMLR* 6:1705â€“1749. â€” Bregman divergences unify many clustering algorithms.\n",
    "- **Raskutti, G. & Mukherjee, S.** (2015). \"The Information Geometry of Mirror Descent.\" *IEEE Trans. IT* 61(3):1451â€“1457.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: The Sloppy Model Phenomenon\n",
    "\n",
    "### The Universality of Sloppiness\n",
    "\n",
    "**Empirical observation** [Sethna et al., 2014]: In diverse complex models â€” systems biology, neural networks, materials science â€” the Fisher Information eigenvalues span **many orders of magnitude**:\n",
    "\n",
    "$$\\lambda_1 \\gg \\lambda_2 \\gg \\cdots \\gg \\lambda_n$$\n",
    "\n",
    "Often 6â€“10 orders of magnitude, with eigenvalues roughly evenly spaced on a log scale!\n",
    "\n",
    "### The Model Manifold\n",
    "\n",
    "**Definition**: The *model manifold* is the image of parameter space in prediction space:\n",
    "$$\\mathcal{M} = \\{y(\\theta) : \\theta \\in \\Theta\\} \\subset \\mathbb{R}^m$$\n",
    "\n",
    "where $y(\\theta)$ is the model's prediction at parameters $\\theta$.\n",
    "\n",
    "The Fisher Information $F_{ij} = \\sum_k \\frac{\\partial y_k}{\\partial \\theta^i} \\frac{\\partial y_k}{\\partial \\theta^j}$ is the **metric tensor** of $\\mathcal{M}$ embedded in prediction space.\n",
    "\n",
    "### The Hyper-Ribbon Geometry\n",
    "\n",
    "**Theorem** (informal) [Transtrum & Qiu, 2014]: Sloppy model manifolds have the geometry of a **hyper-ribbon**:\n",
    "- Bounded extent in most directions (width $\\sim O(1)$)\n",
    "- Exponentially varying widths along principal axes\n",
    "- Effectively low-dimensional despite high parameter count\n",
    "\n",
    "**Eigenvalue spectrum**: For sloppy models:\n",
    "$$\\lambda_k \\approx \\lambda_0 \\cdot \\exp(-k/k_0)$$\n",
    "\n",
    "The eigenvalues decay exponentially â€” this is **not** a power law but steeper.\n",
    "\n",
    "### Stiff vs Sloppy Parameter Combinations\n",
    "\n",
    "| Property | Stiff Directions | Sloppy Directions |\n",
    "|----------|------------------|-------------------|\n",
    "| Eigenvalue | Large $\\lambda$ | Small $\\lambda$ |\n",
    "| Effect on predictions | Large | Negligible |\n",
    "| Constrained by data | Strongly | Weakly |\n",
    "| Uncertainty | Small | Large |\n",
    "| Identifiability | Well-identified | Poorly identified |\n",
    "\n",
    "### The CramÃ©r-Rao Interpretation\n",
    "\n",
    "Recall the CramÃ©r-Rao bound: $\\text{Cov}(\\hat{\\theta}) \\geq F^{-1}$.\n",
    "\n",
    "- **Stiff directions**: Large $\\lambda$ â†’ small uncertainty â†’ well-estimated\n",
    "- **Sloppy directions**: Small $\\lambda$ â†’ large uncertainty â†’ poorly estimated\n",
    "\n",
    "**Key insight**: Sloppy directions are **statistically irrelevant** â€” the data doesn't constrain them, so we don't need to estimate them precisely.\n",
    "\n",
    "### Effective Dimensionality\n",
    "\n",
    "**Definition**: The *effective dimensionality* measures how many directions matter:\n",
    "$$d_{\\text{eff}} = \\frac{(\\text{tr } F)^2}{\\text{tr}(F^2)} = \\frac{(\\sum_i \\lambda_i)^2}{\\sum_i \\lambda_i^2}$$\n",
    "\n",
    "For uniform eigenvalues: $d_{\\text{eff}} = n$.\n",
    "For sloppy models: $d_{\\text{eff}} \\ll n$.\n",
    "\n",
    "**Alternative**: Count eigenvalues above a threshold (e.g., 1% of maximum).\n",
    "\n",
    "### Why Overparameterization Works\n",
    "\n",
    "**Paradox**: Neural networks have millions of parameters but generalize well. Classical theory says they should overfit.\n",
    "\n",
    "**Sloppy resolution**: Most parameter directions are sloppy:\n",
    "- They don't affect predictions much\n",
    "- The model is effectively low-dimensional\n",
    "- \"Degrees of freedom\" â‰ˆ $d_{\\text{eff}}$ â‰ª parameter count\n",
    "\n",
    "**Connection to double descent**: The transition from classical to modern ML regime occurs when stiff directions saturate.\n",
    "\n",
    "### Implications for Optimization\n",
    "\n",
    "**Natural gradient** adapts to sloppiness automatically:\n",
    "$$\\Delta\\theta = -F^{-1} \\nabla L$$\n",
    "\n",
    "- **Stiff directions** (large $\\lambda$): $F^{-1}$ is small â†’ small steps (already constrained)\n",
    "- **Sloppy directions** (small $\\lambda$): $F^{-1}$ is large â†’ large steps (need exploration)\n",
    "\n",
    "This is exactly the right behavior â€” focus on directions that matter!\n",
    "\n",
    "### Geometric Insight: Curvature and Information\n",
    "\n",
    "The eigenvalues of Fisher = principal curvatures of the model manifold.\n",
    "\n",
    "- **High curvature** (stiff): Small changes in parameters â†’ large changes in predictions\n",
    "- **Low curvature** (sloppy): Large changes in parameters â†’ predictions barely change\n",
    "\n",
    "The geometry **reveals the structure** of the learning problem.\n",
    "\n",
    "### Connection to Our Data\n",
    "\n",
    "In EEG covariance matrices from brain-computer interfaces:\n",
    "\n",
    "| Direction Type | Interpretation |\n",
    "|----------------|----------------|\n",
    "| **Stiff** | Channel combinations that reliably distinguish brain states (left vs right motor imagery) |\n",
    "| **Sloppy** | Channel combinations that vary randomly trial-to-trial (noise, artifacts) |\n",
    "\n",
    "The eigenvalue spectrum of the data covariance **reveals which features are informative** for classification â€” a geometric feature selection!\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“š Further Reading: Part 7\n",
    "\n",
    "**Foundational papers**:\n",
    "- **Brown, K.S. & Sethna, J.P.** (2003). \"Statistical Mechanical Approaches to Models with Many Poorly Known Parameters.\" *Phys. Rev. E* 68:021904.\n",
    "- **Transtrum, M.K. et al.** (2010). \"Perspective: Sloppiness and Emergent Theories in Physics, Biology, and Beyond.\" *J. Chem. Phys.* 143:010901.\n",
    "- **Machta, B.B. et al.** (2013). \"Parameter Space Compression Underlies Emergent Theories and Predictive Models.\" *Science* 342:604â€“607.\n",
    "\n",
    "**Geometry of sloppiness**:\n",
    "- **Transtrum, M.K. & Qiu, P.** (2014). \"Model Reduction by Manifold Boundaries.\" *Phys. Rev. Lett.* 113:098701.\n",
    "- **Quinn, K.N. et al.** (2019). \"Visualizing Probabilistic Models in Minkowski Space.\" *arXiv:1905.05072*.\n",
    "\n",
    "**Connections to deep learning**:\n",
    "- **Karakida, R. et al.** (2019). \"Universal Statistics of Fisher Information in Deep Neural Networks.\" *NeurIPS*.\n",
    "- **Fort, S. et al.** (2019). \"Emergent Properties of the Local Geometry of Neural Loss Landscapes.\" *arXiv:1910.05929*.\n",
    "- **Papyan, V.** (2020). \"Traces of Class/Cross-Class Structure Pervade Deep Learning Spectra.\" *JMLR* 21:1â€“64.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d77b9f18536945699f549c5ff00a4087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntSlider(value=20, continuous_update=False, description='# params:', max=50, min=5, step=5), Fâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef57bfe0898747579522a470df54be8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Interactive demo: Sloppy model eigenvalue spectrum\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def sloppy_model_viz(n_params=20, stiffness_ratio=4.0):\n",
    "    \"\"\"Visualize the 'sloppy model' eigenvalue spectrum phenomenon.\"\"\"\n",
    "    # Generate a Fisher-like matrix with exponentially decaying eigenvalues\n",
    "    # This mimics real sloppy models\n",
    "    eigenvalues = np.array([10**(stiffness_ratio * (1 - i/(n_params-1))) for i in range(n_params)])\n",
    "    \n",
    "    # Create random orthogonal matrix for eigenvectors\n",
    "    np.random.seed(42)  # Reproducible\n",
    "    Q, _ = np.linalg.qr(np.random.randn(n_params, n_params))\n",
    "    fisher = Q @ np.diag(eigenvalues) @ Q.T\n",
    "    \n",
    "    fig = make_subplots(rows=1, cols=2,\n",
    "                       subplot_titles=['Eigenvalue Spectrum (log scale)',\n",
    "                                      'Model Manifold Geometry (2D projection)'])\n",
    "    \n",
    "    # Left: eigenvalue spectrum\n",
    "    fig.add_trace(go.Scatter(x=list(range(1, n_params+1)), y=eigenvalues,\n",
    "                            mode='lines+markers',\n",
    "                            line=dict(color='steelblue', width=2),\n",
    "                            marker=dict(size=8),\n",
    "                            name='Eigenvalues'), row=1, col=1)\n",
    "    \n",
    "    # Mark stiff vs sloppy threshold (e.g., 1% of max)\n",
    "    threshold = eigenvalues[0] * 0.01\n",
    "    n_stiff = np.sum(eigenvalues > threshold)\n",
    "    fig.add_hline(y=threshold, line_dash='dash', line_color='red', row=1, col=1)\n",
    "    fig.add_annotation(x=n_params/2, y=threshold*2, text=f'{n_stiff} stiff, {n_params-n_stiff} sloppy',\n",
    "                      showarrow=False, row=1, col=1)\n",
    "    \n",
    "    fig.update_yaxes(type='log', title='Eigenvalue', row=1, col=1)\n",
    "    fig.update_xaxes(title='Parameter index', row=1, col=1)\n",
    "    \n",
    "    # Right: 2D projection showing hyper-ribbon shape\n",
    "    # Sample points on a \"model manifold\" with sloppy structure\n",
    "    n_samples = 500\n",
    "    coords = np.random.randn(n_samples, n_params)\n",
    "    # Scale by inverse sqrt of eigenvalues (sloppy directions extend further)\n",
    "    coords = coords / np.sqrt(eigenvalues)\n",
    "    \n",
    "    # Project to 2D using first two eigenvectors\n",
    "    proj_stiff = coords @ Q[:, 0]  # Stiff direction\n",
    "    proj_sloppy = coords @ Q[:, -1]  # Most sloppy direction\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=proj_sloppy, y=proj_stiff,\n",
    "                            mode='markers',\n",
    "                            marker=dict(size=4, color='coral', opacity=0.5),\n",
    "                            name='Model predictions'), row=1, col=2)\n",
    "    \n",
    "    fig.update_xaxes(title='Sloppy direction', row=1, col=2)\n",
    "    fig.update_yaxes(title='Stiff direction', row=1, col=2)\n",
    "    \n",
    "    span = np.log10(eigenvalues[0] / eigenvalues[-1])\n",
    "    fig.update_layout(height=450,\n",
    "                     title_text=f'<b>Sloppy Model: {n_params} parameters, {span:.1f} orders of magnitude</b><br>' +\n",
    "                               '<sub>The model manifold is a \"hyper-ribbon\" â€” flat in sloppy directions, curved in stiff</sub>')\n",
    "    return fig\n",
    "\n",
    "# Interactive controls\n",
    "n_params_slider = widgets.IntSlider(value=20, min=5, max=50, step=5,\n",
    "                                   description='# params:', continuous_update=False)\n",
    "ratio_slider = widgets.FloatSlider(value=4.0, min=1.0, max=8.0, step=0.5,\n",
    "                                  description='Log span:', continuous_update=False)\n",
    "\n",
    "def update(n, r):\n",
    "    display(sloppy_model_viz(n, r).show())\n",
    "\n",
    "out = widgets.interactive_output(update, {'n': n_params_slider, 'r': ratio_slider})\n",
    "display(widgets.HBox([n_params_slider, ratio_slider]), out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Library Design â€” How the Pieces Fit Together\n",
    "\n",
    "### The Conceptual Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                           RAW DATA                                          â”‚\n",
    "â”‚         (time series, images, graphs, covariance matrices)                  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                    â”‚\n",
    "                                    â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     GEOMETRY EXTRACTION                                     â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   DataGeometryExtractor: Fit probabilistic model â†’ Fisher metric emerges   â”‚\n",
    "â”‚   SPDGeometryExtractor: Covariance matrices â†’ Affine-invariant metric      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                    â”‚\n",
    "                                    â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     GEOMETRIC STRUCTURES                                    â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚\n",
    "â”‚   â”‚ MetricTensor â”‚    â”‚ SPDManifold  â”‚    â”‚ RandersMetricâ”‚                 â”‚\n",
    "â”‚   â”‚ (Riemannian) â”‚    â”‚ (Covariance) â”‚    â”‚ (Finsler)    â”‚                 â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚\n",
    "â”‚           â”‚                   â”‚                   â”‚                         â”‚\n",
    "â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚\n",
    "â”‚                               â”‚                                             â”‚\n",
    "â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚\n",
    "â”‚                    â”‚   FisherMetric      â”‚                                  â”‚\n",
    "â”‚                    â”‚   (Information)     â”‚                                  â”‚\n",
    "â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                    â”‚\n",
    "                                    â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     GEOMETRIC OPERATIONS                                    â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   â€¢ Geodesic computation (shortest paths)                                   â”‚\n",
    "â”‚   â€¢ Parallel transport (moving vectors along curves)                        â”‚\n",
    "â”‚   â€¢ FrÃ©chet mean (geometric average)                                        â”‚\n",
    "â”‚   â€¢ Natural gradient (geometry-aware optimization)                          â”‚\n",
    "â”‚   â€¢ Dualization (vector â†” covector conversion)                              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                    â”‚\n",
    "                                    â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     APPLICATIONS                                            â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   â€¢ Classification (Riemannian classifiers on SPD)                          â”‚\n",
    "â”‚   â€¢ Optimization (natural gradient descent)                                 â”‚\n",
    "â”‚   â€¢ Interpolation (geodesic interpolation)                                  â”‚\n",
    "â”‚   â€¢ Embedding (preserve geometric structure)                                â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Key Design Decisions\n",
    "\n",
    "| Decision | Rationale |\n",
    "|----------|-----------|\n",
    "| **Explicit tensor variance tracking** | Prevents mixing vectors/covectors (a common source of coordinate-dependent bugs) |\n",
    "| **Metric as first-class object** | All geometric operations flow from the metric; changing metric changes behavior consistently |\n",
    "| **Fisher = metric** | Fisher Information IS a Riemannian metric; we don't treat them as separate concepts |\n",
    "| **Finsler extension** | Real data often has asymmetry; Randers metrics handle this gracefully |\n",
    "| **Geometry extraction** | Users shouldn't need to hand-craft manifolds; learn geometry from data |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: The Geometric View of Information\n",
    "\n",
    "### The Core Ideas\n",
    "\n",
    "| # | Concept | Mathematical Statement | Practical Implication |\n",
    "|---|---------|----------------------|----------------------|\n",
    "| 1 | **Statistical manifold** | Distributions form manifold $\\mathcal{S}$ | Parameter spaces have intrinsic geometry |\n",
    "| 2 | **Fisher metric** | Unique metric satisfying ÄŒencov's axioms | Use $F^{-1}\\nabla L$ not $\\nabla L$ for updates |\n",
    "| 3 | **Vector â‰  Covector** | $\\nabla L \\in T^*$, updates $\\in T$ | Metric converts between them |\n",
    "| 4 | **SPD geometry** | $\\mathcal{P}_n$ with AI metric is Hadamard | Use geodesic mean, not arithmetic mean |\n",
    "| 5 | **Finsler for asymmetry** | $F(v) \\neq F(-v)$ allowed | Model KL divergence, directed graphs |\n",
    "| 6 | **Dual flatness** | $\\theta$ and $\\eta$ coordinates both flat | Bregman projections, EM algorithm |\n",
    "| 7 | **Sloppiness** | Eigenvalues span orders of magnitude | Effective dimension â‰ª parameter count |\n",
    "\n",
    "### The Unifying Theme: Covariance\n",
    "\n",
    "**All of these are aspects of COVARIANCE in the geometric sense:**\n",
    "\n",
    "| Aspect | How It Manifests |\n",
    "|--------|-----------------|\n",
    "| **Tensor covariance** | Objects transform consistently under coordinate changes |\n",
    "| **Statistical covariance** | Fisher metric measures parameter co-variation with predictions |\n",
    "| **Random variable covariance** | SPD manifold is the space of covariance matrices |\n",
    "| **Directional covariance** | Finsler metrics capture how forward/backward costs co-vary |\n",
    "\n",
    "### The Mathematical Chain\n",
    "\n",
    "$$\\text{Data} \\xrightarrow{\\text{model}} p(x|\\theta) \\xrightarrow{\\text{Fisher}} g_{ij}(\\theta) \\xrightarrow{\\text{geometry}} \\text{geodesics, means, gradients}$$\n",
    "\n",
    "Each arrow preserves structure when done correctly:\n",
    "- Model fitting respects data geometry\n",
    "- Fisher metric emerges from the model\n",
    "- Geometric operations respect the metric\n",
    "\n",
    "### Key Theorems Summary\n",
    "\n",
    "1. **ÄŒencov (1982)**: Fisher is the unique information-monotonic metric\n",
    "2. **CramÃ©r-Rao (1945)**: $\\text{Cov}(\\hat\\theta) \\geq F^{-1}$ â€” Fisher bounds estimation\n",
    "3. **Amari (1985)**: Exponential families are dually flat\n",
    "4. **Skovgaard (1984)**: Gaussian Fisher = SPD affine-invariant metric\n",
    "5. **Zermelo (1931)**: Randers metrics model navigation with drift\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Consolidated Bibliography\n",
    "\n",
    "### Foundational Texts\n",
    "\n",
    "**Differential Geometry**:\n",
    "- do Carmo, M.P. (1992). *Riemannian Geometry*. BirkhÃ¤user.\n",
    "- Lee, J.M. (2018). *Introduction to Riemannian Manifolds*. Springer.\n",
    "- Burke, W.L. (1985). *Applied Differential Geometry*. Cambridge.\n",
    "\n",
    "**Information Geometry**:\n",
    "- Amari, S. (2016). *Information Geometry and Its Applications*. Springer.\n",
    "- Amari, S. & Nagaoka, H. (2000). *Methods of Information Geometry*. AMS/Oxford.\n",
    "- Nielsen, F. (2020). \"An Elementary Introduction to Information Geometry.\" *Entropy* 22(10):1100.\n",
    "\n",
    "**SPD Manifolds**:\n",
    "- Bhatia, R. (2007). *Positive Definite Matrices*. Princeton.\n",
    "- Pennec, X. et al. (2006). \"A Riemannian Framework for Tensor Computing.\" *IJCV* 66(1):41â€“66.\n",
    "\n",
    "**Finsler Geometry**:\n",
    "- Bao, D., Chern, S.-S., Shen, Z. (2000). *An Introduction to Riemann-Finsler Geometry*. Springer.\n",
    "\n",
    "### Key Papers\n",
    "\n",
    "- Fisher, R.A. (1925). \"Theory of Statistical Estimation.\" *Proc. Cambridge Phil. Soc.* 22:700â€“725.\n",
    "- Rao, C.R. (1945). \"Information and accuracy attainable in estimation.\" *Bull. Calcutta Math. Soc.* 37:81â€“91.\n",
    "- ÄŒencov, N.N. (1982). *Statistical Decision Rules and Optimal Inference*. AMS.\n",
    "- Amari, S. (1998). \"Natural Gradient Works Efficiently in Learning.\" *Neural Computation* 10(2):251â€“276.\n",
    "- Martens, J. (2020). \"New Insights on the Natural Gradient Method.\" *JMLR* 21:1â€“76.\n",
    "- Transtrum, M.K. et al. (2015). \"Sloppiness and Emergent Theories.\" *J. Chem. Phys.* 143:010901.\n",
    "\n",
    "### Applications\n",
    "\n",
    "- Barachant, A. et al. (2012). \"Multiclass BCI Classification by Riemannian Geometry.\" *IEEE Trans. BME* 59(4):920â€“928.\n",
    "- Banerjee, A. et al. (2005). \"Clustering with Bregman Divergences.\" *JMLR* 6:1705â€“1749.\n",
    "- Absil, P.-A. et al. (2008). *Optimization Algorithms on Matrix Manifolds*. Princeton.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Appendix: Seeing It In Practice\n",
    "\n",
    "Let's see these concepts with actual data from the library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 496 covariance matrices\n",
      "Each matrix is 64Ã—64\n",
      "Labels: [1 2] (1=left fist, 2=right fist)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Load EEG data as SPD covariance matrices\n",
    "from data.loaders import PhysioNetEEGLoader\n",
    "\n",
    "loader = PhysioNetEEGLoader()\n",
    "dataset = loader.load_subject(\"S001\", runs=[3, 4])  # Left vs right fist\n",
    "\n",
    "print(f\"Loaded {dataset.n_samples} covariance matrices\")\n",
    "print(f\"Each matrix is {dataset.manifold_dim}Ã—{dataset.manifold_dim}\")\n",
    "print(f\"Labels: {np.unique(dataset.labels)} (1=left fist, 2=right fist)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rocm_plugin_extension not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean distance: 6.8211\n",
      "Riemannian distance: 11.2735\n",
      "\n",
      "Ratio: 1.65x\n",
      "\n",
      "The Riemannian distance accounts for the curved geometry of SPD space!\n"
     ]
    }
   ],
   "source": [
    "# The SPD Manifold in action\n",
    "from diffgeo.geometry import SPDManifold\n",
    "\n",
    "# Create manifold\n",
    "dim = dataset.manifold_dim\n",
    "manifold = SPDManifold(dim)\n",
    "\n",
    "# Take two covariance matrices\n",
    "P = dataset.matrices[0]  \n",
    "Q = dataset.matrices[1]\n",
    "\n",
    "# Euclidean distance (WRONG for covariances)\n",
    "d_euclidean = np.linalg.norm(P - Q, 'fro')\n",
    "\n",
    "# Riemannian distance (CORRECT)\n",
    "d_riemannian = manifold.distance(P, Q)\n",
    "\n",
    "print(f\"Euclidean distance: {d_euclidean:.4f}\")\n",
    "print(f\"Riemannian distance: {d_riemannian:.4f}\")\n",
    "print(f\"\\nRatio: {d_riemannian/d_euclidean:.2f}x\")\n",
    "print(\"\\nThe Riemannian distance accounts for the curved geometry of SPD space!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data covariance eigenvalue spectrum:\n",
      "  Largest:  1.68e+00\n",
      "  Smallest: 7.34e-07\n",
      "  Condition number: 2.29e+06\n",
      "\n",
      "Span: 6.4 orders of magnitude\n",
      "\n",
      "Effective dimensionality: ~5 of 36 dimensions\n",
      "\n",
      "This is the 'sloppy model' phenomenon â€” most directions are noise!\n"
     ]
    }
   ],
   "source": [
    "# Data geometry reveals stiff/sloppy directions\n",
    "# For raw data, the sample covariance captures the same geometric structure\n",
    "# (For a Gaussian model, Fisher = inverse covariance!)\n",
    "\n",
    "# Flatten covariance matrices to vectors and compute their sample covariance\n",
    "subset_matrices = dataset.matrices[:50, :8, :8]  # First 50 samples, 8Ã—8 submatrix\n",
    "data_vectors = subset_matrices.reshape(50, -1)  # Each matrix â†’ 64-dim vector\n",
    "\n",
    "# Sample covariance of the data (this IS the data geometry)\n",
    "data_covariance = np.cov(data_vectors, rowvar=False)\n",
    "\n",
    "# Analyze eigenvalue spectrum (precision matrix = Fisher for Gaussian)\n",
    "eigenvalues = np.linalg.eigvalsh(data_covariance)\n",
    "eigenvalues = eigenvalues[::-1]  # Descending order\n",
    "eigenvalues = eigenvalues[eigenvalues > 1e-10]  # Remove numerical zeros\n",
    "\n",
    "print(\"Data covariance eigenvalue spectrum:\")\n",
    "print(f\"  Largest:  {eigenvalues[0]:.2e}\")\n",
    "print(f\"  Smallest: {eigenvalues[-1]:.2e}\")\n",
    "print(f\"  Condition number: {eigenvalues[0]/eigenvalues[-1]:.2e}\")\n",
    "print(f\"\\nSpan: {np.log10(eigenvalues[0]/eigenvalues[-1]):.1f} orders of magnitude\")\n",
    "print(f\"\\nEffective dimensionality: ~{np.sum(eigenvalues > eigenvalues[0]*0.01)} of {len(eigenvalues)} dimensions\")\n",
    "print(\"\\nThis is the 'sloppy model' phenomenon â€” most directions are noise!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randers metric asymmetry demo:\n",
      "  Forward  (with drift):    F(v)  = 1.3000\n",
      "  Backward (against drift): F(-v) = 0.7000\n",
      "  Ratio: 0.54x\n",
      "\n",
      "Going against the 'wind' costs -46% more!\n",
      "\n",
      "This models directed relationships, causal effects, irreversibility...\n"
     ]
    }
   ],
   "source": [
    "# Finsler geometry for asymmetric distances\n",
    "from diffgeo.geometry import RandersMetric\n",
    "\n",
    "# Create a Randers metric (Riemannian + drift)\n",
    "A = np.eye(3)  # Riemannian part\n",
    "b = np.array([0.3, 0.1, 0.0])  # Drift vector\n",
    "\n",
    "randers = RandersMetric(A, b)\n",
    "\n",
    "# Test asymmetry\n",
    "v = np.array([1.0, 0.0, 0.0])  # Direction vector\n",
    "\n",
    "forward_cost = randers.norm(v)\n",
    "backward_cost = randers.norm(-v)\n",
    "\n",
    "print(f\"Randers metric asymmetry demo:\")\n",
    "print(f\"  Forward  (with drift):    F(v)  = {forward_cost:.4f}\")\n",
    "print(f\"  Backward (against drift): F(-v) = {backward_cost:.4f}\")\n",
    "print(f\"  Ratio: {backward_cost/forward_cost:.2f}x\")\n",
    "print(f\"\\nGoing against the 'wind' costs {(backward_cost/forward_cost - 1)*100:.0f}% more!\")\n",
    "print(\"\\nThis models directed relationships, causal effects, irreversibility...\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
