"""
Bregman Divergences and Information Divergences

Implements various divergence measures used in information geometry,
including Bregman divergences, KL divergence, and their geometric properties.

From the research document Section 4:
"Information Geometry" treats statistical objects geometrically. Bregman
divergences generalize KL-divergence and provide a dual structure on
statistical manifolds.

Key properties:
- Non-symmetric: D(p||q) ≠ D(q||p) in general
- Non-negative: D(p||q) ≥ 0, = 0 iff p = q
- Convex in first argument
- Dual connections (e/m geometry)

Reference: Banerjee et al. (2005). "Clustering with Bregman Divergences"
"""
from __future__ import annotations

import jax
import jax.numpy as jnp
from typing import Callable, Optional, Tuple
from abc import ABC, abstractmethod


class BregmanDivergence:
    """
    Bregman divergence generated by a convex function.
    
    For a strictly convex, differentiable function φ:R^n → R, the
    Bregman divergence is:
    
        D_φ(p || q) = φ(p) - φ(q) - ⟨∇φ(q), p - q⟩
    
    This is the difference between φ(p) and the linear approximation
    of φ at q evaluated at p.
    
    Properties:
    - Non-negative (by convexity of φ)
    - Zero iff p = q
    - Not a metric (non-symmetric, no triangle inequality)
    - Convex in first argument, generally not in second
    
    The Bregman divergence induces a dual structure on the space,
    with primal coordinates θ and dual coordinates η = ∇φ(θ).
    """
    
    def __init__(self, 
                 phi: Callable[[jnp.ndarray], float],
                 grad_phi: Optional[Callable[[jnp.ndarray], jnp.ndarray]] = None,
                 name: str = "Bregman"):
        """
        Args:
            phi: Convex generating function φ
            grad_phi: Gradient of φ (computed via autodiff if not provided)
            name: Name for this divergence
        """
        self.phi = phi
        self.grad_phi = grad_phi if grad_phi is not None else jax.grad(phi)
        self.name = name
    
    def __call__(self, p: jnp.ndarray, q: jnp.ndarray) -> float:
        """
        Compute D_φ(p || q).
        
        Args:
            p: First argument (target)
            q: Second argument (reference)
            
        Returns:
            Divergence value
        """
        grad_q = self.grad_phi(q)
        return float(self.phi(p) - self.phi(q) - jnp.dot(grad_q, p - q))
    
    def reverse(self, p: jnp.ndarray, q: jnp.ndarray) -> float:
        """Compute reverse divergence D_φ(q || p)."""
        return self(q, p)
    
    def symmetrized(self, p: jnp.ndarray, q: jnp.ndarray) -> float:
        """
        Compute symmetrized divergence: D_φ(p||q) + D_φ(q||p).
        
        Also known as Jeffrey's divergence for KL.
        """
        return self(p, q) + self(q, p)
    
    def is_symmetric(self, p: jnp.ndarray, q: jnp.ndarray, tol: float = 1e-8) -> bool:
        """Check if divergence is symmetric for these points."""
        return abs(self(p, q) - self(q, p)) < tol
    
    @property
    def dual_divergence(self) -> 'BregmanDivergence':
        """
        Compute dual Bregman divergence using Legendre transform.
        
        If φ* is the Legendre-Fenchel dual of φ, then:
            D_φ*(η_p || η_q) = D_φ(q || p)
        
        Note the argument swap! This is the duality relationship.
        """
        # For implementation, we'd need the dual function φ*
        # This is a placeholder for the full dual computation
        raise NotImplementedError(
            "Dual divergence requires Legendre transform of φ"
        )
    
    def e_projection(self, 
                     p: jnp.ndarray, 
                     constraint_fn: Callable[[jnp.ndarray], bool],
                     constraint_set_sample: jnp.ndarray,
                     max_iter: int = 100) -> jnp.ndarray:
        """
        Exponential (e) projection onto constraint set.
        
        Finds q* = argmin_{q in C} D_φ(q || p)
        
        This is the projection using the "first" Bregman divergence
        (minimizing over the first argument).
        
        For KL divergence, this gives the maximum entropy distribution
        satisfying the constraints.
        """
        # Simple implementation: search over samples
        best_q = constraint_set_sample[0]
        best_div = float('inf')
        
        for q in constraint_set_sample:
            if constraint_fn(q):
                div = self(q, p)
                if div < best_div:
                    best_div = div
                    best_q = q
        
        return best_q
    
    def m_projection(self,
                     p: jnp.ndarray,
                     constraint_fn: Callable[[jnp.ndarray], bool],
                     constraint_set_sample: jnp.ndarray) -> jnp.ndarray:
        """
        Mixture (m) projection onto constraint set.
        
        Finds q* = argmin_{q in C} D_φ(p || q)
        
        This is the projection using the "second" Bregman divergence
        (minimizing over the second argument).
        
        For KL divergence, this gives moment matching.
        """
        best_q = constraint_set_sample[0]
        best_div = float('inf')
        
        for q in constraint_set_sample:
            if constraint_fn(q):
                div = self(p, q)
                if div < best_div:
                    best_div = div
                    best_q = q
        
        return best_q


# =============================================================================
# Common Bregman Divergences
# =============================================================================

class KLDivergence(BregmanDivergence):
    """
    Kullback-Leibler divergence (relative entropy).
    
    Generated by φ(p) = Σ p_i log(p_i) (negative entropy).
    
    KL(p || q) = Σ p_i log(p_i / q_i)
    
    This is the most important divergence in information theory
    and machine learning.
    """
    
    def __init__(self):
        def neg_entropy(p):
            p_safe = jnp.maximum(p, 1e-10)
            return jnp.sum(p_safe * jnp.log(p_safe))
        
        def grad_neg_entropy(p):
            p_safe = jnp.maximum(p, 1e-10)
            return jnp.log(p_safe) + 1
        
        super().__init__(neg_entropy, grad_neg_entropy, "KL")
    
    def __call__(self, p: jnp.ndarray, q: jnp.ndarray) -> float:
        """
        Compute KL(p || q) = Σ p_i log(p_i / q_i).
        
        Direct computation (more stable than generic Bregman).
        """
        p_safe = jnp.maximum(p, 1e-10)
        q_safe = jnp.maximum(q, 1e-10)
        return float(jnp.sum(p_safe * jnp.log(p_safe / q_safe)))


class SquaredEuclidean(BregmanDivergence):
    """
    Squared Euclidean distance as Bregman divergence.
    
    Generated by φ(p) = ||p||² / 2.
    
    D(p || q) = ||p - q||² / 2
    
    This is the unique Bregman divergence that is symmetric
    and satisfies the triangle inequality (it's a true metric squared).
    """
    
    def __init__(self):
        def half_norm_sq(p):
            return 0.5 * jnp.sum(p ** 2)
        
        super().__init__(half_norm_sq, name="SquaredEuclidean")
    
    def __call__(self, p: jnp.ndarray, q: jnp.ndarray) -> float:
        """Compute ||p - q||² / 2."""
        return float(0.5 * jnp.sum((p - q) ** 2))


class ItakuraSaito(BregmanDivergence):
    """
    Itakura-Saito divergence.
    
    Generated by φ(p) = -Σ log(p_i) (negative log).
    
    D_IS(p || q) = Σ (p_i/q_i - log(p_i/q_i) - 1)
    
    Used in audio/speech processing for spectral analysis.
    Scale-invariant: D_IS(αp || αq) = D_IS(p || q).
    """
    
    def __init__(self):
        def neg_log_sum(p):
            p_safe = jnp.maximum(p, 1e-10)
            return -jnp.sum(jnp.log(p_safe))
        
        super().__init__(neg_log_sum, name="ItakuraSaito")
    
    def __call__(self, p: jnp.ndarray, q: jnp.ndarray) -> float:
        """Compute Itakura-Saito divergence."""
        p_safe = jnp.maximum(p, 1e-10)
        q_safe = jnp.maximum(q, 1e-10)
        ratio = p_safe / q_safe
        return float(jnp.sum(ratio - jnp.log(ratio) - 1))


class LogDet(BregmanDivergence):
    """
    Log-determinant divergence for SPD matrices.
    
    Generated by φ(P) = -log det(P).
    
    D_LD(P || Q) = tr(PQ^{-1}) - log det(PQ^{-1}) - n
    
    Also known as Stein's loss or AIRM (Affine Invariant Riemannian Metric).
    This is the Bregman divergence version of the SPD Riemannian distance.
    """
    
    def __init__(self, dim: int):
        self.dim = dim
        
        def neg_log_det(P):
            return -jnp.linalg.slogdet(P)[1]
        
        super().__init__(neg_log_det, name="LogDet")
    
    def __call__(self, P: jnp.ndarray, Q: jnp.ndarray) -> float:
        """Compute log-det divergence for matrices."""
        Q_inv = jnp.linalg.inv(Q)
        PQ_inv = P @ Q_inv
        trace_term = jnp.trace(PQ_inv)
        log_det_term = jnp.linalg.slogdet(PQ_inv)[1]
        return float(trace_term - log_det_term - self.dim)


class AlphaDivergence:
    """
    Alpha-divergence (Rényi divergence family).
    
    D_α(p || q) = (1/(α(α-1))) (1 - Σ p_i^α q_i^{1-α})
    
    Interpolates between:
    - α → 0: Reverse KL
    - α = 0.5: Hellinger distance
    - α → 1: KL divergence
    - α = 2: Chi-squared divergence
    
    Not a Bregman divergence in general, but related to the
    exponential family structure.
    """
    
    def __init__(self, alpha: float):
        if alpha == 0 or alpha == 1:
            raise ValueError("Use KL divergence for α=0 or α=1")
        self.alpha = alpha
    
    def __call__(self, p: jnp.ndarray, q: jnp.ndarray) -> float:
        """Compute α-divergence."""
        p_safe = jnp.maximum(p, 1e-10)
        q_safe = jnp.maximum(q, 1e-10)
        
        a = self.alpha
        integral = jnp.sum(jnp.power(p_safe, a) * jnp.power(q_safe, 1 - a))
        
        return float((1 - integral) / (a * (a - 1)))


# =============================================================================
# Utility Functions
# =============================================================================

def js_divergence(p: jnp.ndarray, q: jnp.ndarray) -> float:
    """
    Jensen-Shannon divergence (symmetric, bounded KL).
    
    JS(p || q) = 0.5 * KL(p || m) + 0.5 * KL(q || m)
    
    where m = (p + q) / 2.
    
    Properties:
    - Symmetric: JS(p||q) = JS(q||p)
    - Bounded: 0 ≤ JS ≤ log(2)
    - sqrt(JS) is a true metric
    """
    m = (p + q) / 2
    kl = KLDivergence()
    return 0.5 * kl(p, m) + 0.5 * kl(q, m)


def total_variation(p: jnp.ndarray, q: jnp.ndarray) -> float:
    """
    Total variation distance.
    
    TV(p, q) = 0.5 * Σ |p_i - q_i|
    
    This is a true metric and equals the maximum difference
    in probability of any event.
    """
    return float(0.5 * jnp.sum(jnp.abs(p - q)))


def hellinger_distance(p: jnp.ndarray, q: jnp.ndarray) -> float:
    """
    Hellinger distance.
    
    H(p, q) = sqrt(0.5 * Σ (sqrt(p_i) - sqrt(q_i))²)
             = sqrt(1 - BC(p, q))
    
    where BC is Bhattacharyya coefficient.
    
    This is a true metric with range [0, 1].
    """
    p_safe = jnp.maximum(p, 0)
    q_safe = jnp.maximum(q, 0)
    return float(jnp.sqrt(0.5 * jnp.sum((jnp.sqrt(p_safe) - jnp.sqrt(q_safe)) ** 2)))


__all__ = [
    'BregmanDivergence',
    'KLDivergence',
    'SquaredEuclidean',
    'ItakuraSaito',
    'LogDet',
    'AlphaDivergence',
    'js_divergence',
    'total_variation',
    'hellinger_distance',
]

