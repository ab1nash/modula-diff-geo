"""
Bregman Divergences and Information Divergences

Implements various divergence measures used in information geometry,
including Bregman divergences, KL divergence, and their geometric properties.

From the research document Section 4:
"Information Geometry" treats statistical objects geometrically. Bregman
divergences generalize KL-divergence and provide a dual structure on
statistical manifolds.

Key properties:
- Non-symmetric: D(p||q) ≠ D(q||p) in general
- Non-negative: D(p||q) ≥ 0, = 0 iff p = q
- Convex in first argument
- Dual connections (e/m geometry)

Reference: Banerjee et al. (2005). "Clustering with Bregman Divergences"
"""
from __future__ import annotations

import jax
import jax.numpy as jnp
from typing import Callable, Optional, Tuple
from abc import ABC, abstractmethod


class BregmanDivergence:
    """
    Bregman divergence generated by a convex function.
    
    For a strictly convex, differentiable function φ:R^n → R, the
    Bregman divergence is:
    
        D_φ(p || q) = φ(p) - φ(q) - ⟨∇φ(q), p - q⟩
    
    This is the difference between φ(p) and the linear approximation
    of φ at q evaluated at p.
    
    Properties:
    - Non-negative (by convexity of φ)
    - Zero iff p = q
    - Not a metric (non-symmetric, no triangle inequality)
    - Convex in first argument, generally not in second
    
    The Bregman divergence induces a dual structure on the space,
    with primal coordinates θ and dual coordinates η = ∇φ(θ).
    """
    
    def __init__(self, 
                 phi: Callable[[jnp.ndarray], float],
                 grad_phi: Optional[Callable[[jnp.ndarray], jnp.ndarray]] = None,
                 name: str = "Bregman"):
        """
        Args:
            phi: Convex generating function φ
            grad_phi: Gradient of φ (computed via autodiff if not provided)
            name: Name for this divergence
        """
        self.phi = phi
        self.grad_phi = grad_phi if grad_phi is not None else jax.grad(phi)
        self.name = name
    
    def __call__(self, p: jnp.ndarray, q: jnp.ndarray) -> float:
        """
        Compute D_φ(p || q).
        
        Args:
            p: First argument (target)
            q: Second argument (reference)
            
        Returns:
            Divergence value
        """
        grad_q = self.grad_phi(q)
        return float(self.phi(p) - self.phi(q) - jnp.dot(grad_q, p - q))
    
    def reverse(self, p: jnp.ndarray, q: jnp.ndarray) -> float:
        """Compute reverse divergence D_φ(q || p)."""
        return self(q, p)
    
    def symmetrized(self, p: jnp.ndarray, q: jnp.ndarray) -> float:
        """
        Compute symmetrized divergence: D_φ(p||q) + D_φ(q||p).
        
        Also known as Jeffrey's divergence for KL.
        """
        return self(p, q) + self(q, p)
    
    def is_symmetric(self, p: jnp.ndarray, q: jnp.ndarray, tol: float = 1e-8) -> bool:
        """Check if divergence is symmetric for these points."""
        return abs(self(p, q) - self(q, p)) < tol
    
    @property
    def dual_divergence(self) -> 'BregmanDivergence':
        """
        Compute dual Bregman divergence using Legendre transform.
        
        If φ* is the Legendre-Fenchel dual of φ, then:
            D_φ*(η_p || η_q) = D_φ(q || p)
        
        Note the argument swap! This is the duality relationship.
        """
        # For implementation, we'd need the dual function φ*
        # This is a placeholder for the full dual computation
        raise NotImplementedError(
            "Dual divergence requires Legendre transform of φ"
        )
    
    def e_projection(self, 
                     p: jnp.ndarray, 
                     constraint_fn: Callable[[jnp.ndarray], bool],
                     constraint_set_sample: jnp.ndarray,
                     max_iter: int = 100) -> jnp.ndarray:
        """
        Exponential (e) projection onto constraint set.
        
        Finds q* = argmin_{q in C} D_φ(q || p)
        
        This is the projection using the "first" Bregman divergence
        (minimizing over the first argument).
        
        For KL divergence, this gives the maximum entropy distribution
        satisfying the constraints.
        """
        # Simple implementation: search over samples
        best_q = constraint_set_sample[0]
        best_div = float('inf')
        
        for q in constraint_set_sample:
            if constraint_fn(q):
                div = self(q, p)
                if div < best_div:
                    best_div = div
                    best_q = q
        
        return best_q
    
    def m_projection(self,
                     p: jnp.ndarray,
                     constraint_fn: Callable[[jnp.ndarray], bool],
                     constraint_set_sample: jnp.ndarray) -> jnp.ndarray:
        """
        Mixture (m) projection onto constraint set.
        
        Finds q* = argmin_{q in C} D_φ(p || q)
        
        This is the projection using the "second" Bregman divergence
        (minimizing over the second argument).
        
        For KL divergence, this gives moment matching.
        """
        best_q = constraint_set_sample[0]
        best_div = float('inf')
        
        for q in constraint_set_sample:
            if constraint_fn(q):
                div = self(p, q)
                if div < best_div:
                    best_div = div
                    best_q = q
        
        return best_q


# =============================================================================
# Common Bregman Divergences
# =============================================================================

class KLDivergence(BregmanDivergence):
    """
    Kullback-Leibler divergence (relative entropy).
    
    Generated by φ(p) = Σ p_i log(p_i) (negative entropy).
    
    KL(p || q) = Σ p_i log(p_i / q_i)
    
    This is the most important divergence in information theory
    and machine learning.
    """
    
    def __init__(self):
        def neg_entropy(p):
            p_safe = jnp.maximum(p, 1e-10)
            return jnp.sum(p_safe * jnp.log(p_safe))
        
        def grad_neg_entropy(p):
            p_safe = jnp.maximum(p, 1e-10)
            return jnp.log(p_safe) + 1
        
        super().__init__(neg_entropy, grad_neg_entropy, "KL")
    
    def __call__(self, p: jnp.ndarray, q: jnp.ndarray) -> float:
        """
        Compute KL(p || q) = Σ p_i log(p_i / q_i).
        
        Direct computation (more stable than generic Bregman).
        """
        p_safe = jnp.maximum(p, 1e-10)
        q_safe = jnp.maximum(q, 1e-10)
        return float(jnp.sum(p_safe * jnp.log(p_safe / q_safe)))


class SquaredEuclidean(BregmanDivergence):
    """
    Squared Euclidean distance as Bregman divergence.
    
    Generated by φ(p) = ||p||² / 2.
    
    D(p || q) = ||p - q||² / 2
    
    This is the unique Bregman divergence that is symmetric
    and satisfies the triangle inequality (it's a true metric squared).
    """
    
    def __init__(self):
        def half_norm_sq(p):
            return 0.5 * jnp.sum(p ** 2)
        
        super().__init__(half_norm_sq, name="SquaredEuclidean")
    
    def __call__(self, p: jnp.ndarray, q: jnp.ndarray) -> float:
        """Compute ||p - q||² / 2."""
        return float(0.5 * jnp.sum((p - q) ** 2))


class ItakuraSaito(BregmanDivergence):
    """
    Itakura-Saito divergence.
    
    Generated by φ(p) = -Σ log(p_i) (negative log).
    
    D_IS(p || q) = Σ (p_i/q_i - log(p_i/q_i) - 1)
    
    Used in audio/speech processing for spectral analysis.
    Scale-invariant: D_IS(αp || αq) = D_IS(p || q).
    """
    
    def __init__(self):
        def neg_log_sum(p):
            p_safe = jnp.maximum(p, 1e-10)
            return -jnp.sum(jnp.log(p_safe))
        
        super().__init__(neg_log_sum, name="ItakuraSaito")
    
    def __call__(self, p: jnp.ndarray, q: jnp.ndarray) -> float:
        """Compute Itakura-Saito divergence."""
        p_safe = jnp.maximum(p, 1e-10)
        q_safe = jnp.maximum(q, 1e-10)
        ratio = p_safe / q_safe
        return float(jnp.sum(ratio - jnp.log(ratio) - 1))


class LogDet(BregmanDivergence):
    """
    Log-determinant divergence for SPD matrices.
    
    Generated by φ(P) = -log det(P).
    
    D_LD(P || Q) = tr(PQ^{-1}) - log det(PQ^{-1}) - n
    
    Also known as Stein's loss or AIRM (Affine Invariant Riemannian Metric).
    This is the Bregman divergence version of the SPD Riemannian distance.
    """
    
    def __init__(self, dim: int):
        self.dim = dim
        
        def neg_log_det(P):
            return -jnp.linalg.slogdet(P)[1]
        
        super().__init__(neg_log_det, name="LogDet")
    
    def __call__(self, P: jnp.ndarray, Q: jnp.ndarray) -> float:
        """Compute log-det divergence for matrices."""
        Q_inv = jnp.linalg.inv(Q)
        PQ_inv = P @ Q_inv
        trace_term = jnp.trace(PQ_inv)
        log_det_term = jnp.linalg.slogdet(PQ_inv)[1]
        return float(trace_term - log_det_term - self.dim)


class AlphaDivergence:
    """
    Alpha-divergence (Rényi divergence family).
    
    D_α(p || q) = (1/(α(α-1))) (1 - Σ p_i^α q_i^{1-α})
    
    Interpolates between:
    - α → 0: Reverse KL
    - α = 0.5: Hellinger distance
    - α → 1: KL divergence
    - α = 2: Chi-squared divergence
    
    Not a Bregman divergence in general, but related to the
    exponential family structure.
    """
    
    def __init__(self, alpha: float):
        if alpha == 0 or alpha == 1:
            raise ValueError("Use KL divergence for α=0 or α=1")
        self.alpha = alpha
    
    def __call__(self, p: jnp.ndarray, q: jnp.ndarray) -> float:
        """Compute α-divergence."""
        p_safe = jnp.maximum(p, 1e-10)
        q_safe = jnp.maximum(q, 1e-10)
        
        a = self.alpha
        integral = jnp.sum(jnp.power(p_safe, a) * jnp.power(q_safe, 1 - a))
        
        return float((1 - integral) / (a * (a - 1)))


# =============================================================================
# Utility Functions
# =============================================================================

def js_divergence(p: jnp.ndarray, q: jnp.ndarray) -> float:
    """
    Jensen-Shannon divergence (symmetric, bounded KL).
    
    JS(p || q) = 0.5 * KL(p || m) + 0.5 * KL(q || m)
    
    where m = (p + q) / 2.
    
    Properties:
    - Symmetric: JS(p||q) = JS(q||p)
    - Bounded: 0 ≤ JS ≤ log(2)
    - sqrt(JS) is a true metric
    """
    m = (p + q) / 2
    kl = KLDivergence()
    return 0.5 * kl(p, m) + 0.5 * kl(q, m)


def total_variation(p: jnp.ndarray, q: jnp.ndarray) -> float:
    """
    Total variation distance.
    
    TV(p, q) = 0.5 * Σ |p_i - q_i|
    
    This is a true metric and equals the maximum difference
    in probability of any event.
    """
    return float(0.5 * jnp.sum(jnp.abs(p - q)))


def hellinger_distance(p: jnp.ndarray, q: jnp.ndarray) -> float:
    """
    Hellinger distance.
    
    H(p, q) = sqrt(0.5 * Σ (sqrt(p_i) - sqrt(q_i))²)
             = sqrt(1 - BC(p, q))
    
    where BC is Bhattacharyya coefficient.
    
    This is a true metric with range [0, 1].
    """
    p_safe = jnp.maximum(p, 0)
    q_safe = jnp.maximum(q, 0)
    return float(jnp.sqrt(0.5 * jnp.sum((jnp.sqrt(p_safe) - jnp.sqrt(q_safe)) ** 2)))


# =============================================================================
# BREGMAN-FISHER CONNECTION
# =============================================================================
#
# The following functions connect Bregman divergences to Fisher geometry.
# This is the mathematical bridge between information theory and differential
# geometry.
#
# Key insight from research doc [3]:
#   "The Fisher Information Matrix is the Hessian of the Bregman generator
#    for exponential families."
#
# Connection to other modules:
#   - statistical_manifold.py: StatisticalManifold uses Fisher as metric
#   - information.py: FisherMetric class
#   - core.py: TensorVariance (Bregman duality ↔ covariant/contravariant)
#
# =============================================================================


def fisher_from_bregman(bregman: BregmanDivergence, 
                        point: jnp.ndarray) -> jnp.ndarray:
    """
    Extract Fisher metric from Bregman divergence.
    
    The Fisher Information Matrix equals the Hessian of the Bregman
    generating function φ:
    
        F_ij(θ) = ∂²φ/∂θ_i∂θ_j
    
    This connects Bregman geometry (divergences) to Riemannian geometry
    (metrics). The local behavior of a Bregman divergence IS the Fisher metric:
    
        D_φ(θ + δ || θ) ≈ ½ δᵀ F(θ) δ
    
    Args:
        bregman: BregmanDivergence instance
        point: Point θ where to compute Fisher metric
        
    Returns:
        Fisher Information Matrix (n x n)
        
    Example:
        >>> kl = KLDivergence()
        >>> probs = jnp.array([0.3, 0.5, 0.2])
        >>> fisher = fisher_from_bregman(kl, probs)
        >>> # For categorical, Fisher is diag(1/p_i)
    
    See also:
        - information.py: FisherMetric class
        - statistical_manifold.py: StatisticalManifold.fisher_metric
    """
    # Fisher = Hessian of the generating function φ
    hessian_fn = jax.hessian(bregman.phi)
    return hessian_fn(point)


def local_divergence_approximation(bregman: BregmanDivergence,
                                   center: jnp.ndarray,
                                   displacement: jnp.ndarray) -> float:
    """
    Local quadratic approximation of Bregman divergence.
    
    For small δ:
        D_φ(θ + δ || θ) ≈ ½ δᵀ F(θ) δ
    
    This shows the Bregman divergence is locally equivalent to the
    Fisher-Rao (Riemannian) distance squared.
    
    Args:
        bregman: BregmanDivergence instance
        center: Reference point θ
        displacement: Small displacement δ
        
    Returns:
        Quadratic approximation ½ δᵀ F δ
    """
    fisher = fisher_from_bregman(bregman, center)
    return 0.5 * float(displacement @ fisher @ displacement)


class DuallyFlatManifold:
    """
    A dually flat manifold induced by a Bregman divergence.
    
    This is the geometric structure underlying exponential families
    and connects to information geometry.
    
    ==========================================================================
    THE DUAL STRUCTURE:
    ==========================================================================
    
    A Bregman divergence D_φ induces TWO coordinate systems:
    
    1. PRIMAL (θ): Natural parameters
       - Transforms CONTRAVARIANTLY
       - Geodesics are straight lines in θ-space
       
    2. DUAL (η): Expectation parameters  
       - η = ∇φ(θ) (gradient of generator)
       - Transforms COVARIANTLY
       - Geodesics are straight in η-space
    
    The Legendre transform φ*(η) defines the dual generator, and:
        θ = ∇φ*(η)
    
    This duality is exactly the covariant/contravariant distinction!
    
    ==========================================================================
    CONNECTION TO FISHER:
    ==========================================================================
    
    The Fisher metric in both coordinates is:
        F_ij(θ) = ∂²φ/∂θ_i∂θ_j = ∂η_i/∂θ_j
        F^ij(η) = ∂²φ*/∂η_i∂η_j = ∂θ_i/∂η_j
    
    These are inverses of each other (index raising/lowering).
    
    See also:
        - core.py: TensorVariance.COVARIANT, CONTRAVARIANT
        - metric.py: MetricTensor.raise_index(), lower_index()
    """
    
    def __init__(self, bregman: BregmanDivergence):
        """
        Create dually flat manifold from Bregman divergence.
        
        Args:
            bregman: The generating Bregman divergence
        """
        self.bregman = bregman
        self.phi = bregman.phi
        self.grad_phi = bregman.grad_phi
    
    def primal_to_dual(self, theta: jnp.ndarray) -> jnp.ndarray:
        """
        Convert primal coordinates to dual.
        
        η = ∇φ(θ)
        
        This is the "Legendre map" from contravariant to covariant.
        """
        return self.grad_phi(theta)
    
    def dual_to_primal(self, eta: jnp.ndarray, 
                       theta_init: Optional[jnp.ndarray] = None,
                       max_iter: int = 100,
                       tol: float = 1e-6) -> jnp.ndarray:
        """
        Convert dual coordinates back to primal.
        
        θ = ∇φ*(η) = (∇φ)^{-1}(η)
        
        Requires solving ∇φ(θ) = η (Newton's method).
        """
        # Initialize
        if theta_init is None:
            theta = eta.copy()  # Reasonable starting point
        else:
            theta = theta_init
        
        for _ in range(max_iter):
            # Current dual coordinates
            eta_current = self.grad_phi(theta)
            error = eta_current - eta
            
            if jnp.linalg.norm(error) < tol:
                break
            
            # Newton step: Hessian^{-1} @ error
            hess = jax.hessian(self.phi)(theta)
            delta = jnp.linalg.solve(hess, error)
            theta = theta - delta
        
        return theta
    
    def fisher_metric_primal(self, theta: jnp.ndarray) -> jnp.ndarray:
        """
        Fisher metric in primal (θ) coordinates.
        
        F_ij = ∂²φ/∂θ_i∂θ_j
        
        This is covariant (lower indices).
        """
        return fisher_from_bregman(self.bregman, theta)
    
    def fisher_metric_dual(self, eta: jnp.ndarray) -> jnp.ndarray:
        """
        Fisher metric in dual (η) coordinates.
        
        F^ij = ∂²φ*/∂η_i∂η_j
        
        This is contravariant (upper indices).
        For dually flat manifolds, this equals (F_ij)^{-1}.
        """
        # First convert to primal
        theta = self.dual_to_primal(eta)
        fisher_primal = self.fisher_metric_primal(theta)
        return jnp.linalg.inv(fisher_primal)
    
    def e_geodesic(self, 
                   theta_0: jnp.ndarray, 
                   theta_1: jnp.ndarray,
                   t: float) -> jnp.ndarray:
        """
        Exponential (e) geodesic in primal coordinates.
        
        These are straight lines in θ-space:
            γ_e(t) = (1-t)θ_0 + t·θ_1
        
        Called "e-geodesic" because exponential families have
        straight geodesics in natural parameters.
        """
        return (1 - t) * theta_0 + t * theta_1
    
    def m_geodesic(self,
                   theta_0: jnp.ndarray,
                   theta_1: jnp.ndarray, 
                   t: float) -> jnp.ndarray:
        """
        Mixture (m) geodesic in primal coordinates.
        
        These are straight lines in η-space (dual coordinates),
        then mapped back to θ-space.
        
        Called "m-geodesic" because mixture families have
        straight geodesics in expectation parameters.
        """
        eta_0 = self.primal_to_dual(theta_0)
        eta_1 = self.primal_to_dual(theta_1)
        eta_t = (1 - t) * eta_0 + t * eta_1
        return self.dual_to_primal(eta_t)
    
    def pythagorean_theorem(self,
                            p: jnp.ndarray,
                            q: jnp.ndarray,
                            r: jnp.ndarray) -> Tuple[float, float, float]:
        """
        Verify generalized Pythagorean theorem.
        
        For Bregman divergences, if r is the e-projection of p onto
        the e-flat submanifold containing q, then:
        
            D(p || q) = D(p || r) + D(r || q)
        
        Returns the three divergences for verification.
        """
        d_pq = self.bregman(p, q)
        d_pr = self.bregman(p, r)
        d_rq = self.bregman(r, q)
        return d_pq, d_pr, d_rq


def bregman_to_statistical_manifold(bregman: BregmanDivergence,
                                    params: jnp.ndarray,
                                    samples: Optional[jnp.ndarray] = None):
    """
    Create a StatisticalManifold from a Bregman divergence.
    
    This bridges the gap between divergence-based (information theory)
    and metric-based (differential geometry) perspectives.
    
    The log-probability is derived from the Bregman generator:
        log p(x|θ) ∝ θ·T(x) - φ(θ)  (exponential family form)
    
    Args:
        bregman: BregmanDivergence defining the geometry
        params: Current parameter values θ
        samples: Data samples (optional, for empirical Fisher)
        
    Returns:
        StatisticalManifold with Fisher metric from Bregman Hessian
        
    See also:
        - statistical_manifold.py: StatisticalManifold class
    """
    # Import here to avoid circular dependency
    from .statistical_manifold import StatisticalManifold
    from .information import FisherMetric
    
    # Compute Fisher from Bregman Hessian
    fisher_matrix = fisher_from_bregman(bregman, params)
    fisher = FisherMetric(fisher_matrix, params=params)
    
    # Create exponential family log-prob
    def log_prob(theta, x):
        # Exponential family: log p ∝ θ·x - φ(θ)
        return jnp.dot(theta, x) - bregman.phi(theta)
    
    manifold = StatisticalManifold(
        log_prob_fn=log_prob,
        params=params,
        _samples=samples
    )
    manifold._fisher_metric = fisher
    
    return manifold


__all__ = [
    'BregmanDivergence',
    'KLDivergence',
    'SquaredEuclidean',
    'ItakuraSaito',
    'LogDet',
    'AlphaDivergence',
    'js_divergence',
    'total_variation',
    'hellinger_distance',
    # New Bregman-Fisher connections
    'fisher_from_bregman',
    'local_divergence_approximation',
    'DuallyFlatManifold',
    'bregman_to_statistical_manifold',
]

