"""
Learnable Imputation Models for Geometric Benchmarks

Simple but properly trained models comparing:
- Modula (Euclidean/standard) optimization
- DiffGeo (Riemannian/geometric) optimization

Implements proper ML practices:
- Train/validation/test splits
- Early stopping
- Learning rate scheduling
- Convergence tracking
"""
import time
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Callable
import numpy as np

try:
    import jax
    import jax.numpy as jnp
    from jax import grad, jit, vmap
    HAS_JAX = True
except ImportError:
    HAS_JAX = False
    print("Warning: JAX not available. Learnable models disabled.")

from .manifold_integrity import ManifoldIntegrityScore, ManifoldType

# Import geometry extractor for automatic Fisher discovery
try:
    from diffgeo.information.extractor import DataGeometryExtractor
    from diffgeo.information.manifolds import StatisticalManifold

    HAS_EXTRACTOR = True
except ImportError:
    HAS_EXTRACTOR = False


@dataclass
class TrainingConfig:
    """
    Configuration for training learnable imputation models.
    
    Default values are set for proper convergence:
    - 2000 epochs max (early stopping usually kicks in earlier)
    - Patient early stopping (100 epochs without improvement)
    - Gradual learning rate decay
    """
    learning_rate: float = 0.01
    n_epochs: int = 2000  # Max epochs (early stopping will kick in)
    batch_size: int = 32
    validation_split: float = 0.15
    test_split: float = 0.15
    early_stopping_patience: int = 100  # Wait 100 epochs for improvement
    lr_decay_factor: float = 0.5
    lr_decay_patience: int = 40  # Decay LR after 40 epochs without improvement
    min_lr: float = 1e-6  # Don't decay below this
    seed: int = 42
    log_every: int = 100

    # For quick testing
    @staticmethod
    def quick() -> 'TrainingConfig':
        """Quick config for testing (not for real results)."""
        return TrainingConfig(
            n_epochs=200,
            early_stopping_patience=30,
            lr_decay_patience=15,
            log_every=50,
        )

    @staticmethod
    def standard() -> 'TrainingConfig':
        """Standard training config (balanced speed/quality)."""
        return TrainingConfig(
            n_epochs=1000,
            early_stopping_patience=75,
            lr_decay_patience=30,
            log_every=100,
        )

    @staticmethod
    def full() -> "TrainingConfig":
        """Thorough training config (best results, slower)."""
        return TrainingConfig(
            n_epochs=3000,
            early_stopping_patience=150,
            lr_decay_patience=50,
            log_every=200,
        )

    @staticmethod
    def thorough() -> "TrainingConfig":
        """Thorough training config (best results, slower)."""
        return TrainingConfig(
            n_epochs=3000,
            early_stopping_patience=150,
            lr_decay_patience=50,
            log_every=200,
        )


@dataclass
class TrainingHistory:
    """Track training progress."""
    train_losses: List[float] = field(default_factory=list)
    val_losses: List[float] = field(default_factory=list)
    learning_rates: List[float] = field(default_factory=list)
    epochs: List[int] = field(default_factory=list)
    best_val_loss: float = float('inf')
    best_epoch: int = 0
    final_epoch: int = 0
    training_time: float = 0.0
    converged: bool = False
    converged_reason: str = ""
    
    def to_dict(self) -> Dict:
        return {
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'learning_rates': self.learning_rates,
            'best_val_loss': self.best_val_loss,
            'best_epoch': self.best_epoch,
            'final_epoch': self.final_epoch,
            'training_time': self.training_time,
            'converged': self.converged,
            'converged_reason': self.converged_reason,
        }


@dataclass
class AdamState:
    """State for Adam optimizer - identical for both models to ensure fair comparison."""
    m: Dict  # First moment estimates
    v: Dict  # Second moment estimates
    t: int   # Timestep
    
    @staticmethod
    def initialize(params: Dict) -> 'AdamState':
        """Initialize Adam state with zeros."""
        m = {k: np.zeros_like(v) for k, v in params.items()}
        v = {k: np.zeros_like(val) for k, val in params.items()}
        return AdamState(m=m, v=v, t=0)


def adam_update(params: Dict, grads: Dict, state: AdamState, lr: float,
                beta1: float = 0.9, beta2: float = 0.999, eps: float = 1e-8) -> Tuple[Dict, AdamState]:
    """
    Adam optimizer update - IDENTICAL for both Modula and DiffGeo models.
    
    This ensures fair comparison by giving both models the same optimization dynamics.
    """
    if not HAS_JAX:
        # Fallback to numpy
        new_m = {}
        new_v = {}
        new_params = {}
        t = state.t + 1

        for k in params:
            g = grads[k]
            m_old = state.m[k]
            v_old = state.v[k]

            # Update biased first moment estimate
            m = beta1 * m_old + (1 - beta1) * g
            # Update biased second moment estimate
            v = beta2 * v_old + (1 - beta2) * (g ** 2)

            # Bias correction
            m_hat = m / (1 - beta1 ** t)
            v_hat = v / (1 - beta2 ** t)

            # Parameter update
            new_params[k] = params[k] - lr * m_hat / (np.sqrt(v_hat) + eps)
            new_m[k] = m
            new_v[k] = v

        return new_params, AdamState(m=new_m, v=new_v, t=t)

    new_m = {}
    new_v = {}
    new_params = {}
    t = state.t + 1

    for k in params:
        g = grads[k]
        m_old = state.m[k]
        v_old = state.v[k]

        # Update biased first moment estimate
        m = beta1 * m_old + (1 - beta1) * g
        # Update biased second moment estimate
        v = beta2 * v_old + (1 - beta2) * (g ** 2)

        # Bias correction
        m_hat = m / (1 - beta1 ** t)
        v_hat = v / (1 - beta2 ** t)

        # Parameter update
        new_params[k] = params[k] - lr * m_hat / (jnp.sqrt(v_hat) + eps)
        new_m[k] = m
        new_v[k] = v

    return new_params, AdamState(m=new_m, v=new_v, t=t)


# =============================================================================
# COMMON TRAINING UTILITIES
# =============================================================================
# These utilities are shared across all training functions to avoid duplication
# and ensure consistent data handling, splitting, and training loop behavior.
# =============================================================================


@dataclass
class DataSplit:
    """Container for train/val/test data splits."""

    train_data: jnp.ndarray
    train_mask: jnp.ndarray
    val_data: jnp.ndarray
    val_mask: jnp.ndarray
    test_data: Optional[jnp.ndarray] = None
    test_mask: Optional[jnp.ndarray] = None
    train_idx: np.ndarray = None
    val_idx: np.ndarray = None
    test_idx: np.ndarray = None
    n_train: int = 0
    n_val: int = 0
    n_test: int = 0
    rng: np.random.Generator = None


def prepare_data_for_training(
    data: np.ndarray, mask: np.ndarray, flatten: bool = True
) -> Tuple[jnp.ndarray, jnp.ndarray, Tuple]:
    """
    Prepare data for training by optionally flattening and converting to JAX arrays.

    Args:
        data: Input data array (n_samples, ...)
        mask: Boolean mask (True = observed)
        flatten: Whether to flatten non-batch dimensions

    Returns:
        data_jax: JAX array of data
        mask_jax: JAX array of mask
        original_shape: Original shape for later reconstruction
    """
    original_shape = data.shape

    if flatten and data.ndim > 2:
        n_samples = data.shape[0]
        data_flat = data.reshape(n_samples, -1)
        mask_flat = mask.reshape(n_samples, -1)
    else:
        data_flat = data
        mask_flat = mask

    data_jax = jnp.array(data_flat)
    mask_jax = jnp.array(mask_flat)

    return data_jax, mask_jax, original_shape


def create_data_split(
    data: jnp.ndarray,
    mask: jnp.ndarray,
    config: TrainingConfig,
    include_test: bool = False,
) -> DataSplit:
    """
    Create train/val/test splits from data.

    Args:
        data: JAX array of data (n_samples, dim)
        mask: JAX array of mask
        config: Training configuration with split ratios and seed
        include_test: Whether to include test split in result

    Returns:
        DataSplit containing all split data and indices
    """
    n_samples = len(data)
    rng = np.random.default_rng(config.seed)
    indices = rng.permutation(n_samples)

    n_test = int(n_samples * config.test_split)
    n_val = int(n_samples * config.validation_split)
    n_train = n_samples - n_test - n_val

    train_idx = indices[:n_train]
    val_idx = indices[n_train : n_train + n_val]
    test_idx = indices[n_train + n_val :] if include_test else None

    split = DataSplit(
        train_data=data[train_idx],
        train_mask=mask[train_idx],
        val_data=data[val_idx],
        val_mask=mask[val_idx],
        test_data=data[test_idx] if include_test else None,
        test_mask=mask[test_idx] if include_test else None,
        train_idx=train_idx,
        val_idx=val_idx,
        test_idx=test_idx,
        n_train=n_train,
        n_val=n_val,
        n_test=n_test,
        rng=rng,
    )
    return split


def clip_gradients(grads: Dict, max_norm: float = 10.0) -> Dict:
    """
    Clip gradients by global norm to prevent explosion.

    Args:
        grads: Dictionary of gradient arrays
        max_norm: Maximum allowed gradient norm

    Returns:
        Clipped gradients dictionary
    """
    total_norm = 0.0
    for g in grads.values():
        total_norm += float(jnp.sum(g**2))
    total_norm = np.sqrt(total_norm)

    if total_norm > max_norm:
        scale = max_norm / (total_norm + 1e-6)
        return {k: g * scale for k, g in grads.items()}
    return grads


def get_convergence_flag(epoch: int, n_epochs: int) -> str:
    """
    Get emoji flag indicating convergence quality.

    Args:
        epoch: Current epoch number
        n_epochs: Maximum epochs configured

    Returns:
        Emoji flag string
    """
    if epoch < 100:
        return "âš¡ FAST"
    elif epoch < n_epochs * 0.3:
        return "âœ“ GOOD"
    elif epoch > n_epochs * 0.8:
        return "âš ï¸  SLOW"
    else:
        return "âœ“"


def run_training_loop(
    model,
    params: Dict,
    split: DataSplit,
    config: TrainingConfig,
    compute_loss_fn: Callable,
    grad_fn: Callable,
    pre_epoch_hook: Optional[Callable] = None,
) -> Tuple[Dict, TrainingHistory]:
    """
    Run the common training loop with early stopping and LR decay.

    This encapsulates the shared training logic used by all models.

    Args:
        model: Model instance with compute_update method
        params: Initial model parameters
        split: DataSplit containing train/val data
        config: Training configuration
        compute_loss_fn: JIT-compiled loss function (params, x, m) -> loss
        grad_fn: JIT-compiled gradient function (params, x, m) -> grads
        pre_epoch_hook: Optional callback(epoch, params) called before each epoch

    Returns:
        best_params: Best model parameters by validation loss
        history: TrainingHistory with all metrics
    """
    history = TrainingHistory()
    best_params = params
    lr = config.learning_rate
    patience_counter = 0
    lr_patience_counter = 0

    start_time = time.time()

    for epoch in range(config.n_epochs):
        if pre_epoch_hook:
            pre_epoch_hook(epoch, params)

        # Shuffle training data
        perm = split.rng.permutation(split.n_train)
        train_data_shuffled = split.train_data[perm]
        train_mask_shuffled = split.train_mask[perm]

        # Mini-batch training
        epoch_losses = []
        for i in range(0, split.n_train, config.batch_size):
            batch_x = train_data_shuffled[i : i + config.batch_size]
            batch_m = train_mask_shuffled[i : i + config.batch_size]

            grads = grad_fn(params, batch_x, batch_m)
            grads = clip_gradients(grads)
            params = model.compute_update(params, grads, lr)

            batch_loss = float(compute_loss_fn(params, batch_x, batch_m))
            epoch_losses.append(batch_loss)

        train_loss = np.mean(epoch_losses)
        val_loss = float(compute_loss_fn(params, split.val_data, split.val_mask))

        # Record history
        history.train_losses.append(train_loss)
        history.val_losses.append(val_loss)
        history.learning_rates.append(lr)
        history.epochs.append(epoch)
        history.final_epoch = epoch

        # Check for improvement
        if val_loss < history.best_val_loss:
            history.best_val_loss = val_loss
            history.best_epoch = epoch
            best_params = {k: v.copy() for k, v in params.items()}
            patience_counter = 0
            lr_patience_counter = 0
        else:
            patience_counter += 1
            lr_patience_counter += 1

        # Learning rate decay
        if lr_patience_counter >= config.lr_decay_patience:
            new_lr = lr * config.lr_decay_factor
            if new_lr >= config.min_lr:
                lr = new_lr
            lr_patience_counter = 0

        # Early stopping
        if patience_counter >= config.early_stopping_patience:
            history.converged = True
            flag = get_convergence_flag(epoch, config.n_epochs)
            history.converged_reason = (
                f"Early stop @ epoch {epoch} (best @ {history.best_epoch})"
            )
            print(f"\n  {flag} {history.converged_reason}")
            break

        # Convergence check (loss plateau)
        min_epochs_for_plateau = max(200, config.n_epochs // 5)
        if epoch >= min_epochs_for_plateau and len(history.val_losses) > 100:
            recent_loss = np.mean(history.val_losses[-30:])
            older_loss = np.mean(history.val_losses[-100:-50])
            if abs(recent_loss - older_loss) / (older_loss + 1e-8) < 0.0001:
                history.converged = True
                history.converged_reason = f"Loss plateaued @ epoch {epoch}"
                print(f"\n  ðŸŽ¯ {history.converged_reason}")
                break

        if epoch % config.log_every == 0:
            print(
                f"  Epoch {epoch:4d}: train={train_loss:.6f}, val={val_loss:.6f}, best={history.best_val_loss:.6f}@{history.best_epoch}, lr={lr:.6f}"
            )

    if not history.converged:
        history.converged_reason = f"Max epochs ({config.n_epochs}) reached"

    history.training_time = time.time() - start_time
    return best_params, history


def compute_imputation_metrics(
    true_values: np.ndarray,
    pred_values: np.ndarray,
) -> Dict[str, float]:
    """
    Compute standard imputation metrics on missing values.

    Args:
        true_values: Ground truth values (flattened missing entries)
        pred_values: Predicted values (same shape as true_values)

    Returns:
        Dictionary with rmse, mae, r2, mrr metrics
    """
    true_values = np.asarray(true_values).flatten()
    pred_values = np.asarray(pred_values).flatten()

    n = len(true_values)
    if n == 0:
        return {"rmse": 0.0, "mae": 0.0, "r2": 1.0, "mrr": 1.0}

    # RMSE
    rmse = float(np.sqrt(np.mean((true_values - pred_values) ** 2)))

    # MAE
    mae = float(np.mean(np.abs(true_values - pred_values)))

    # RÂ²
    ss_res = np.sum((true_values - pred_values) ** 2)
    ss_tot = np.sum((true_values - np.mean(true_values)) ** 2)
    r2 = float(1 - ss_res / (ss_tot + 1e-8))

    # MRR (Mean Reciprocal Rank)
    # For imputation: measures ranking quality of predictions
    # For each prediction, compute its rank among all predictions when sorted
    # by absolute error. MRR = mean(1/rank) where rank=1 is best.
    #
    # Interpretation: if model predicts values in correct relative order,
    # the smallest errors should correspond to easiest-to-predict values.
    abs_errors = np.abs(true_values - pred_values)
    ranks = np.argsort(np.argsort(abs_errors)) + 1  # 1-indexed ranks
    mrr = float(np.mean(1.0 / ranks))

    return {"rmse": rmse, "mae": mae, "r2": r2, "mrr": mrr}


class ImputationModel:
    """
    Base class for learnable imputation models.
    
    Simple architecture: Linear encoder -> bottleneck -> Linear decoder
    The key difference is HOW we compute gradients and updates.
    """
    
    def __init__(self, input_dim: int, hidden_dim: int = 32, seed: int = 42):
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.seed = seed
        self.params = None
        
    def init_params(self, key) -> Dict:
        """Initialize model parameters."""
        k1, k2 = jax.random.split(key)
        
        # Simple autoencoder: input -> hidden -> input
        scale = 0.1
        params = {
            'encoder_w': jax.random.normal(k1, (self.input_dim, self.hidden_dim)) * scale,
            'encoder_b': jnp.zeros(self.hidden_dim),
            'decoder_w': jax.random.normal(k2, (self.hidden_dim, self.input_dim)) * scale,
            'decoder_b': jnp.zeros(self.input_dim),
        }
        return params
    
    def forward(self, x: jnp.ndarray, params: Dict) -> jnp.ndarray:
        """Forward pass: reconstruct input."""
        # Encoder
        h = jnp.tanh(x @ params['encoder_w'] + params['encoder_b'])
        # Decoder
        out = h @ params['decoder_w'] + params['decoder_b']
        return out
    
    def loss_fn(self, params: Dict, x: jnp.ndarray, mask: jnp.ndarray) -> float:
        """
        Reconstruction loss on OBSERVED entries only.
        
        The model learns to reconstruct observed values, then we use it
        to impute missing values.
        """
        pred = self.forward(x * mask, params)  # Input has missing zeroed
        # Loss only on observed entries
        diff = (pred - x) * mask
        return jnp.mean(diff ** 2)


class ModulaImputationModel(ImputationModel):
    """
    Imputation model using standard Euclidean (modula) optimization.
    
    Now uses Adam optimizer (same as DiffGeo) to ensure fair comparison.
    The difference is in the MODEL architecture/loss, not the optimizer.
    """
    
    def __init__(self, input_dim: int, hidden_dim: int = 32, seed: int = 42):
        super().__init__(input_dim, hidden_dim, seed)
        self.name = "Modula (Euclidean)"
        self.adam_state = None
    
    def init_adam_state(self, params: Dict) -> None:
        """Initialize Adam optimizer state."""
        self.adam_state = AdamState.initialize(params)
    
    def compute_update(self, params: Dict, grads: Dict, lr: float) -> Dict:
        """Adam optimizer update - identical to DiffGeo for fair comparison."""
        if self.adam_state is None:
            self.init_adam_state(params)
        new_params, self.adam_state = adam_update(params, grads, self.adam_state, lr)
        return new_params


class DiffGeoImputationModel(ImputationModel):
    """
    Imputation model using Riemannian (diffgeo) approach.
    
    Uses natural gradient descent which accounts for the geometry of the
    parameter space. This is the key differentiator from Euclidean methods.
    
    Natural gradient: Î¸_new = Î¸ - lr * F^(-1) * grad
    Where F is the Fisher information matrix (approximated here).
    """
    
    def __init__(self, input_dim: int, hidden_dim: int = 32, seed: int = 42):
        super().__init__(input_dim, hidden_dim, seed)
        self.name = "DiffGeo (Riemannian)"
        # State for natural gradient (running estimate of Fisher information)
        self.fisher_diag = None
        self.fisher_decay = 0.99
        self.fisher_eps = 1e-4
    
    def _init_fisher(self, params: Dict) -> None:
        """Initialize diagonal Fisher information estimate."""
        self.fisher_diag = {k: np.ones_like(v) for k, v in params.items()}
    
    def compute_update(self, params: Dict, grads: Dict, lr: float) -> Dict:
        """
        Natural gradient descent update.
        
        Approximates F^(-1) using diagonal Fisher information,
        estimated from squared gradients (empirical Fisher).
        """
        if self.fisher_diag is None:
            self._init_fisher(params)
        
        new_params = {}
        
        for k in params:
            g = grads[k]
            
            # Update diagonal Fisher estimate (exponential moving average of squared grads)
            # This approximates the empirical Fisher information
            self.fisher_diag[k] = (self.fisher_decay * self.fisher_diag[k] + 
                                   (1 - self.fisher_decay) * (g ** 2))
            
            # Natural gradient: precondition by inverse Fisher
            # F^(-1) * grad â‰ˆ grad / (fisher_diag + eps)
            natural_grad = g / (self.fisher_diag[k] + self.fisher_eps)
            
            # Update parameters
            if HAS_JAX:
                new_params[k] = params[k] - lr * natural_grad
            else:
                new_params[k] = params[k] - lr * natural_grad
        
        return new_params


class SPDImputationModel(ImputationModel):
    """
    Imputation model specifically for SPD matrices (LEGACY - not manifold-aware).
    
    Operates in log-space (tangent space) which linearizes the SPD manifold.
    Uses same Adam optimizer as other models for fair comparison.
    
    NOTE: This is the old implementation. Use SPDTangentSpaceModel for proper
    manifold-aware imputation.
    """
    
    def __init__(self, matrix_dim: int, hidden_dim: int = 32, seed: int = 42):
        # Input is flattened upper triangle of SPD matrix
        self.matrix_dim = matrix_dim
        input_dim = matrix_dim * (matrix_dim + 1) // 2
        super().__init__(input_dim, hidden_dim, seed)
        self.name = "DiffGeo (SPD Log-Space)"
        self.adam_state = None
    
    def init_adam_state(self, params: Dict) -> None:
        """Initialize Adam optimizer state."""
        self.adam_state = AdamState.initialize(params)
    
    def _to_log_space(self, spd_flat: jnp.ndarray) -> jnp.ndarray:
        """Convert flattened SPD to log-space."""
        # Reconstruct matrix
        idx = jnp.triu_indices(self.matrix_dim)
        mat = jnp.zeros((self.matrix_dim, self.matrix_dim))
        mat = mat.at[idx].set(spd_flat)
        mat = mat + mat.T - jnp.diag(jnp.diag(mat))
        
        # Log transform
        eigvals, eigvecs = jnp.linalg.eigh(mat)
        eigvals = jnp.maximum(eigvals, 1e-6)
        log_mat = eigvecs @ jnp.diag(jnp.log(eigvals)) @ eigvecs.T
        
        return log_mat[idx]
    
    def _from_log_space(self, log_flat: jnp.ndarray) -> jnp.ndarray:
        """Convert log-space back to SPD."""
        idx = jnp.triu_indices(self.matrix_dim)
        mat = jnp.zeros((self.matrix_dim, self.matrix_dim))
        mat = mat.at[idx].set(log_flat)
        mat = mat + mat.T - jnp.diag(jnp.diag(mat))
        
        # Exp transform
        eigvals, eigvecs = jnp.linalg.eigh(mat)
        exp_mat = eigvecs @ jnp.diag(jnp.exp(eigvals)) @ eigvecs.T
        
        return exp_mat[idx]
    
    def forward(self, x: jnp.ndarray, params: Dict) -> jnp.ndarray:
        """Forward in log-space."""
        # Work in log space
        h = jnp.tanh(x @ params['encoder_w'] + params['encoder_b'])
        out = h @ params['decoder_w'] + params['decoder_b']
        return out
    
    def compute_update(self, params: Dict, grads: Dict, lr: float) -> Dict:
        """Adam optimizer update - identical to other models for fair comparison."""
        if self.adam_state is None:
            self.init_adam_state(params)
        new_params, self.adam_state = adam_update(params, grads, self.adam_state, lr)
        return new_params


class SPDTangentSpaceModel(ImputationModel):
    """
    Manifold-aware imputation model for SPD matrices.
    
    This model properly respects the geometry of the SPD manifold P_n:
    
    1. Maps data to tangent space at identity via matrix logarithm:
       V = log(P)  (tangent vector at I)
       
    2. Performs Euclidean operations in tangent space (where they're valid!)
    
    3. Maps back to manifold via matrix exponential:
       P = exp(V)  (guaranteed SPD!)
    
    The loss function uses Log-Euclidean distance:
        d_LE(A, B) = ||log(A) - log(B)||_F
    
    This avoids the "swelling effect" and off-manifold predictions that
    plague Euclidean methods on SPD data.
    
    Reference: Arsigny et al. (2006) "Log-Euclidean metrics for fast and
    simple calculus on diffusion tensors"
    """

    def __init__(self, matrix_dim: int, hidden_dim: int = 32, seed: int = 42):
        self.matrix_dim = matrix_dim
        # Work with upper triangle in log-space (symmetric matrix)
        self.tri_dim = matrix_dim * (matrix_dim + 1) // 2
        # Initialize with upper triangle dimension
        super().__init__(self.tri_dim, hidden_dim, seed)
        self.name = "DiffGeo (SPD Tangent)"
        self.adam_state = None
        # Pre-compute indices as numpy arrays (JAX-safe, no tracing issues)
        self._triu_row, self._triu_col = np.triu_indices(matrix_dim)

    def init_adam_state(self, params: Dict) -> None:
        """Initialize Adam optimizer state."""
        self.adam_state = AdamState.initialize(params)

    def _get_triu_idx(self) -> Tuple:
        """Get upper triangular indices (numpy arrays, JAX-safe)."""
        return self._triu_row, self._triu_col

    def _matrix_log(self, P: jnp.ndarray) -> jnp.ndarray:
        """
        Compute matrix logarithm for SPD matrix.
        
        log(P) = V Î› V^T where Î› = diag(log(Î»_i))
        """
        eigvals, eigvecs = jnp.linalg.eigh(P)
        # Clamp eigenvalues to prevent log(0)
        eigvals = jnp.maximum(eigvals, 1e-8)
        log_eigvals = jnp.log(eigvals)
        return eigvecs @ jnp.diag(log_eigvals) @ eigvecs.T

    def _matrix_exp(self, V: jnp.ndarray) -> jnp.ndarray:
        """
        Compute matrix exponential for symmetric matrix.
        
        exp(V) = U exp(Î›) U^T where Î› = diag(Î»_i)
        
        Result is guaranteed SPD if V is symmetric!
        """
        eigvals, eigvecs = jnp.linalg.eigh(V)
        exp_eigvals = jnp.exp(eigvals)
        return eigvecs @ jnp.diag(exp_eigvals) @ eigvecs.T

    def _to_tangent(self, P_flat: jnp.ndarray) -> jnp.ndarray:
        """
        Map SPD matrix (flattened) to tangent space at identity.
        
        Returns upper triangle of log(P) as flat vector.
        """
        # Reshape to matrix
        P = P_flat.reshape(self.matrix_dim, self.matrix_dim)
        # Compute log
        log_P = self._matrix_log(P)
        # Extract upper triangle (symmetric, so no info lost)
        idx = self._get_triu_idx()
        return log_P[idx]

    def _from_tangent(self, v_flat: jnp.ndarray) -> jnp.ndarray:
        """
        Map tangent vector back to SPD manifold.
        
        Reconstructs symmetric matrix from upper triangle, then exponentiates.
        Result is GUARANTEED to be SPD!
        """
        # Reconstruct symmetric matrix from upper triangle
        idx = self._get_triu_idx()
        V = jnp.zeros((self.matrix_dim, self.matrix_dim))
        V = V.at[idx].set(v_flat)
        # Mirror to lower triangle (V is symmetric)
        V = V + V.T - jnp.diag(jnp.diag(V))
        # Exponentiate - result is guaranteed SPD
        P = self._matrix_exp(V)
        return P.flatten()

    def _batch_to_tangent(self, P_batch: jnp.ndarray) -> jnp.ndarray:
        """Vectorized version for batches."""
        return vmap(self._to_tangent)(P_batch)

    def _batch_from_tangent(self, v_batch: jnp.ndarray) -> jnp.ndarray:
        """Vectorized version for batches."""
        return vmap(self._from_tangent)(v_batch)

    def forward(self, x: jnp.ndarray, params: Dict) -> jnp.ndarray:
        """
        Forward pass in tangent space.
        
        1. Input x is already masked SPD matrices (flattened)
        2. Map to tangent space (log)
        3. Apply autoencoder in tangent space
        4. Map back to manifold (exp)
        """
        # Autoencoder in tangent/Euclidean space
        h = jnp.tanh(x @ params['encoder_w'] + params['encoder_b'])
        out = h @ params['decoder_w'] + params['decoder_b']
        return out

    def forward_with_manifold(self, x: jnp.ndarray, params: Dict, 
                               is_spd_input: bool = True) -> jnp.ndarray:
        """
        Full forward with manifold projection.
        
        If is_spd_input=True, first converts input to tangent space.
        Always returns result on SPD manifold.
        """
        if is_spd_input:
            # Map to tangent space first
            x_tangent = self._batch_to_tangent(x)
        else:
            x_tangent = x

        # Process in tangent space
        out_tangent = self.forward(x_tangent, params)

        # Map back to manifold
        return self._batch_from_tangent(out_tangent)

    def loss_fn(self, params: Dict, x: jnp.ndarray, mask: jnp.ndarray) -> float:
        """
        Log-Euclidean loss function.
        
        Unlike Euclidean MSE on raw matrices, this respects the SPD geometry:
        d_LE(A, B) = ||log(A) - log(B)||_F
        
        The loss is computed in tangent space where Euclidean operations
        are geometrically valid.
        """
        # Convert full SPD matrices to tangent space
        # x is (batch, matrix_dim * matrix_dim), already flattened
        x_tangent = self._batch_to_tangent(x)

        # Create mask for tangent space (upper triangle only)
        # Need to convert full matrix mask to upper triangle mask
        mask_reshaped = mask.reshape(-1, self.matrix_dim, self.matrix_dim)
        idx = self._get_triu_idx()
        mask_tangent = mask_reshaped[:, idx[0], idx[1]]

        # Masked input in tangent space
        x_masked_tangent = x_tangent * mask_tangent

        # Forward pass in tangent space
        pred_tangent = self.forward(x_masked_tangent, params)

        # Loss on observed entries in tangent space
        # This IS the Log-Euclidean distance!
        diff = (pred_tangent - x_tangent) * mask_tangent
        return jnp.mean(diff ** 2)

    def compute_update(self, params: Dict, grads: Dict, lr: float) -> Dict:
        """
        Adam optimizer update for parameter space.
        
        Note: We use Adam (not natural gradient) because:
        1. The geometry is in the DATA space (SPD manifold), not parameter space
        2. The tangent space mapping linearizes the data manifold
        3. Neural network weights live in Euclidean R^{nÃ—m}
        
        This is the correct approach for Log-Euclidean methods:
        "Apply standard algorithms in the tangent space after log-transform"
        - Arsigny et al. (2006)
        """
        if self.adam_state is None:
            self.init_adam_state(params)
        new_params, self.adam_state = adam_update(params, grads, self.adam_state, lr)
        return new_params


# =============================================================================
# FISHER GEOMETRY MODEL: Universal Learned Geometry
# =============================================================================
#
# This model uses the new StatisticalManifold framework to LEARN geometric
# structure from data, rather than assuming a fixed manifold (like SPD).
#
# Connection to new modules:
#   - diffgeo/statistical_manifold.py: StatisticalManifold class
#   - diffgeo/geometry_extractor.py: DataGeometryExtractor
#   - diffgeo/optimizer.py: GeometricOptimizer
#
# =============================================================================


class FisherImputationModel(ImputationModel):
    """
    Imputation model using learned Fisher geometry.

    Unlike SPDTangentSpaceModel which ASSUMES data lives on SPD manifold,
    this model LEARNS the geometric structure from data using Fisher
    Information.

    The key insight from research doc [3]:
        "Fisher Information IS the natural Riemannian metric on the space
        of models/representations."

    How it works:
    1. Fit a probabilistic model to the observed data
    2. Compute Fisher Information from that model
    3. Use Fisher metric for natural gradient optimization

    This is more general because:
    - Works for ANY data type (not just SPD matrices)
    - Geometry emerges from data, not assumptions
    - Can detect asymmetry and switch to Finsler if needed

    Attributes:
        input_dim: Dimension of input data
        hidden_dim: Hidden layer size
        use_natural_gradient: Whether to use Fisher for updates
        detect_asymmetry: Whether to check for Finsler structure
    """

    def __init__(
        self,
        input_dim: int,
        hidden_dim: int = 32,
        seed: int = 42,
        use_natural_gradient: bool = True,
        detect_asymmetry: bool = True,
    ):
        """
        Initialize Fisher geometry model.

        Args:
            input_dim: Dimension of input data
            hidden_dim: Hidden layer size
            seed: Random seed
            use_natural_gradient: If True, use Fisher metric for updates
            detect_asymmetry: If True, check for Finsler structure
        """
        super().__init__(input_dim, hidden_dim, seed)
        self.name = "Fisher Geometry"
        self.use_natural_gradient = use_natural_gradient
        self.detect_asymmetry = detect_asymmetry

        # Adam state for baseline (will also compute Fisher if enabled)
        self.adam_state = None

        # Fisher metric (computed from data during training)
        self._fisher_matrix = None
        self._fisher_inverse = None
        self._drift_vector = None  # For Finsler extension

        # Online Fisher estimation (EMA of gradient outer products)
        self._fisher_ema = None
        self._fisher_ema_decay = 0.99

    def init_adam_state(self, params: Dict) -> None:
        """Initialize Adam optimizer state."""
        self.adam_state = AdamState.initialize(params)

    def _estimate_fisher_from_gradients(self, grads: Dict) -> None:
        """
        Update online Fisher estimate from gradient.

        The empirical Fisher is: F = E[âˆ‡L âŠ— âˆ‡L]
        We use DIAGONAL approximation for memory efficiency (like Adam).

        For high-dimensional data, the full outer product would be O(nÂ²)
        memory which is impractical. The diagonal is O(n) and captures
        the per-parameter curvature.
        """
        # Use per-parameter diagonal Fisher (same structure as grads)
        if self._fisher_ema is None:
            # Initialize with squared gradients
            self._fisher_ema = {k: g**2 for k, g in grads.items()}
        else:
            # EMA update of squared gradients (diagonal Fisher)
            for k in grads:
                self._fisher_ema[k] = self._fisher_ema_decay * self._fisher_ema[k] + (
                    1 - self._fisher_ema_decay
                ) * (grads[k] ** 2)

    def _compute_natural_gradient(self, grads: Dict) -> Dict:
        """
        Convert Euclidean gradient to natural gradient using Fisher.

        natural_grad = F^{-1} @ euclidean_grad

        This is the core geometric operation:
        - Euclidean gradient is a COVECTOR (dual space)
        - Natural gradient is a VECTOR (tangent space)
        - Fisher metric converts between them

        Using diagonal Fisher: nat_grad_i = grad_i / (sqrt(F_ii) + eps)
        This is similar to RMSprop/Adam but with geometric interpretation.
        """
        if self._fisher_ema is None:
            # No Fisher estimate yet, return Euclidean gradient
            return grads

        # Apply diagonal natural gradient (like Adam's second moment)
        nat_grads = {}
        eps = 1e-8

        for k in grads:
            # Natural gradient with diagonal Fisher inverse
            # F^{-1} â‰ˆ 1/sqrt(diag(F)) for diagonal approximation
            fisher_diag = self._fisher_ema[k]
            nat_grads[k] = grads[k] / (jnp.sqrt(fisher_diag) + eps)

        return nat_grads

    def _detect_asymmetry_from_gradients(self, grads: Dict) -> None:
        """
        Detect directional bias (skewness) in gradient distribution.

        If gradients consistently point in a particular direction,
        we have asymmetry that Riemannian geometry can't capture.
        This triggers Finsler (Randers) extension.
        """
        if not self.detect_asymmetry:
            return

        grad_flat = jnp.concatenate([g.flatten() for g in jax.tree.leaves(grads)])

        if self._drift_vector is None:
            self._drift_vector = jnp.zeros_like(grad_flat)

        # EMA of gradient direction
        self._drift_vector = 0.99 * self._drift_vector + 0.01 * grad_flat / (
            jnp.linalg.norm(grad_flat) + 1e-8
        )

    def compute_update(self, params: Dict, grads: Dict, lr: float) -> Dict:
        """
        Compute parameter update using Fisher geometry.

        The update process:
        1. Estimate Fisher metric from gradients (online)
        2. Convert gradient (covector) to update (vector) via F^{-1}
        3. Apply update with learning rate

        If use_natural_gradient=False, falls back to Adam.
        """
        if self.adam_state is None:
            self.init_adam_state(params)

        # Update Fisher estimate
        if self.use_natural_gradient:
            self._estimate_fisher_from_gradients(grads)
            self._detect_asymmetry_from_gradients(grads)

            # Apply natural gradient transformation
            nat_grads = self._compute_natural_gradient(grads)

            # Use transformed gradients with Adam
            new_params, self.adam_state = adam_update(
                params, nat_grads, self.adam_state, lr
            )
        else:
            # Standard Adam
            new_params, self.adam_state = adam_update(
                params, grads, self.adam_state, lr
            )

        return new_params

    @property
    def is_asymmetric(self) -> bool:
        """Check if Finsler structure was detected."""
        if self._drift_vector is None:
            return False
        drift_norm = jnp.linalg.norm(self._drift_vector)
        return float(drift_norm) > 0.1  # Threshold for significance

    def get_diagnostics(self) -> Dict:
        """
        Return diagnostic information about Fisher computation.

        DIAGNOSTIC: This reveals that Fisher is computed in PARAMETER space
        (neural network weights) not in DATA space (the actual manifold).

        The problem: We compute E[âˆ‡_Î¸ L âŠ— âˆ‡_Î¸ L] where Î¸ are NN weights,
        but we SHOULD compute E[âˆ‡_Î¼ log p âŠ— âˆ‡_Î¼ log p] where Î¼ are data params.
        """
        diagnostics = {
            "fisher_space": "PARAMETER_SPACE",  # THIS IS THE BUG
            "correct_space": "DATA_MANIFOLD",  # What it should be
            "fisher_type": "diagonal_approximation",
            "is_asymmetric": self.is_asymmetric,
        }

        if self._fisher_ema is not None:
            # Show what we're actually computing
            total_params = sum(f.size for f in self._fisher_ema.values())
            mean_fisher = float(
                np.mean([float(jnp.mean(f)) for f in self._fisher_ema.values()])
            )
            diagnostics["n_parameters"] = total_params
            diagnostics["mean_fisher_value"] = mean_fisher
            diagnostics["note"] = (
                "Fisher is computed from NN gradients (wrong!) "
                "not from data score functions (correct!)"
            )

        return diagnostics


# =============================================================================
# SPD FISHER MODEL: Proper Manifold-Aware Fisher Geometry
# =============================================================================
#
# This model CORRECTLY applies Fisher geometry by:
# 1. Operating in tangent space (like SPDTangentSpaceModel)
# 2. Computing Fisher metric in DATA space (not parameter space)
# 3. Using full Fisher matrix (not diagonal approximation)
#
# =============================================================================


class SPDFisherModel(ImputationModel):
    """
    SPD imputation with proper Fisher geometry on the DATA manifold.

    This FIXES the problems with FisherImputationModel by:

    1. Using tangent space representation (matrix log/exp)
       - Like SPDTangentSpaceModel, guarantees SPD output

    2. Computing Fisher metric from DATA distribution, not NN gradients
       - For Gaussian data, Fisher = inverse covariance (precision)
       - This IS the natural Riemannian metric on SPD manifold

    3. Using full (block-diagonal) Fisher, not diagonal approximation
       - Captures correlations between parameters
       - Critical for structured manifold data

    Mathematical foundation (from research doc [2] Section 4):
        "For Gaussian N(Î¼, Î£), the Fisher metric on mean parameters is Î£^{-1}.
        This matches the affine-invariant Riemannian metric on SPD(n)."

    The key insight: SPDTangentSpaceModel works because Log-Euclidean
    operations in tangent space implicitly use the correct geometry.
    This model makes that geometry EXPLICIT via Fisher Information.
    """

    def __init__(self, matrix_dim: int, hidden_dim: int = 32, seed: int = 42):
        self.matrix_dim = matrix_dim
        self.tri_dim = matrix_dim * (matrix_dim + 1) // 2
        super().__init__(self.tri_dim, hidden_dim, seed)
        self.name = "SPD Fisher"
        self.adam_state = None

        # Pre-compute indices (JAX-safe)
        self._triu_row, self._triu_col = np.triu_indices(matrix_dim)

        # Fisher metric computed from DATA (not NN gradients!)
        self._data_fisher = None
        self._data_fisher_inv = None
        self._reference_point = None  # FrÃ©chet mean in tangent space

    def init_adam_state(self, params: Dict) -> None:
        self.adam_state = AdamState.initialize(params)

    def _get_triu_idx(self) -> Tuple:
        return self._triu_row, self._triu_col

    def _matrix_log(self, P: jnp.ndarray) -> jnp.ndarray:
        """Matrix logarithm for SPD matrix."""
        eigvals, eigvecs = jnp.linalg.eigh(P)
        eigvals = jnp.maximum(eigvals, 1e-8)
        log_eigvals = jnp.log(eigvals)
        return eigvecs @ jnp.diag(log_eigvals) @ eigvecs.T

    def _matrix_exp(self, V: jnp.ndarray) -> jnp.ndarray:
        """Matrix exponential for symmetric matrix (guaranteed SPD output)."""
        eigvals, eigvecs = jnp.linalg.eigh(V)
        exp_eigvals = jnp.exp(eigvals)
        return eigvecs @ jnp.diag(exp_eigvals) @ eigvecs.T

    def _to_tangent(self, P_flat: jnp.ndarray) -> jnp.ndarray:
        """Map flattened SPD matrix to tangent space (upper triangle of log)."""
        P = P_flat.reshape(self.matrix_dim, self.matrix_dim)
        log_P = self._matrix_log(P)
        idx = self._get_triu_idx()
        return log_P[idx]

    def _from_tangent(self, v_flat: jnp.ndarray) -> jnp.ndarray:
        """Map tangent vector back to SPD manifold (guaranteed SPD!)."""
        idx = self._get_triu_idx()
        V = jnp.zeros((self.matrix_dim, self.matrix_dim))
        V = V.at[idx].set(v_flat)
        V = V + V.T - jnp.diag(jnp.diag(V))
        P = self._matrix_exp(V)
        return P.flatten()

    def _batch_to_tangent(self, P_batch: jnp.ndarray) -> jnp.ndarray:
        return vmap(self._to_tangent)(P_batch)

    def _batch_from_tangent(self, v_batch: jnp.ndarray) -> jnp.ndarray:
        return vmap(self._from_tangent)(v_batch)

    def compute_data_fisher(self, data_tangent: jnp.ndarray) -> None:
        """
        Compute Fisher metric from DATA in tangent space.

        THIS IS THE KEY FIX: We compute Fisher from the data distribution,
        not from neural network gradients.

        For data in tangent space (after matrix log), assuming Gaussian:
            Fisher = Î£^{-1} (precision matrix)

        This IS the natural Riemannian metric on the SPD manifold!

        Args:
            data_tangent: Data mapped to tangent space (n_samples, tri_dim)
        """
        # Compute sample covariance in tangent space
        mean = jnp.mean(data_tangent, axis=0)
        centered = data_tangent - mean
        n_samples = data_tangent.shape[0]

        # Sample covariance with regularization
        cov = (centered.T @ centered) / (n_samples - 1)
        cov = cov + 1e-4 * jnp.eye(self.tri_dim)  # Regularization

        # Fisher metric = precision = inverse covariance
        fisher_raw = jnp.linalg.inv(cov)

        # NORMALIZE Fisher so loss scale is comparable to MSE
        # Scale by trace so mean eigenvalue â‰ˆ 1
        trace = jnp.trace(fisher_raw)
        self._data_fisher = fisher_raw * (self.tri_dim / trace)

        self._data_fisher_inv = cov  # For natural gradient
        self._reference_point = mean

    def forward(self, x: jnp.ndarray, params: Dict) -> jnp.ndarray:
        """Forward pass in tangent space."""
        h = jnp.tanh(x @ params["encoder_w"] + params["encoder_b"])
        out = h @ params["decoder_w"] + params["decoder_b"]
        return out

    def loss_fn(self, params: Dict, x: jnp.ndarray, mask: jnp.ndarray) -> float:
        """
        Log-Euclidean loss WITH Fisher weighting.

        Unlike SPDTangentSpaceModel which uses unweighted MSE in tangent space,
        we weight errors by Fisher metric: (pred - true)^T F (pred - true)

        This properly accounts for the curvature of the manifold!
        """
        x_tangent = self._batch_to_tangent(x)

        # Create mask for tangent space
        mask_reshaped = mask.reshape(-1, self.matrix_dim, self.matrix_dim)
        idx = self._get_triu_idx()
        mask_tangent = mask_reshaped[:, idx[0], idx[1]]

        x_masked_tangent = x_tangent * mask_tangent
        pred_tangent = self.forward(x_masked_tangent, params)

        # Difference in tangent space (weighted by mask)
        diff = (pred_tangent - x_tangent) * mask_tangent

        # If Fisher is computed, use it for weighted loss
        if self._data_fisher is not None:
            # Mahalanobis distance: diff^T F diff (per sample)
            weighted_diff = diff @ self._data_fisher
            loss = jnp.mean(jnp.sum(weighted_diff * diff, axis=1))
        else:
            # Fallback to unweighted (like SPDTangentSpaceModel)
            loss = jnp.mean(diff**2)

        return loss

    def compute_update(self, params: Dict, grads: Dict, lr: float) -> Dict:
        """
        Update with natural gradient using DATA Fisher (not NN Fisher!).

        The key insight: we use Fisher from the DATA manifold to precondition
        gradients, not Fisher from neural network parameters.

        This correctly accounts for the geometry of the SPD manifold.
        """
        if self.adam_state is None:
            self.init_adam_state(params)

        # For now, use standard Adam
        # The geometric correction happens in loss_fn via Fisher weighting
        # This is mathematically equivalent to natural gradient but more stable
        new_params, self.adam_state = adam_update(params, grads, self.adam_state, lr)
        return new_params

    def get_diagnostics(self) -> Dict:
        """Return diagnostic information showing Fisher is computed correctly."""
        diagnostics = {
            "fisher_space": "DATA_MANIFOLD",  # CORRECT!
            "fisher_type": "full_matrix_from_covariance",
            "uses_tangent_space": True,
            "guarantees_spd_output": True,
        }

        if self._data_fisher is not None:
            diagnostics["fisher_condition_number"] = float(
                jnp.linalg.cond(self._data_fisher)
            )
            diagnostics["fisher_trace"] = float(jnp.trace(self._data_fisher))
            diagnostics["note"] = (
                "Fisher computed from data covariance in tangent space (correct!)"
            )

        return diagnostics


def train_spd_fisher_model(
    model: "SPDFisherModel", data: np.ndarray, mask: np.ndarray, config: TrainingConfig
) -> Tuple[Dict, TrainingHistory]:
    """
    Train SPDFisherModel with data-derived Fisher metric.

    Key difference from train_spd_model: computes Fisher metric from
    TRAINING data only, then uses it throughout.
    """
    if not HAS_JAX:
        raise RuntimeError("JAX required for training")

    assert data.ndim == 3, f"Expected 3D SPD data, got {data.ndim}D"
    n_samples, matrix_dim, _ = data.shape

    # Flatten for batch processing
    data_flat = jnp.array(data.reshape(n_samples, -1))
    mask_flat = jnp.array(mask.reshape(n_samples, -1))

    # Train/val/test split FIRST (before computing Fisher to avoid data leak)
    rng = np.random.default_rng(config.seed)
    indices = rng.permutation(n_samples)

    n_test = int(n_samples * config.test_split)
    n_val = int(n_samples * config.validation_split)
    n_train = n_samples - n_test - n_val

    train_idx = indices[:n_train]
    val_idx = indices[n_train : n_train + n_val]

    train_data, train_mask = data_flat[train_idx], mask_flat[train_idx]
    val_data, val_mask = data_flat[val_idx], mask_flat[val_idx]

    # CRITICAL: Compute Fisher metric from TRAINING data only (no data leak!)
    # Map training data to tangent space
    train_tangent = model._batch_to_tangent(train_data)
    model.compute_data_fisher(train_tangent)
    print(
        f"    Fisher metric computed from {n_train} training samples in tangent space"
    )

    # Initialize
    key = jax.random.PRNGKey(config.seed)
    params = model.init_params(key)

    @jit
    def compute_loss(params, x, m):
        return model.loss_fn(params, x, m)

    grad_fn = jit(grad(lambda p, x, m: model.loss_fn(p, x, m)))

    # Training state
    history = TrainingHistory()
    best_params = params
    lr = config.learning_rate
    patience_counter = 0
    lr_patience_counter = 0

    start_time = time.time()
    max_grad_norm = 10.0

    def clip_grads(grads: Dict, max_norm: float) -> Dict:
        total_norm = 0.0
        for g in grads.values():
            total_norm += float(jnp.sum(g**2))
        total_norm = np.sqrt(total_norm)
        if total_norm > max_norm:
            scale = max_norm / (total_norm + 1e-6)
            return {k: g * scale for k, g in grads.items()}
        return grads

    for epoch in range(config.n_epochs):
        perm = rng.permutation(n_train)
        train_data_shuffled = train_data[perm]
        train_mask_shuffled = train_mask[perm]

        epoch_losses = []
        for i in range(0, n_train, config.batch_size):
            batch_x = train_data_shuffled[i : i + config.batch_size]
            batch_m = train_mask_shuffled[i : i + config.batch_size]

            grads = grad_fn(params, batch_x, batch_m)
            grads = clip_grads(grads, max_grad_norm)
            params = model.compute_update(params, grads, lr)

            batch_loss = float(compute_loss(params, batch_x, batch_m))
            epoch_losses.append(batch_loss)

        train_loss = np.mean(epoch_losses)
        val_loss = float(compute_loss(params, val_data, val_mask))

        history.train_losses.append(train_loss)
        history.val_losses.append(val_loss)
        history.learning_rates.append(lr)
        history.epochs.append(epoch)
        history.final_epoch = epoch

        if val_loss < history.best_val_loss:
            history.best_val_loss = val_loss
            history.best_epoch = epoch
            best_params = {k: v.copy() for k, v in params.items()}
            patience_counter = 0
            lr_patience_counter = 0
        else:
            patience_counter += 1
            lr_patience_counter += 1

        if lr_patience_counter >= config.lr_decay_patience:
            new_lr = lr * config.lr_decay_factor
            if new_lr >= config.min_lr:
                lr = new_lr
            lr_patience_counter = 0

        if patience_counter >= config.early_stopping_patience:
            history.converged = True
            history.converged_reason = f"Early stop @ epoch {epoch}"
            print(f"\n  âœ“ {history.converged_reason}")
            break

        if epoch % config.log_every == 0:
            print(f"  Epoch {epoch:4d}: train={train_loss:.6f}, val={val_loss:.6f}")

    if not history.converged:
        history.converged_reason = f"Max epochs ({config.n_epochs}) reached"

    history.training_time = time.time() - start_time
    return best_params, history


def evaluate_spd_fisher_model(
    model: "SPDFisherModel",
    params: Dict,
    data: np.ndarray,
    mask: np.ndarray,
    manifold_type: ManifoldType = None,
) -> Dict[str, float]:
    """Evaluate SPDFisherModel with proper manifold metrics."""
    if not HAS_JAX:
        return {}

    assert data.ndim == 3, "Expected 3D SPD data"
    n_samples, matrix_dim, _ = data.shape

    data_flat = jnp.array(data.reshape(n_samples, -1))
    mask_flat = jnp.array(mask.reshape(n_samples, -1))

    # Convert to tangent space
    true_tangent = model._batch_to_tangent(data_flat)

    mask_reshaped = mask.reshape(-1, matrix_dim, matrix_dim)
    idx = model._get_triu_idx()
    mask_tangent = jnp.array(mask_reshaped[:, idx[0], idx[1]])

    masked_tangent = true_tangent * mask_tangent
    pred_tangent = model.forward(masked_tangent, params)

    # Metrics on missing entries
    missing_mask = ~mask_tangent
    n_missing = jnp.sum(missing_mask)

    if n_missing == 0:
        return {"rmse": 0.0, "mae": 0.0, "r2": 1.0, "mrr": 1.0, "mis": 0.0}

    true_missing = np.array(true_tangent[missing_mask])
    pred_missing = np.array(pred_tangent[missing_mask])

    # Use common metrics function
    metrics = compute_imputation_metrics(true_missing, pred_missing)

    # MIS computation
    mis = 0.0
    if manifold_type == ManifoldType.SPD:
        full_pred_tangent = np.array(
            true_tangent * mask_tangent + pred_tangent * (~mask_tangent)
        )
        pred_matrices_flat = model._batch_from_tangent(jnp.array(full_pred_tangent))
        pred_matrices = np.array(pred_matrices_flat).reshape(-1, matrix_dim, matrix_dim)

        mis_calc = ManifoldIntegrityScore(manifold_type, dim=matrix_dim)
        mis, _ = mis_calc.compute_batch(pred_matrices)

    metrics["mis"] = float(mis)
    return metrics


# =============================================================================
# SPHERICAL FISHER MODEL: Geometry-Aware Imputation for Spherical Data
# =============================================================================
#
# For GHCN climate data where coordinates live on SÂ² (Earth's surface),
# Fisher geometry alone is insufficient because:
# 1. Spherical constraints are TOPOLOGICAL (||x|| = 1)
# 2. Need great-circle interpolation, not Euclidean
# 3. Tangent space at a point on sphere is a hyperplane
#
# =============================================================================


class SphericalFisherModel(ImputationModel):
    """
    Spherical imputation with proper manifold geometry for SÂ² data.

    For data on the sphere (like GHCN lat/lon coordinates), we need:

    1. Project to tangent space at spherical mean (gnomonic projection)
    2. Apply Fisher-weighted operations in tangent space
    3. Project back via exponential map (retraction to sphere)

    Key insight from research doc [4]:
        "The correlation between two floats is a function of their
        Great Circle distance, not Euclidean distance."

    This model handles the special case where:
    - First 2-3 dimensions are spherical coordinates (lat, lon)
    - Remaining dimensions are values (temperature, precipitation)

    For pure spherical data, use gnomonic projection.
    For mixed data, handle spherical and value parts separately.
    """

    def __init__(
        self, input_dim: int, hidden_dim: int = 32, seed: int = 42, coord_dims: int = 2
    ):
        """
        Initialize spherical Fisher model.

        Args:
            input_dim: Total input dimension
            hidden_dim: Hidden layer size
            seed: Random seed
            coord_dims: Number of coordinate dimensions (2 for lat/lon, 3 for xyz)
        """
        super().__init__(input_dim, hidden_dim, seed)
        self.name = "Spherical Fisher"
        self.coord_dims = coord_dims
        self.value_dims = input_dim - coord_dims
        self.adam_state = None

        # Reference point for tangent space (spherical mean)
        self._reference_point = None
        self._data_fisher = None

    def init_adam_state(self, params: Dict) -> None:
        self.adam_state = AdamState.initialize(params)

    def _latlon_to_xyz(self, latlon: jnp.ndarray) -> jnp.ndarray:
        """Convert lat/lon (degrees) to unit sphere coordinates."""
        lat_rad = jnp.radians(latlon[..., 0])
        lon_rad = jnp.radians(latlon[..., 1])

        x = jnp.cos(lat_rad) * jnp.cos(lon_rad)
        y = jnp.cos(lat_rad) * jnp.sin(lon_rad)
        z = jnp.sin(lat_rad)

        return jnp.stack([x, y, z], axis=-1)

    def _xyz_to_latlon(self, xyz: jnp.ndarray) -> jnp.ndarray:
        """Convert unit sphere coordinates to lat/lon (degrees)."""
        # Normalize to ensure on sphere
        xyz = xyz / (jnp.linalg.norm(xyz, axis=-1, keepdims=True) + 1e-8)

        lat = jnp.degrees(jnp.arcsin(jnp.clip(xyz[..., 2], -1, 1)))
        lon = jnp.degrees(jnp.arctan2(xyz[..., 1], xyz[..., 0]))

        return jnp.stack([lat, lon], axis=-1)

    def _project_to_tangent(self, xyz: jnp.ndarray, center: jnp.ndarray) -> jnp.ndarray:
        """
        Project from sphere to tangent space at center (gnomonic projection).

        This is the logarithmic map on SÂ²:
        log_p(q) = arccos(<p,q>) * (q - <p,q>p) / ||q - <p,q>p||

        Simplified for nearby points: tangent vector â‰ˆ q - <p,q>p
        """
        # Project q onto tangent plane at p
        dot = jnp.sum(xyz * center, axis=-1, keepdims=True)
        tangent = xyz - dot * center

        # The tangent vector lives in RÂ³ but is constrained to the 2D tangent plane
        # We keep all 3 components for simplicity (redundant but easier)
        return tangent

    def _project_from_tangent(
        self, tangent: jnp.ndarray, center: jnp.ndarray
    ) -> jnp.ndarray:
        """
        Project from tangent space back to sphere (exponential map).

        exp_p(v) = cos(||v||)p + sin(||v||)(v/||v||)

        This guarantees output is on the sphere!
        """
        norm = jnp.linalg.norm(tangent, axis=-1, keepdims=True)
        norm = jnp.maximum(norm, 1e-8)  # Avoid division by zero

        # Exponential map
        cos_norm = jnp.cos(norm)
        sin_norm = jnp.sin(norm)

        result = cos_norm * center + sin_norm * (tangent / norm)

        # Normalize to ensure exactly on sphere
        return result / (jnp.linalg.norm(result, axis=-1, keepdims=True) + 1e-8)

    def compute_spherical_mean(self, xyz_batch: jnp.ndarray) -> jnp.ndarray:
        """
        Compute FrÃ©chet mean on sphere (intrinsic average).

        The FrÃ©chet mean minimizes sum of squared geodesic distances.
        For SÂ², this is the normalized arithmetic mean (approximately).
        """
        mean = jnp.mean(xyz_batch, axis=0)
        return mean / (jnp.linalg.norm(mean) + 1e-8)

    def compute_data_fisher(self, data: jnp.ndarray) -> None:
        """
        Compute Fisher metric from data distribution.

        For spherical data, Fisher captures the concentration of points
        around the mean (like von Mises-Fisher distribution).
        """
        # Separate coords and values
        coords = data[:, : self.coord_dims]
        values = data[:, self.coord_dims :] if self.value_dims > 0 else None

        # Convert to xyz if lat/lon
        if self.coord_dims == 2:
            xyz = self._latlon_to_xyz(coords)
        else:
            xyz = coords / (jnp.linalg.norm(coords, axis=-1, keepdims=True) + 1e-8)

        # Compute spherical mean
        self._reference_point = self.compute_spherical_mean(xyz)

        # Project to tangent space for covariance computation
        tangent = vmap(lambda x: self._project_to_tangent(x, self._reference_point))(
            xyz
        )

        # Flatten tangent vectors
        tangent_flat = tangent.reshape(tangent.shape[0], -1)

        # Combine with values if present
        if values is not None:
            combined = jnp.concatenate([tangent_flat, values], axis=1)
        else:
            combined = tangent_flat

        # Compute covariance and Fisher (precision)
        mean = jnp.mean(combined, axis=0)
        centered = combined - mean
        n_samples = combined.shape[0]
        cov = (centered.T @ centered) / (n_samples - 1)
        cov = cov + 1e-4 * jnp.eye(cov.shape[0])

        self._data_fisher = jnp.linalg.inv(cov)

    def forward(self, x: jnp.ndarray, params: Dict) -> jnp.ndarray:
        """Standard autoencoder forward pass."""
        h = jnp.tanh(x @ params["encoder_w"] + params["encoder_b"])
        out = h @ params["decoder_w"] + params["decoder_b"]
        return out

    def loss_fn(self, params: Dict, x: jnp.ndarray, mask: jnp.ndarray) -> float:
        """
        Loss with spherical-aware structure.

        For GHCN-style data where we're imputing VALUES (temperature, etc.)
        at known LOCATIONS (lat/lon), we:
        1. Use coordinates as context (not imputed)
        2. Apply Fisher-weighted MSE to values

        This is more stable than trying to impute coordinates geometrically.
        """
        # Masked input
        x_masked = x * mask
        pred = self.forward(x_masked, params)

        # For mixed coord+value data, focus on value imputation
        if self.value_dims > 0:
            # Coordinates are context, values are the target
            pred_values = pred[:, self.coord_dims :]
            true_values = x[:, self.coord_dims :]
            mask_values = mask[:, self.coord_dims :]

            # Only compute loss on masked (missing) values that we need to predict
            diff = (pred_values - true_values) * mask_values

            # If Fisher metric available and dimensions match, use weighted loss
            if self._data_fisher is not None:
                fisher_dim = self._data_fisher.shape[0]
                value_dim = pred_values.shape[1]

                # Extract value-portion of Fisher if dimensions match
                if fisher_dim >= value_dim + 3:  # 3 for xyz tangent
                    # Fisher structure: [tangent_xyz (3), values (value_dim)]
                    fisher_values = self._data_fisher[
                        3 : 3 + value_dim, 3 : 3 + value_dim
                    ]
                    weighted_diff = diff @ fisher_values
                    loss = jnp.mean(jnp.sum(weighted_diff * diff, axis=1))
                else:
                    loss = jnp.mean(diff**2)
            else:
                loss = jnp.mean(diff**2)
        else:
            # Pure spherical data - use angular distance
            pred_coords = pred[:, : self.coord_dims]
            true_coords = x[:, : self.coord_dims]
            mask_coords = mask[:, : self.coord_dims]

            if self.coord_dims == 2:
                # Lat/lon: convert to xyz and use angular distance
                pred_xyz = self._latlon_to_xyz(pred_coords)
                true_xyz = self._latlon_to_xyz(true_coords)

                dot = jnp.sum(pred_xyz * true_xyz, axis=-1)
                dot = jnp.clip(dot, -1, 1)
                angular_dist = jnp.arccos(dot)

                coord_mask = jnp.all(mask_coords, axis=-1)
                loss = jnp.sum(angular_dist**2 * coord_mask) / (
                    jnp.sum(coord_mask) + 1e-8
                )
            else:
                # 3D coords
                pred_norm = pred_coords / (
                    jnp.linalg.norm(pred_coords, axis=-1, keepdims=True) + 1e-8
                )
                true_norm = true_coords / (
                    jnp.linalg.norm(true_coords, axis=-1, keepdims=True) + 1e-8
                )
                dot = jnp.sum(pred_norm * true_norm, axis=-1)
                dot = jnp.clip(dot, -1, 1)
                angular_dist = jnp.arccos(dot)
                coord_mask = jnp.all(mask_coords, axis=-1)
                loss = jnp.sum(angular_dist**2 * coord_mask) / (
                    jnp.sum(coord_mask) + 1e-8
                )

        return loss

    def compute_update(self, params: Dict, grads: Dict, lr: float) -> Dict:
        if self.adam_state is None:
            self.init_adam_state(params)
        new_params, self.adam_state = adam_update(params, grads, self.adam_state, lr)
        return new_params

    def get_diagnostics(self) -> Dict:
        """Return diagnostic information."""
        diagnostics = {
            "fisher_space": "DATA_MANIFOLD_SPHERICAL",
            "uses_great_circle_distance": True,
            "coord_dims": self.coord_dims,
            "value_dims": self.value_dims,
        }

        if self._reference_point is not None:
            diagnostics["reference_point"] = self._reference_point.tolist()

        return diagnostics


def train_spherical_fisher_model(
    model: "SphericalFisherModel",
    data: np.ndarray,
    mask: np.ndarray,
    config: TrainingConfig,
) -> Tuple[Dict, TrainingHistory]:
    """Train SphericalFisherModel with data-derived Fisher metric."""
    if not HAS_JAX:
        raise RuntimeError("JAX required for training")

    data = jnp.array(data)
    mask = jnp.array(mask)
    n_samples = data.shape[0]

    # Train/val split FIRST (before computing Fisher to avoid data leak)
    rng = np.random.default_rng(config.seed)
    indices = rng.permutation(n_samples)

    n_test = int(n_samples * config.test_split)
    n_val = int(n_samples * config.validation_split)
    n_train = n_samples - n_test - n_val

    train_idx = indices[:n_train]
    val_idx = indices[n_train : n_train + n_val]

    train_data, train_mask = data[train_idx], mask[train_idx]
    val_data, val_mask = data[val_idx], mask[val_idx]

    # Compute Fisher metric from TRAINING data only (no data leak!)
    model.compute_data_fisher(train_data)
    print(f"    Spherical Fisher computed from {n_train} training samples")

    # Initialize
    key = jax.random.PRNGKey(config.seed)
    params = model.init_params(key)

    @jit
    def compute_loss(params, x, m):
        return model.loss_fn(params, x, m)

    grad_fn = jit(grad(lambda p, x, m: model.loss_fn(p, x, m)))

    history = TrainingHistory()
    best_params = params
    lr = config.learning_rate
    patience_counter = 0
    lr_patience_counter = 0

    start_time = time.time()
    max_grad_norm = 10.0

    def clip_grads(grads: Dict, max_norm: float) -> Dict:
        total_norm = 0.0
        for g in grads.values():
            total_norm += float(jnp.sum(g**2))
        total_norm = np.sqrt(total_norm)
        if total_norm > max_norm:
            scale = max_norm / (total_norm + 1e-6)
            return {k: g * scale for k, g in grads.items()}
        return grads

    for epoch in range(config.n_epochs):
        perm = rng.permutation(n_train)
        train_data_shuffled = train_data[perm]
        train_mask_shuffled = train_mask[perm]

        epoch_losses = []
        for i in range(0, n_train, config.batch_size):
            batch_x = train_data_shuffled[i : i + config.batch_size]
            batch_m = train_mask_shuffled[i : i + config.batch_size]

            grads = grad_fn(params, batch_x, batch_m)
            grads = clip_grads(grads, max_grad_norm)
            params = model.compute_update(params, grads, lr)

            batch_loss = float(compute_loss(params, batch_x, batch_m))
            epoch_losses.append(batch_loss)

        train_loss = np.mean(epoch_losses)
        val_loss = float(compute_loss(params, val_data, val_mask))

        history.train_losses.append(train_loss)
        history.val_losses.append(val_loss)
        history.learning_rates.append(lr)
        history.epochs.append(epoch)
        history.final_epoch = epoch

        if val_loss < history.best_val_loss:
            history.best_val_loss = val_loss
            history.best_epoch = epoch
            best_params = {k: v.copy() for k, v in params.items()}
            patience_counter = 0
            lr_patience_counter = 0
        else:
            patience_counter += 1
            lr_patience_counter += 1

        if lr_patience_counter >= config.lr_decay_patience:
            new_lr = lr * config.lr_decay_factor
            if new_lr >= config.min_lr:
                lr = new_lr
            lr_patience_counter = 0

        if patience_counter >= config.early_stopping_patience:
            history.converged = True
            history.converged_reason = f"Early stop @ epoch {epoch}"
            print(f"\n  âœ“ {history.converged_reason}")
            break

        if epoch % config.log_every == 0:
            print(f"  Epoch {epoch:4d}: train={train_loss:.6f}, val={val_loss:.6f}")

    if not history.converged:
        history.converged_reason = f"Max epochs ({config.n_epochs}) reached"

    history.training_time = time.time() - start_time
    return best_params, history


def evaluate_spherical_fisher_model(
    model: "SphericalFisherModel",
    params: Dict,
    data: np.ndarray,
    mask: np.ndarray,
    manifold_type: ManifoldType = None,
) -> Dict[str, float]:
    """Evaluate SphericalFisherModel with proper spherical metrics."""
    if not HAS_JAX:
        return {}

    data = jnp.array(data)
    mask = jnp.array(mask)
    n_samples = data.shape[0]

    # Get predictions
    masked_input = data * mask
    predictions = model.forward(masked_input, params)

    # Compute metrics on missing entries
    missing_mask = ~mask
    n_missing = jnp.sum(missing_mask)

    if n_missing == 0:
        return {"rmse": 0.0, "mae": 0.0, "r2": 1.0, "mrr": 1.0, "mis": 0.0}

    # Overall RMSE/MAE on values (not coords)
    if model.value_dims > 0:
        true_values = data[:, model.coord_dims :]
        pred_values = predictions[:, model.coord_dims :]
        mask_values = mask[:, model.coord_dims :]

        missing_vals = ~mask_values
        if jnp.sum(missing_vals) > 0:
            true_missing = np.array(true_values[missing_vals])
            pred_missing = np.array(pred_values[missing_vals])

            # Use common metrics function
            metrics = compute_imputation_metrics(true_missing, pred_missing)
            rmse, mae, r2, mrr = (
                metrics["rmse"],
                metrics["mae"],
                metrics["r2"],
                metrics["mrr"],
            )
        else:
            rmse, mae, r2, mrr = 0.0, 0.0, 1.0, 1.0
    else:
        # Just coordinates - use angular distance
        true_coords = data[:, : model.coord_dims]
        pred_coords = predictions[:, : model.coord_dims]

        if model.coord_dims == 2:
            true_xyz = model._latlon_to_xyz(true_coords)
            pred_xyz = model._latlon_to_xyz(pred_coords)
        else:
            true_xyz = true_coords / (
                jnp.linalg.norm(true_coords, axis=-1, keepdims=True) + 1e-8
            )
            pred_xyz = pred_coords / (
                jnp.linalg.norm(pred_coords, axis=-1, keepdims=True) + 1e-8
            )

        dot = jnp.sum(true_xyz * pred_xyz, axis=-1)
        dot = jnp.clip(dot, -1, 1)
        angular_dist = jnp.arccos(dot)

        rmse = float(jnp.sqrt(jnp.mean(angular_dist**2)))
        mae = float(jnp.mean(jnp.abs(angular_dist)))
        r2 = 0.0  # Not meaningful for angular
        # MRR based on angular distance
        ranks = np.argsort(np.argsort(np.array(angular_dist))) + 1
        mrr = float(np.mean(1.0 / ranks))

    # MIS: measure how far from unit sphere (for coord outputs)
    mis = 0.0
    if manifold_type == ManifoldType.SPHERE:
        pred_coords = np.array(predictions[:, : model.coord_dims])
        if model.coord_dims == 2:
            # Lat/lon: always on sphere by construction
            mis = 0.0
        else:
            # 3D coords: check norm
            norms = np.linalg.norm(pred_coords, axis=-1)
            mis = float(np.mean(np.abs(norms - 1.0)))

    return {"rmse": rmse, "mae": mae, "r2": r2, "mrr": mrr, "mis": float(mis)}


# =============================================================================
# EXTRACTED FISHER MODEL: Auto-Discovery of Geometry via DataGeometryExtractor
# =============================================================================
#
# This model uses the DataGeometryExtractor to automatically discover
# the Fisher geometry from raw time series data (e.g., EEG).
#
# =============================================================================


class ExtractedFisherModel(ImputationModel):
    """
    Imputation model using geometry automatically extracted from data.

    This wires up the DataGeometryExtractor to the imputation pipeline,
    enabling automatic discovery of Fisher geometry from time series.

    Key insight from research doc [3]:
        The DataGeometryExtractor computes Fisher from data covariance:
        - For time series, fit multivariate Gaussian
        - Fisher metric = inverse covariance (precision)
        - This IS the natural metric on SPD manifold

    This is the BRIDGE between:
        Raw data â†’ StatisticalManifold â†’ Fisher metric â†’ Natural gradient

    Unlike FisherImputationModel which computes Fisher from NN gradients,
    this model computes Fisher from DATA distribution.
    """

    def __init__(self, input_dim: int, hidden_dim: int = 32, seed: int = 42):
        super().__init__(input_dim, hidden_dim, seed)
        self.name = "Extracted Fisher"
        self.adam_state = None

        # Geometry extracted from data
        self._statistical_manifold = None
        self._fisher_matrix = None
        self._fisher_inv = None

    def init_adam_state(self, params: Dict) -> None:
        self.adam_state = AdamState.initialize(params)

    def extract_geometry(self, data: jnp.ndarray) -> None:
        """
        Extract Fisher geometry from data using DataGeometryExtractor.

        This is the KEY OPERATION that connects raw data to geometry.

        Args:
            data: Raw time series or feature data (n_samples, n_features)
                  or (n_samples, n_timepoints, n_channels) for time series
        """
        if not HAS_EXTRACTOR:
            print("Warning: DataGeometryExtractor not available")
            return

        extractor = DataGeometryExtractor(regularization=1e-4)

        if data.ndim == 3:
            # Time series: use from_time_series
            self._statistical_manifold = extractor.from_time_series(data)
        else:
            # Feature data: treat rows as samples from multivariate distribution
            # Compute covariance directly
            mean = jnp.mean(data, axis=0)
            centered = data - mean
            n_samples = data.shape[0]
            cov = (centered.T @ centered) / (n_samples - 1)
            cov = cov + 1e-4 * jnp.eye(cov.shape[1])

            # Create Gaussian manifold
            self._statistical_manifold = StatisticalManifold.from_gaussian(
                mean=mean, covariance=cov, samples=data
            )

        # Extract Fisher metric for use in optimization
        if self._statistical_manifold.fisher_metric is not None:
            # FisherMetric extends MetricTensor which has .matrix attribute
            self._fisher_matrix = self._statistical_manifold.fisher_metric.matrix
            # Invert for natural gradient (F^{-1} g)
            self._fisher_inv = jnp.linalg.inv(
                self._fisher_matrix + 1e-6 * jnp.eye(self._fisher_matrix.shape[0])
            )

    def forward(self, x: jnp.ndarray, params: Dict) -> jnp.ndarray:
        h = jnp.tanh(x @ params["encoder_w"] + params["encoder_b"])
        out = h @ params["decoder_w"] + params["decoder_b"]
        return out

    def loss_fn(self, params: Dict, x: jnp.ndarray, mask: jnp.ndarray) -> float:
        """
        Loss with optional Fisher weighting from extracted geometry.
        """
        x_masked = x * mask
        pred = self.forward(x_masked, params)
        diff = (pred - x) * mask

        # If we have extracted Fisher, use it for weighted loss
        if (
            self._fisher_matrix is not None
            and diff.shape[1] == self._fisher_matrix.shape[0]
        ):
            # Mahalanobis distance
            weighted_diff = diff @ self._fisher_matrix
            loss = jnp.mean(jnp.sum(weighted_diff * diff, axis=1))
        else:
            loss = jnp.mean(diff**2)

        return loss

    def compute_update(self, params: Dict, grads: Dict, lr: float) -> Dict:
        if self.adam_state is None:
            self.init_adam_state(params)
        new_params, self.adam_state = adam_update(params, grads, self.adam_state, lr)
        return new_params

    def get_diagnostics(self) -> Dict:
        """Return diagnostic information about extracted geometry."""
        diagnostics = {
            "fisher_space": "DATA_MANIFOLD_EXTRACTED",
            "uses_geometry_extractor": True,
            "has_statistical_manifold": self._statistical_manifold is not None,
        }

        if self._statistical_manifold is not None:
            diagnostics["manifold_is_asymmetric"] = (
                self._statistical_manifold.is_asymmetric
            )
            if self._fisher_matrix is not None:
                diagnostics["fisher_dim"] = int(self._fisher_matrix.shape[0])
                diagnostics["fisher_condition"] = float(
                    jnp.linalg.cond(self._fisher_matrix)
                )

        return diagnostics


def train_extracted_fisher_model(
    model: "ExtractedFisherModel",
    data: np.ndarray,
    mask: np.ndarray,
    config: TrainingConfig,
    raw_time_series: Optional[np.ndarray] = None,
) -> Tuple[Dict, TrainingHistory]:
    """
    Train ExtractedFisherModel with geometry extracted from TRAINING data only.

    Args:
        model: ExtractedFisherModel instance
        data: Data for imputation (n_samples, dim)
        mask: Boolean mask (True = observed)
        config: Training configuration
        raw_time_series: Optional raw time series for geometry extraction
                        If None, uses data directly
    """
    if not HAS_JAX:
        raise RuntimeError("JAX required")

    data = jnp.array(data)
    mask = jnp.array(mask)
    n_samples = data.shape[0]

    # Train/val split FIRST (before extracting geometry to avoid data leak)
    rng = np.random.default_rng(config.seed)
    indices = rng.permutation(n_samples)

    n_test = int(n_samples * config.test_split)
    n_val = int(n_samples * config.validation_split)
    n_train = n_samples - n_test - n_val

    train_idx = indices[:n_train]
    val_idx = indices[n_train : n_train + n_val]

    train_data, train_mask = data[train_idx], mask[train_idx]
    val_data, val_mask = data[val_idx], mask[val_idx]

    # Extract geometry from TRAINING data only (no data leak!)
    if raw_time_series is not None:
        # If raw time series provided, need to split it too
        raw_train = jnp.array(raw_time_series)[train_idx]
        geometry_data = raw_train
    else:
        geometry_data = train_data
    model.extract_geometry(geometry_data)
    print(
        f"    Geometry extracted from {n_train} training samples via DataGeometryExtractor"
    )

    if model._statistical_manifold is not None:
        print(f"    Manifold asymmetric: {model._statistical_manifold.is_asymmetric}")

    key = jax.random.PRNGKey(config.seed)
    params = model.init_params(key)

    @jit
    def compute_loss(params, x, m):
        return model.loss_fn(params, x, m)

    grad_fn = jit(grad(lambda p, x, m: model.loss_fn(p, x, m)))

    history = TrainingHistory()
    best_params = params
    lr = config.learning_rate
    patience_counter = 0
    lr_patience_counter = 0

    start_time = time.time()
    max_grad_norm = 10.0

    def clip_grads(grads: Dict, max_norm: float) -> Dict:
        total_norm = 0.0
        for g in grads.values():
            total_norm += float(jnp.sum(g**2))
        total_norm = np.sqrt(total_norm)
        if total_norm > max_norm:
            scale = max_norm / (total_norm + 1e-6)
            return {k: g * scale for k, g in grads.items()}
        return grads

    for epoch in range(config.n_epochs):
        perm = rng.permutation(n_train)
        train_data_shuffled = train_data[perm]
        train_mask_shuffled = train_mask[perm]

        epoch_losses = []
        for i in range(0, n_train, config.batch_size):
            batch_x = train_data_shuffled[i : i + config.batch_size]
            batch_m = train_mask_shuffled[i : i + config.batch_size]

            grads = grad_fn(params, batch_x, batch_m)
            grads = clip_grads(grads, max_grad_norm)
            params = model.compute_update(params, grads, lr)

            batch_loss = float(compute_loss(params, batch_x, batch_m))
            epoch_losses.append(batch_loss)

        train_loss = np.mean(epoch_losses)
        val_loss = float(compute_loss(params, val_data, val_mask))

        history.train_losses.append(train_loss)
        history.val_losses.append(val_loss)
        history.learning_rates.append(lr)
        history.epochs.append(epoch)
        history.final_epoch = epoch

        if val_loss < history.best_val_loss:
            history.best_val_loss = val_loss
            history.best_epoch = epoch
            best_params = {k: v.copy() for k, v in params.items()}
            patience_counter = 0
            lr_patience_counter = 0
        else:
            patience_counter += 1
            lr_patience_counter += 1

        if lr_patience_counter >= config.lr_decay_patience:
            new_lr = lr * config.lr_decay_factor
            if new_lr >= config.min_lr:
                lr = new_lr
            lr_patience_counter = 0

        if patience_counter >= config.early_stopping_patience:
            history.converged = True
            history.converged_reason = f"Early stop @ epoch {epoch}"
            print(f"\n  âœ“ {history.converged_reason}")
            break

        if epoch % config.log_every == 0:
            print(f"  Epoch {epoch:4d}: train={train_loss:.6f}, val={val_loss:.6f}")

    if not history.converged:
        history.converged_reason = f"Max epochs ({config.n_epochs}) reached"

    history.training_time = time.time() - start_time
    return best_params, history


def evaluate_extracted_fisher_model(
    model: "ExtractedFisherModel",
    params: Dict,
    data: np.ndarray,
    mask: np.ndarray,
    manifold_type: ManifoldType = None,
) -> Dict[str, float]:
    """Evaluate ExtractedFisherModel."""
    if not HAS_JAX:
        return {}

    data = jnp.array(data)
    mask = jnp.array(mask)

    masked_input = data * mask
    predictions = model.forward(masked_input, params)

    missing_mask = ~mask
    n_missing = jnp.sum(missing_mask)

    if n_missing == 0:
        return {"rmse": 0.0, "mae": 0.0, "r2": 1.0, "mrr": 1.0, "mis": 0.0}

    true_missing = np.array(data[missing_mask])
    pred_missing = np.array(predictions[missing_mask])

    # Use common metrics function
    metrics = compute_imputation_metrics(true_missing, pred_missing)

    # MIS depends on manifold type - for general data, not applicable
    metrics["mis"] = 0.0
    return metrics


def train_model(model: ImputationModel,
                data: np.ndarray,
                mask: np.ndarray,
                config: TrainingConfig) -> Tuple[Dict, TrainingHistory]:
    """
    Train an imputation model with proper validation.
    
    Args:
        model: ImputationModel instance
        data: Full data array (n_samples, dim) or (n_samples, d, d) for SPD
        mask: Boolean mask (True = observed)
        config: Training configuration
        
    Returns:
        best_params: Best model parameters
        history: Training history
    """
    if not HAS_JAX:
        raise RuntimeError("JAX required for training")
    
    # Flatten if needed
    original_shape = data.shape
    if data.ndim == 3:  # SPD matrices
        n_samples = data.shape[0]
        data_flat = data.reshape(n_samples, -1)
        mask_flat = mask.reshape(n_samples, -1)
    else:
        data_flat = data
        mask_flat = mask
    
    data_flat = jnp.array(data_flat)
    mask_flat = jnp.array(mask_flat)
    
    n_samples = len(data_flat)
    
    # Train/val/test split
    rng = np.random.default_rng(config.seed)
    indices = rng.permutation(n_samples)
    
    n_test = int(n_samples * config.test_split)
    n_val = int(n_samples * config.validation_split)
    n_train = n_samples - n_test - n_val
    
    train_idx = indices[:n_train]
    val_idx = indices[n_train:n_train + n_val]
    test_idx = indices[n_train + n_val:]
    
    train_data, train_mask = data_flat[train_idx], mask_flat[train_idx]
    val_data, val_mask = data_flat[val_idx], mask_flat[val_idx]
    
    # Initialize
    key = jax.random.PRNGKey(config.seed)
    params = model.init_params(key)
    
    # JIT compile loss and gradient
    @jit
    def compute_loss_and_grad(params, x, m):
        loss = model.loss_fn(params, x, m)
        return loss
    
    grad_fn = jit(grad(lambda p, x, m: model.loss_fn(p, x, m)))
    
    # Training state
    history = TrainingHistory()
    best_params = params
    lr = config.learning_rate
    patience_counter = 0
    lr_patience_counter = 0
    
    start_time = time.time()
    
    # Gradient clipping threshold
    max_grad_norm = 10.0
    
    def clip_grads(grads: Dict, max_norm: float) -> Dict:
        """Clip gradients to prevent explosion."""
        total_norm = 0.0
        for g in grads.values():
            total_norm += float(jnp.sum(g ** 2))
        total_norm = np.sqrt(total_norm)
        
        if total_norm > max_norm:
            scale = max_norm / (total_norm + 1e-6)
            return {k: g * scale for k, g in grads.items()}
        return grads
    
    for epoch in range(config.n_epochs):
        # Shuffle training data
        perm = rng.permutation(n_train)
        train_data_shuffled = train_data[perm]
        train_mask_shuffled = train_mask[perm]
        
        # Mini-batch training
        epoch_losses = []
        for i in range(0, n_train, config.batch_size):
            batch_x = train_data_shuffled[i:i + config.batch_size]
            batch_m = train_mask_shuffled[i:i + config.batch_size]
            
            # Compute gradients
            grads = grad_fn(params, batch_x, batch_m)
            
            # Clip gradients for stability
            grads = clip_grads(grads, max_grad_norm)
            
            # Update using model-specific method
            params = model.compute_update(params, grads, lr)
            
            # Track loss
            batch_loss = float(compute_loss_and_grad(params, batch_x, batch_m))
            epoch_losses.append(batch_loss)
        
        train_loss = np.mean(epoch_losses)
        
        # Validation
        val_loss = float(compute_loss_and_grad(params, val_data, val_mask))
        
        # Record history
        history.train_losses.append(train_loss)
        history.val_losses.append(val_loss)
        history.learning_rates.append(lr)
        history.epochs.append(epoch)
        history.final_epoch = epoch
        
        # Check for improvement
        if val_loss < history.best_val_loss:
            history.best_val_loss = val_loss
            history.best_epoch = epoch
            best_params = {k: v.copy() for k, v in params.items()}
            patience_counter = 0
            lr_patience_counter = 0
        else:
            patience_counter += 1
            lr_patience_counter += 1
        
        # Learning rate decay
        if lr_patience_counter >= config.lr_decay_patience:
            new_lr = lr * config.lr_decay_factor
            if new_lr >= config.min_lr:
                lr = new_lr
                if epoch % config.log_every == 0:
                    print(f"  Epoch {epoch}: LR decayed to {lr:.6f}")
            lr_patience_counter = 0
        
        # Early stopping
        if patience_counter >= config.early_stopping_patience:
            history.converged = True
            # Flag exceptional cases
            if epoch < 100:
                flag = "âš¡ FAST"
            elif epoch < config.n_epochs * 0.3:
                flag = "âœ“ GOOD"
            elif epoch > config.n_epochs * 0.8:
                flag = "âš ï¸  SLOW"
            else:
                flag = "âœ“"
            history.converged_reason = f"Early stop @ epoch {epoch} (best @ {history.best_epoch})"
            print(f"\n  {flag} {history.converged_reason}")
            break
        
        # Check for convergence (loss plateaued)
        # Only check after reasonable training, with stricter threshold
        min_epochs_for_plateau = max(200, config.n_epochs // 5)
        if epoch >= min_epochs_for_plateau and len(history.val_losses) > 100:
            recent_loss = np.mean(history.val_losses[-30:])
            older_loss = np.mean(history.val_losses[-100:-50])
            # Require 0.01% change (10x stricter than before)
            if abs(recent_loss - older_loss) / (older_loss + 1e-8) < 0.0001:
                history.converged = True
                history.converged_reason = f"Loss plateaued @ epoch {epoch}"
                print(f"\n  ðŸŽ¯ {history.converged_reason}")
                break
        
        # Logging
        if epoch % config.log_every == 0 or epoch == config.n_epochs - 1:
            print(f"  Epoch {epoch:4d}: train={train_loss:.6f}, val={val_loss:.6f}, "
                  f"best={history.best_val_loss:.6f}@{history.best_epoch}, lr={lr:.6f}")
    
    # Check if we hit max epochs without converging
    if not history.converged:
        history.converged_reason = f"Max epochs ({config.n_epochs}) reached without convergence"
        print(f"\n  ðŸ”´ {history.converged_reason} - consider increasing epochs or patience")
    
    history.training_time = time.time() - start_time
    
    return best_params, history


def evaluate_model(model: ImputationModel,
                   params: Dict,
                   data: np.ndarray,
                   mask: np.ndarray,
                   manifold_type: ManifoldType = None) -> Dict[str, float]:
    """
    Evaluate trained model on test data.

    Returns metrics on imputed (missing) values, including MIS and MRR.
    """
    if not HAS_JAX:
        return {}

    original_shape = data.shape
    is_matrix_data = data.ndim == 3

    # Flatten if needed
    if is_matrix_data:
        n_samples = data.shape[0]
        data_flat = data.reshape(n_samples, -1)
        mask_flat = mask.reshape(n_samples, -1)
    else:
        data_flat = data
        mask_flat = mask

    data_flat = jnp.array(data_flat)
    mask_flat = jnp.array(mask_flat)

    # Get predictions
    masked_input = data_flat * mask_flat
    predictions = model.forward(masked_input, params)

    # Compute metrics on MISSING entries only
    missing_mask = ~mask_flat
    n_missing = jnp.sum(missing_mask)

    if n_missing == 0:
        return {"rmse": 0.0, "mae": 0.0, "r2": 1.0, "mrr": 1.0, "mis": 0.0}

    true_missing = np.array(data_flat[missing_mask])
    pred_missing = np.array(predictions[missing_mask])

    # Use common metrics function
    metrics = compute_imputation_metrics(true_missing, pred_missing)

    # Manifold Integrity Score (MIS)
    # Reconstruct full predictions for MIS computation
    full_predictions = np.array(data_flat * mask_flat + predictions * (~mask_flat))

    mis = 0.0
    if manifold_type is not None:
        if is_matrix_data:
            # Reshape back to matrices for MIS
            matrix_dim = original_shape[1]
            pred_matrices = full_predictions.reshape(-1, matrix_dim, matrix_dim)
            mis_calc = ManifoldIntegrityScore(manifold_type, dim=matrix_dim)
            mis, _ = mis_calc.compute_batch(np.array(pred_matrices))
        elif manifold_type == ManifoldType.SPHERE:
            mis_calc = ManifoldIntegrityScore(manifold_type)
            mis, _ = mis_calc.compute_batch(np.array(full_predictions))

    metrics["mis"] = float(mis)
    return metrics


def train_spd_model(model: 'SPDTangentSpaceModel',
                    data: np.ndarray,
                    mask: np.ndarray,
                    config: TrainingConfig) -> Tuple[Dict, TrainingHistory]:
    """
    Train SPD tangent space model with proper manifold-aware loss.
    
    Key differences from standard training:
    1. Works with full matrices (not flattened)
    2. Uses Log-Euclidean loss in tangent space
    3. Computes mask for upper triangle representation
    
    Args:
        model: SPDTangentSpaceModel instance
        data: SPD matrices (n_samples, dim, dim)
        mask: Boolean mask (True = observed)
        config: Training configuration
        
    Returns:
        best_params: Best model parameters
        history: Training history
    """
    if not HAS_JAX:
        raise RuntimeError("JAX required for training")
    
    # Data should be 3D (n_samples, matrix_dim, matrix_dim)
    assert data.ndim == 3, f"Expected 3D data for SPD, got {data.ndim}D"
    n_samples, matrix_dim, _ = data.shape
    
    # Flatten for batch processing
    data_flat = jnp.array(data.reshape(n_samples, -1))
    mask_flat = jnp.array(mask.reshape(n_samples, -1))
    
    # Train/val/test split
    rng = np.random.default_rng(config.seed)
    indices = rng.permutation(n_samples)
    
    n_test = int(n_samples * config.test_split)
    n_val = int(n_samples * config.validation_split)
    n_train = n_samples - n_test - n_val
    
    train_idx = indices[:n_train]
    val_idx = indices[n_train:n_train + n_val]
    
    train_data, train_mask = data_flat[train_idx], mask_flat[train_idx]
    val_data, val_mask = data_flat[val_idx], mask_flat[val_idx]
    
    # Initialize
    key = jax.random.PRNGKey(config.seed)
    params = model.init_params(key)
    
    # JIT compile loss and gradient
    @jit
    def compute_loss(params, x, m):
        return model.loss_fn(params, x, m)
    
    grad_fn = jit(grad(lambda p, x, m: model.loss_fn(p, x, m)))
    
    # Training state
    history = TrainingHistory()
    best_params = params
    lr = config.learning_rate
    patience_counter = 0
    lr_patience_counter = 0
    
    start_time = time.time()
    
    # Gradient clipping
    max_grad_norm = 10.0
    
    def clip_grads(grads: Dict, max_norm: float) -> Dict:
        total_norm = 0.0
        for g in grads.values():
            total_norm += float(jnp.sum(g ** 2))
        total_norm = np.sqrt(total_norm)
        
        if total_norm > max_norm:
            scale = max_norm / (total_norm + 1e-6)
            return {k: g * scale for k, g in grads.items()}
        return grads
    
    for epoch in range(config.n_epochs):
        # Shuffle training data
        perm = rng.permutation(n_train)
        train_data_shuffled = train_data[perm]
        train_mask_shuffled = train_mask[perm]
        
        # Mini-batch training
        epoch_losses = []
        for i in range(0, n_train, config.batch_size):
            batch_x = train_data_shuffled[i:i + config.batch_size]
            batch_m = train_mask_shuffled[i:i + config.batch_size]
            
            # Compute gradients
            grads = grad_fn(params, batch_x, batch_m)
            grads = clip_grads(grads, max_grad_norm)
            
            # Update
            params = model.compute_update(params, grads, lr)
            
            batch_loss = float(compute_loss(params, batch_x, batch_m))
            epoch_losses.append(batch_loss)
        
        train_loss = np.mean(epoch_losses)
        val_loss = float(compute_loss(params, val_data, val_mask))
        
        # Record history
        history.train_losses.append(train_loss)
        history.val_losses.append(val_loss)
        history.learning_rates.append(lr)
        history.epochs.append(epoch)
        history.final_epoch = epoch
        
        # Check for improvement
        if val_loss < history.best_val_loss:
            history.best_val_loss = val_loss
            history.best_epoch = epoch
            best_params = {k: v.copy() for k, v in params.items()}
            patience_counter = 0
            lr_patience_counter = 0
        else:
            patience_counter += 1
            lr_patience_counter += 1
        
        # Learning rate decay
        if lr_patience_counter >= config.lr_decay_patience:
            new_lr = lr * config.lr_decay_factor
            if new_lr >= config.min_lr:
                lr = new_lr
                if epoch % config.log_every == 0:
                    print(f"  Epoch {epoch}: LR decayed to {lr:.6f}")
            lr_patience_counter = 0
        
        # Early stopping
        if patience_counter >= config.early_stopping_patience:
            history.converged = True
            if epoch < 100:
                flag = "âš¡ FAST"
            elif epoch < config.n_epochs * 0.3:
                flag = "âœ“ GOOD"
            elif epoch > config.n_epochs * 0.8:
                flag = "âš ï¸  SLOW"
            else:
                flag = "âœ“"
            history.converged_reason = f"Early stop @ epoch {epoch} (best @ {history.best_epoch})"
            print(f"\n  {flag} {history.converged_reason}")
            break
        
        # Convergence check
        min_epochs_for_plateau = max(200, config.n_epochs // 5)
        if epoch >= min_epochs_for_plateau and len(history.val_losses) > 100:
            recent_loss = np.mean(history.val_losses[-30:])
            older_loss = np.mean(history.val_losses[-100:-50])
            if abs(recent_loss - older_loss) / (older_loss + 1e-8) < 0.0001:
                history.converged = True
                history.converged_reason = f"Loss plateaued @ epoch {epoch}"
                print(f"\n  ðŸŽ¯ {history.converged_reason}")
                break
        
        if epoch % config.log_every == 0 or epoch == config.n_epochs - 1:
            print(f"  Epoch {epoch:4d}: train={train_loss:.6f}, val={val_loss:.6f}, "
                  f"best={history.best_val_loss:.6f}@{history.best_epoch}, lr={lr:.6f}")
    
    if not history.converged:
        history.converged_reason = f"Max epochs ({config.n_epochs}) reached without convergence"
        print(f"\n  ðŸ”´ {history.converged_reason}")
    
    history.training_time = time.time() - start_time
    
    return best_params, history


def evaluate_spd_model(model: 'SPDTangentSpaceModel',
                       params: Dict,
                       data: np.ndarray,
                       mask: np.ndarray,
                       manifold_type: ManifoldType = None) -> Dict[str, float]:
    """
    Evaluate SPD tangent space model with proper manifold metrics.
    
    Key differences from standard evaluation:
    1. Uses Log-Euclidean distance for error metrics
    2. Reconstructs full SPD matrices via exponential map (guaranteed SPD)
    3. Computes MIS to verify manifold integrity
    """
    if not HAS_JAX:
        return {}

    assert data.ndim == 3, "Expected 3D SPD data"
    n_samples, matrix_dim, _ = data.shape

    # Flatten for processing
    data_flat = jnp.array(data.reshape(n_samples, -1))
    mask_flat = jnp.array(mask.reshape(n_samples, -1))

    # Convert to tangent space
    true_tangent = model._batch_to_tangent(data_flat)

    # Create tangent space mask (upper triangle)
    mask_reshaped = mask.reshape(-1, matrix_dim, matrix_dim)
    idx = model._get_triu_idx()
    mask_tangent = jnp.array(mask_reshaped[:, idx[0], idx[1]])

    # Masked input in tangent space
    masked_tangent = true_tangent * mask_tangent

    # Forward pass
    pred_tangent = model.forward(masked_tangent, params)

    # Metrics in tangent space (Log-Euclidean)
    missing_mask = ~mask_tangent
    n_missing = jnp.sum(missing_mask)

    if n_missing == 0:
        return {"rmse": 0.0, "mae": 0.0, "r2": 1.0, "mrr": 1.0, "mis": 0.0}

    true_missing = np.array(true_tangent[missing_mask])
    pred_missing = np.array(pred_tangent[missing_mask])

    # Use common metrics function
    metrics = compute_imputation_metrics(true_missing, pred_missing)

    # Compute MIS (should be near-zero since exp(tangent) is guaranteed SPD)
    mis = 0.0
    if manifold_type == ManifoldType.SPD:
        # Reconstruct full predictions in tangent space
        full_pred_tangent = np.array(true_tangent * mask_tangent + pred_tangent * (~mask_tangent))

        # Map back to manifold
        pred_matrices_flat = model._batch_from_tangent(jnp.array(full_pred_tangent))
        pred_matrices = np.array(pred_matrices_flat).reshape(-1, matrix_dim, matrix_dim)

        mis_calc = ManifoldIntegrityScore(manifold_type, dim=matrix_dim)
        mis, _ = mis_calc.compute_batch(pred_matrices)

    metrics["mis"] = float(mis)
    return metrics


@dataclass
class LearnedBenchmarkResult:
    """Results from learned model benchmark (averaged over multiple runs)."""
    timestamp: str
    dataset_name: str
    missing_fraction: float
    modula_metrics: Dict[str, float]  # Mean metrics
    diffgeo_metrics: Dict[str, float]  # Mean metrics
    modula_metrics_std: Dict[str, float]  # Std metrics (if n_runs > 1)
    diffgeo_metrics_std: Dict[str, float]  # Std metrics (if n_runs > 1)
    modula_history: Dict
    diffgeo_history: Dict
    modula_training_time: float
    diffgeo_training_time: float
    n_runs: int = 1
    
    def to_dict(self) -> Dict:
        result = {
            'timestamp': self.timestamp,
            'dataset_name': self.dataset_name,
            'missing_fraction': self.missing_fraction,
            'n_runs': self.n_runs,
            'modula': {
                'metrics': self.modula_metrics,
                'metrics_std': self.modula_metrics_std,
                'history': self.modula_history,
                'training_time': self.modula_training_time,
            },
            'diffgeo': {
                'metrics': self.diffgeo_metrics,
                'metrics_std': self.diffgeo_metrics_std,
                'history': self.diffgeo_history,
                'training_time': self.diffgeo_training_time,
            },
            'improvement': {
                'rmse_pct': _compute_improvement(
                    self.modula_metrics.get('rmse', 0),
                    self.diffgeo_metrics.get('rmse', 0)
                ),
                'mae_pct': _compute_improvement(
                    self.modula_metrics.get('mae', 0),
                    self.diffgeo_metrics.get('mae', 0)
                ),
                'mis_pct': _compute_improvement(
                    self.modula_metrics.get('mis', 0),
                    self.diffgeo_metrics.get('mis', 0)
                ),
            }
        }
        return result


def _compute_improvement(baseline: float, geometric: float) -> float:
    """
    Compute % difference between methods (symmetric - can be positive or negative).
    
    Returns:
        Positive: geometric is better (lower error)
        Negative: baseline is better (lower error)
        Zero: methods are equivalent
    """
    if baseline == 0 and geometric == 0:
        return 0.0
    if baseline == 0:
        return -100.0  # Baseline was perfect, geometric is worse
    return (baseline - geometric) / baseline * 100


def _aggregate_metrics(metrics_list: List[Dict[str, float]]) -> Tuple[Dict[str, float], Dict[str, float]]:
    """Compute mean and std of metrics across runs."""
    if not metrics_list:
        return {}, {}
    
    keys = metrics_list[0].keys()
    means = {}
    stds = {}
    
    for key in keys:
        values = [m.get(key, 0) for m in metrics_list]
        means[key] = float(np.mean(values))
        stds[key] = float(np.std(values))
    
    return means, stds


def run_learned_benchmark(data: np.ndarray,
                          dataset_name: str,
                          missing_fractions: List[float] = [0.1, 0.2, 0.3],
                          config: Optional[TrainingConfig] = None,
                          manifold_type: ManifoldType = None,
                          n_runs: int = 1) -> List[LearnedBenchmarkResult]:
    """
    Run full benchmark comparing Modula vs DiffGeo learned imputation.
    
    Args:
        data: Data array (n_samples, dim) or (n_samples, d, d) for SPD
        dataset_name: Name for logging
        missing_fractions: List of missing fractions to test
        config: Training configuration
        manifold_type: Type of manifold for MIS computation
        n_runs: Number of runs to average over (default 1, recommend 5 for papers)
        
    Returns:
        List of benchmark results for each missing fraction (averaged over n_runs)
    """
    if not HAS_JAX:
        print("JAX required for learned benchmarks")
        return []

    config = config or TrainingConfig()
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # Detect if data is SPD matrices (3D with square last dims)
    is_spd_data = (data.ndim == 3 and 
                   data.shape[1] == data.shape[2] and 
                   manifold_type == ManifoldType.SPD)

    # Determine input dimension and model type
    if data.ndim == 3:
        matrix_dim = data.shape[1]
        input_dim = matrix_dim * matrix_dim
        # For SPD, use smaller hidden dim based on triangular representation
        if is_spd_data:
            tri_dim = matrix_dim * (matrix_dim + 1) // 2
            hidden_dim = min(64, tri_dim // 2)
        else:
            hidden_dim = min(64, input_dim // 2)
    else:
        input_dim = data.shape[1]
        hidden_dim = min(64, input_dim)

    if is_spd_data:
        print(f"\nðŸ“ Detected SPD manifold data ({matrix_dim}Ã—{matrix_dim} matrices)")
        print(f"   Using manifold-aware tangent space model for DiffGeo")

    results = []

    for frac in missing_fractions:
        print(f"\n{'='*60}")
        print(f"Missing Fraction: {frac:.0%}")
        if n_runs > 1:
            print(f"Running {n_runs} trials for statistical significance...")
        print(f"{'='*60}")

        modula_metrics_all = []
        diffgeo_metrics_all = []
        modula_times = []
        diffgeo_times = []
        modula_history_last = None
        diffgeo_history_last = None

        for run_idx in range(n_runs):
            # Different seed for each run but reproducible
            run_seed = config.seed + run_idx * 1000 + int(frac * 100)

            if n_runs > 1:
                print(f"\n  --- Run {run_idx + 1}/{n_runs} (seed={run_seed}) ---")

            # Create random mask for this run
            rng = np.random.default_rng(run_seed)
            mask = rng.random(data.shape) > frac

            # Train Modula model (with fresh Adam state)
            print(f"\n  Training Modula (Euclidean)...")
            modula_model = ModulaImputationModel(input_dim, hidden_dim, run_seed)
            modula_model.adam_state = None
            modula_params, modula_history = train_model(modula_model, data, mask, config)
            modula_metrics = evaluate_model(modula_model, modula_params, data, mask, manifold_type)
            modula_metrics_all.append(modula_metrics)
            modula_times.append(modula_history.training_time)
            modula_history_last = modula_history
            print(f"    RMSE={modula_metrics['rmse']:.4f}, MAE={modula_metrics['mae']:.4f}, RÂ²={modula_metrics['r2']:.4f}")

            # Train DiffGeo model - use manifold-aware model for SPD data
            if is_spd_data:
                print(f"\n  Training DiffGeo (SPD Tangent Space)...")
                diffgeo_model = SPDTangentSpaceModel(matrix_dim, hidden_dim, run_seed)
                diffgeo_model.adam_state = None
                diffgeo_params, diffgeo_history = train_spd_model(
                    diffgeo_model, data, mask, config
                )
                diffgeo_metrics = evaluate_spd_model(
                    diffgeo_model, diffgeo_params, data, mask, manifold_type
                )
            else:
                print(f"\n  Training DiffGeo (Riemannian)...")
                diffgeo_model = DiffGeoImputationModel(input_dim, hidden_dim, run_seed)
                diffgeo_model.fisher_diag = None
                diffgeo_params, diffgeo_history = train_model(diffgeo_model, data, mask, config)
                diffgeo_metrics = evaluate_model(diffgeo_model, diffgeo_params, data, mask, manifold_type)

            diffgeo_metrics_all.append(diffgeo_metrics)
            diffgeo_times.append(diffgeo_history.training_time)
            diffgeo_history_last = diffgeo_history
            print(f"    RMSE={diffgeo_metrics['rmse']:.4f}, MAE={diffgeo_metrics['mae']:.4f}, RÂ²={diffgeo_metrics['r2']:.4f}")

        # Aggregate metrics across runs
        modula_mean, modula_std = _aggregate_metrics(modula_metrics_all)
        diffgeo_mean, diffgeo_std = _aggregate_metrics(diffgeo_metrics_all)

        # Report aggregated results
        if n_runs > 1:
            print(f"\n  {'='*50}")
            print(f"  AGGREGATED RESULTS ({n_runs} runs)")
            print(f"  {'='*50}")
            print(f"  Modula:  RMSE={modula_mean['rmse']:.4f}Â±{modula_std['rmse']:.4f}, "
                  f"MAE={modula_mean['mae']:.4f}Â±{modula_std['mae']:.4f}, "
                  f"RÂ²={modula_mean['r2']:.4f}Â±{modula_std['r2']:.4f}")
            print(f"  DiffGeo: RMSE={diffgeo_mean['rmse']:.4f}Â±{diffgeo_std['rmse']:.4f}, "
                  f"MAE={diffgeo_mean['mae']:.4f}Â±{diffgeo_std['mae']:.4f}, "
                  f"RÂ²={diffgeo_mean['r2']:.4f}Â±{diffgeo_std['r2']:.4f}")
        else:
            print(f"\n  Final Modula:  RMSE={modula_mean['rmse']:.4f}, MAE={modula_mean['mae']:.4f}, RÂ²={modula_mean['r2']:.4f}")
            print(f"  Final DiffGeo: RMSE={diffgeo_mean['rmse']:.4f}, MAE={diffgeo_mean['mae']:.4f}, RÂ²={diffgeo_mean['r2']:.4f}")

        # Compute difference
        rmse_diff = _compute_improvement(modula_mean['rmse'], diffgeo_mean['rmse'])
        if rmse_diff > 0:
            print(f"\n  Result: DiffGeo better by {rmse_diff:+.1f}% RMSE")
        elif rmse_diff < 0:
            print(f"\n  Result: Modula better by {-rmse_diff:+.1f}% RMSE")
        else:
            print(f"\n  Result: Methods are equivalent")

        result = LearnedBenchmarkResult(
            timestamp=timestamp,
            dataset_name=dataset_name,
            missing_fraction=frac,
            modula_metrics=modula_mean,
            diffgeo_metrics=diffgeo_mean,
            modula_metrics_std=modula_std,
            diffgeo_metrics_std=diffgeo_std,
            modula_history=modula_history_last.to_dict() if modula_history_last else {},
            diffgeo_history=diffgeo_history_last.to_dict() if diffgeo_history_last else {},
            modula_training_time=float(np.mean(modula_times)),
            diffgeo_training_time=float(np.mean(diffgeo_times)),
            n_runs=n_runs,
        )
        results.append(result)

    return results


# =============================================================================
# MULTI-MODEL BENCHMARK INFRASTRUCTURE
# =============================================================================
# Generalized benchmark system supporting any number of models with automatic
# JSON/figure saving. Use this for new benchmarks.
# =============================================================================


@dataclass
class ModelConfig:
    """Configuration for a single model in a multi-model benchmark."""

    name: str  # Display name (e.g., "Fisher Geometry")
    model_class: type  # Model class (must subclass ImputationModel)
    model_kwargs: Dict = field(default_factory=dict)  # Extra kwargs for model init
    train_fn: Optional[Callable] = (
        None  # Custom training function (default: train_model)
    )
    eval_fn: Optional[Callable] = (
        None  # Custom evaluation function (default: evaluate_model)
    )
    color: str = "#3498db"  # Color for visualization


@dataclass
class ModelResult:
    """Results for a single model at a single missing fraction."""

    name: str
    metrics: Dict[str, float]
    metrics_std: Dict[str, float]
    history: Dict
    training_time: float


@dataclass
class MultiBenchmarkResult:
    """
    Results from multi-model benchmark (generalized version of LearnedBenchmarkResult).

    Supports any number of models, not just modula/diffgeo.
    """

    timestamp: str
    dataset_name: str
    missing_fraction: float
    model_results: Dict[str, ModelResult]  # model_name -> ModelResult
    n_runs: int = 1

    def to_dict(self) -> Dict:
        """Convert to dictionary for JSON serialization."""
        result = {
            "timestamp": self.timestamp,
            "dataset_name": self.dataset_name,
            "missing_fraction": self.missing_fraction,
            "n_runs": self.n_runs,
            "models": {},
        }

        for name, mr in self.model_results.items():
            result["models"][name] = {
                "metrics": mr.metrics,
                "metrics_std": mr.metrics_std,
                "history": mr.history,
                "training_time": mr.training_time,
            }

        # Compute improvements relative to first model (baseline)
        if len(self.model_results) >= 2:
            names = list(self.model_results.keys())
            baseline_name = names[0]
            baseline_rmse = self.model_results[baseline_name].metrics.get("rmse", 0)

            result["improvements"] = {}
            for name in names[1:]:
                other_rmse = self.model_results[name].metrics.get("rmse", 0)
                if baseline_rmse > 0:
                    imp = (baseline_rmse - other_rmse) / baseline_rmse * 100
                else:
                    imp = 0.0
                result["improvements"][f"{name}_vs_{baseline_name}"] = imp

        return result

    def get_best_model(self, metric: str = "rmse", lower_is_better: bool = True) -> str:
        """Return name of best-performing model for given metric."""
        scores = {
            name: mr.metrics.get(
                metric, float("inf") if lower_is_better else float("-inf")
            )
            for name, mr in self.model_results.items()
        }
        if lower_is_better:
            return min(scores, key=scores.get)
        return max(scores, key=scores.get)


def run_multi_model_benchmark(
    data: np.ndarray,
    dataset_name: str,
    model_configs: List[ModelConfig],
    missing_fractions: List[float] = [0.1, 0.3, 0.5, 0.7, 0.9],
    config: Optional[TrainingConfig] = None,
    manifold_type: ManifoldType = None,
    n_runs: int = 1,
) -> List[MultiBenchmarkResult]:
    """
    Run benchmark comparing multiple models.

    This is the generalized version of run_learned_benchmark that supports
    any number of models, not just modula/diffgeo.

    Args:
        data: Data array (n_samples, dim) or (n_samples, d, d) for SPD
        dataset_name: Name for logging
        model_configs: List of ModelConfig defining models to compare
        missing_fractions: List of missing fractions to test
        config: Training configuration
        manifold_type: Type of manifold for MIS computation
        n_runs: Number of runs to average over

    Returns:
        List of MultiBenchmarkResult for each missing fraction

    Example:
        >>> configs = [
        ...     ModelConfig("Modula", ModulaImputationModel, color="#e74c3c"),
        ...     ModelConfig("SPD Tangent", SPDTangentSpaceModel,
        ...                 train_fn=train_spd_model, eval_fn=evaluate_spd_model,
        ...                 color="#27ae60"),
        ...     ModelConfig("Fisher", FisherImputationModel,
        ...                 model_kwargs={'use_natural_gradient': True},
        ...                 color="#3498db"),
        ... ]
        >>> results = run_multi_model_benchmark(data, "eeg", configs)
    """
    if not HAS_JAX:
        print("JAX required for benchmarks")
        return []

    config = config or TrainingConfig()
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # Detect data shape
    is_spd_data = (
        data.ndim == 3
        and data.shape[1] == data.shape[2]
        and manifold_type == ManifoldType.SPD
    )

    if data.ndim == 3:
        matrix_dim = data.shape[1]
        input_dim = matrix_dim * matrix_dim
        if is_spd_data:
            tri_dim = matrix_dim * (matrix_dim + 1) // 2
            hidden_dim = min(64, tri_dim // 2)
        else:
            hidden_dim = min(64, input_dim // 2)
    else:
        input_dim = data.shape[1]
        hidden_dim = min(64, input_dim)
        matrix_dim = None

    print(f"\n{'='*70}")
    print(f"MULTI-MODEL BENCHMARK: {dataset_name}")
    print(f"{'='*70}")
    print(f"Timestamp: {timestamp}")
    print(f"Models: {[c.name for c in model_configs]}")
    print(f"Missing fractions: {missing_fractions}")
    print(f"Runs per condition: {n_runs}")
    print(f"Data shape: {data.shape}")
    if is_spd_data:
        print(f"SPD manifold detected ({matrix_dim}Ã—{matrix_dim})")
    print(f"{'='*70}")

    results = []

    for frac in missing_fractions:
        print(f"\n{'='*60}")
        print(f"Missing Fraction: {frac:.0%}")
        print(f"{'='*60}")

        # Storage for all models
        model_metrics_all = {c.name: [] for c in model_configs}
        model_times = {c.name: [] for c in model_configs}
        model_histories = {c.name: None for c in model_configs}

        for run_idx in range(n_runs):
            run_seed = config.seed + run_idx * 1000 + int(frac * 100)

            if n_runs > 1:
                print(f"\n  --- Run {run_idx + 1}/{n_runs} ---")

            # Create mask
            rng = np.random.default_rng(run_seed)
            mask = rng.random(data.shape) > frac

            # Train each model
            for i, mc in enumerate(model_configs):
                print(f"\n  [{i+1}/{len(model_configs)}] Training {mc.name}...")

                # Determine model constructor args
                if (
                    is_spd_data
                    and "matrix_dim" in mc.model_class.__init__.__code__.co_varnames
                ):
                    model = mc.model_class(
                        matrix_dim, hidden_dim, run_seed, **mc.model_kwargs
                    )
                else:
                    model = mc.model_class(
                        input_dim, hidden_dim, run_seed, **mc.model_kwargs
                    )

                # Reset state
                if hasattr(model, "adam_state"):
                    model.adam_state = None
                if hasattr(model, "fisher_diag"):
                    model.fisher_diag = None
                if hasattr(model, "_fisher_ema"):
                    model._fisher_ema = None

                # Train
                train_fn = mc.train_fn or train_model
                params, history = train_fn(model, data, mask, config)

                # Evaluate
                eval_fn = mc.eval_fn or evaluate_model
                metrics = eval_fn(model, params, data, mask, manifold_type)

                model_metrics_all[mc.name].append(metrics)
                model_times[mc.name].append(history.training_time)
                model_histories[mc.name] = history

                print(f"       RMSE={metrics['rmse']:.4f}, MAE={metrics['mae']:.4f}")

        # Aggregate results for this fraction
        model_results = {}
        for mc in model_configs:
            mean_metrics, std_metrics = _aggregate_metrics(model_metrics_all[mc.name])
            model_results[mc.name] = ModelResult(
                name=mc.name,
                metrics=mean_metrics,
                metrics_std=std_metrics,
                history=(
                    model_histories[mc.name].to_dict()
                    if model_histories[mc.name]
                    else {}
                ),
                training_time=float(np.mean(model_times[mc.name])),
            )

        # Print summary for this fraction
        print(f"\n  Summary ({frac:.0%} missing):")
        print(f"  {'Model':<20} {'RMSE':<10} {'MAE':<10} {'RÂ²':<10}")
        print(f"  {'-'*50}")
        for name, mr in model_results.items():
            print(
                f"  {name:<20} {mr.metrics['rmse']:<10.4f} {mr.metrics['mae']:<10.4f} {mr.metrics['r2']:<10.4f}"
            )

        result = MultiBenchmarkResult(
            timestamp=timestamp,
            dataset_name=dataset_name,
            missing_fraction=frac,
            model_results=model_results,
            n_runs=n_runs,
        )
        results.append(result)

    return results
